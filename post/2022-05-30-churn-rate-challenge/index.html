<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Data Science Challenge - Churn Rate | Dev_Medeiros</title><meta name=keywords content="storytelling,python,data analysis,machine learning,neural network"><meta name=description content="Tools used: Python, seaborn, scikit-learn, imbalanced-learn
Category: Data Analysis, Machine Learning
"><meta name=author content="Jaqueline Souza Medeiros"><link rel=canonical href=https://devmedeiros.com/post/2022-05-30-churn-rate-challenge/><link crossorigin=anonymous href=/assets/css/stylesheet.min.c2b97fdc1ad03d6a7cd347264fa06ef59d8977a5af2e0315ed55c3fc3dade358.css integrity="sha256-wrl/3BrQPWp800cmT6Bu9Z2Jd6WvLgMV7VXD/D2t41g=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.min.e85ad0406048e8176e1c7661b25d5c69297ddfe41dc4124cf75ecb99a4f7b3d1.js integrity="sha256-6FrQQGBI6BduHHZhsl1caSl93+QdxBJM917LmaT3s9E=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://devmedeiros.com/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=16x16 href=https://devmedeiros.com/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=32x32 href=https://devmedeiros.com/%3Clink%20/%20abs%20url%3E><link rel=apple-touch-icon href=https://devmedeiros.com/%3Clink%20/%20abs%20url%3E><link rel=mask-icon href=https://devmedeiros.com/%3Clink%20/%20abs%20url%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><meta name=generator content="Hugo 0.101.0"><link rel=alternate hreflang=en href=https://devmedeiros.com/post/2022-05-30-churn-rate-challenge/><link rel=alternate hreflang=pt href=https://devmedeiros.com/pt/post/2022-05-30-churn-rate-challenge/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><meta property="og:title" content="Data Science Challenge - Churn Rate"><meta property="og:description" content="Tools used: Python, seaborn, scikit-learn, imbalanced-learn
Category: Data Analysis, Machine Learning
"><meta property="og:type" content="article"><meta property="og:url" content="https://devmedeiros.com/post/2022-05-30-churn-rate-challenge/"><meta property="og:image" content="https://devmedeiros.com/cover.png"><meta property="article:section" content="post"><meta property="article:published_time" content="2022-05-30T16:49:00-03:00"><meta property="article:modified_time" content="2022-06-09T18:08:00-03:00"><meta property="og:site_name" content="Dev_Medeiros"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://devmedeiros.com/cover.png"><meta name=twitter:title content="Data Science Challenge - Churn Rate"><meta name=twitter:description content="Tools used: Python, seaborn, scikit-learn, imbalanced-learn
Category: Data Analysis, Machine Learning
"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://devmedeiros.com/post/"},{"@type":"ListItem","position":2,"name":"Data Science Challenge - Churn Rate","item":"https://devmedeiros.com/post/2022-05-30-churn-rate-challenge/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Data Science Challenge - Churn Rate","name":"Data Science Challenge - Churn Rate","description":"Tools used: Python, seaborn, scikit-learn, imbalanced-learn\nCategory: Data Analysis, Machine Learning\n","keywords":["storytelling","python","data analysis","machine learning","neural network"],"articleBody":"Tools used: Python, seaborn, scikit-learn, imbalanced-learn\nCategory: Data Analysis, Machine Learning\nI was challenged to take the role of a new data scientist hired at Alura Voz. This made-up company is a telecommunication company and it needs to reduce the Churn Rate.\nThe challenge is divided into four weeks. For the first week, the goal was to clean the dataset provided by an API. Next, we need to identify clients who are more likely to leave the company, using data exploration and analysis. Then, in the third week, we made machine learning models to predict the churn rate for Alura Voz. The last week is to show off what we made during the challenge and build our portfolio. In case you are interested in seeing the code for the challenge just head over to my GitHub repository.\nFirst Week Reading the dataset The dataset is available in a JSON file, at first glance it looked like a normal data frame.\nBut, as we can see, customer, phone, internet, and account are their own separate table. So I had to normalize them separately and then I just concatenated all these tables into one.\nMissing data The first time I looked for missing data in this dataset I notice that apparently, that wasn’t anything missing, but later on, I noticed that there was empty space and just space not being counted as NaN. So I corrected this, and now the dataset had 224 missing values for Churn and 11 missing for Charges.Total.\nI decided to drop the missing Churn because this is going to be the object of our study and there isn’t a point in studying something that doesn’t exist. For the missing Charges.Total, I think it represents a customer that hasn’t paid anything yet, because all of them had a tenure of 0, meaning that they had just become a client, so I just replaced the missing value for 0.\nFeature Encoding The feature SeniorCitizen was the only one that came with 0 and 1 instead of Yes and No. For now, I’m changing it to yes and no, because it’ll make the analysis simpler to read.\nCharges.Monthly and Charges.Total were renamed to lose the dot because the dot gets in the way when calling the feature in python.\nSecond Week Data Analysis In the first plot, we can see how much unbalanced our data set is. There’re over 5000 clients that didn’t leave the company and a little less than 2000 that left.\nI experimented with oversampling the dataset to handle this imbalance, but it made the machine learning models worse. And undersampling isn’t an option with this dataset size, so I just decided to leave it the way it is, and when it’s time to split the training and test set I’ll stratify the dataset by the Churn feature.\nI also generated 16 plots for all the discrete data, to see all the plots check this notebook. I wanted to see if there was any behavior that made some clients more likely to leave the company. Is clear that all, except for gender, seems to play a role in determining if a client will leave the company or not. More specifically payment methods, contracts, online backup, tech support, and internet service.\nIn the tenure plot, I decided to make a distribution plot for the tenure, one plot for clients that didn’t churn and another for the clients that did churn. We can see that clients that left the company tend to do so at the beginning of their time in the company.\nThe average monthly charge for clients that didn’t churn is 61.27 monetary units, while clients that churn were paying 74.44. This is probably because of the type of contract they prefer, but either way is known that higher prices drive the customers away.\nThe Churn Profile Considering everything that I could see through plots and measures. I came up with a profile for clients that are more likely to churn.\nNew clients are more likely to churn than older clients.\nCustomers that use fewer services and products tend to leave the company. Also, when they aren’t tied down to a longer contract they seem to be more likely to quit.\nRegarding the payment method, clients that churn have a strong preference for electronic checks and usually are spending 13.17 monetary units more than the average client that didn’t leave.\nThird Week Preparing the dataset We start by making dummies variables dropping the first, so we would have n-1 dummies for n categories. Then we move on to look at features correlation.\nWe can see that the InternetService_No feature has a lot of strong correlations with many other features, this is because these other features depend on the client having internet service. So I’ll drop all features that are dependent on this one. The same thing happens with PhoneService_Yes.\ntenure and ChargesTotal also have a strong correlation, so I tried running the models without one of them and both, and it had a worse performance and took a long time to converge, so I decided to keep them as they are relevant as well.\nAfter dropping the features I finish preparing the dataset by normalizing the numeric data, ChargesTotal and tenure.\nTest and training dataset I split the dataset into training and testing sets, 20% for testing and the rest for training. I stratified the data by the Churn feature and I shuffle the dataset before splitting. The same split is used by all the models. After splitting the dataset I decided to oversample the train data using SMOTE1 because the dataset is imbalanced. The reason that I only used this technique on the training set is that I don’t want to have a biased result, oversampling all the datasets would mean that I’d be testing my models on the same data that I trained, and that’s not the goal here.\nModel Evaluation I’ll use a dummy classifier to have a baseline model for the accuracy score, and I’ll also use the metrics: precision, recall and f1 score2. Although the dummy model won’t have values for this metrics, I’ll keep it for comparison on how much the models improved.\nBaseline I made the baseline model using a dummy classifier that guessed that every client behaved the same. It is always guessed that no client will leave the company. By using this approach we got a baseline accuracy score of 0.73456.\nAll models moving forward will have the same random state.\nModel 1 - Random Forest I start by using a grid search with cross-validation to find the best parameters within a given pool of options using the recall as the strategy to evaluate the performance. The best model was:\nRandomForestClassifier(criterion='entropy', max_depth=5, max_leaf_nodes=70, random_state=22) After fitting this model, the evaluating metrics were:\nAccuracy Score: 0.72534 Precision Score: 0.48922 Recall Score: 0.78877 F1 Score: 0.60389 Model 2 - Linear SVC For this model, I just used the default parameters and set the ceiling for the maximum of iterations to 900000.\nLinearSVC(max_iter=900000, random_state=22) After fitting this model, the evaluating metrics were:\nAccuracy Score: 0.71966 Precision Score: 0.48217 Recall Score: 0.75936 F1 Score: 0.58982 Model 3 - Multi-layer Perceptron Here I fixed the solver to LBFGS, because according to the documentation it has a better performance in smaller datasets3, and used grid search with cross-validation to find a hidden layer size that would be the best. The best model was:\nMLPClassifier(hidden_layer_sizes=(1,), max_iter=9999, random_state=22, solver='lbfgs') After fitting this model, the evaluating metrics were:\nAccuracy Score: 0.72818 Precision Score: 0.49133 Recall Score: 0.68182 F1 Score: 0.57111 Conclusion After running the three models, all of them used the same random_state. I got the following accuracy scores and improvements (compared to the baseline model):\nIn the end, the Random Forest had the best metrics overall. This model can recall a great portion of clients that churn correctly, still is not perfect but is certainly a starting point. The accuracy score is not as high as I’d like, but in this particular problem, the goal is to keep clients from leaving the company and is better to use resources to keep a client that will not leave than to do nothing.\nIn the end, I liked this challenge, because I don’t usually practice machine learning, but thanks to the challenge I got the chance to make a small project in this area that is so relevant and important. This was my first time working with neural networks and tunning hyper-parameters, and I’m sure the next time I’ll get even better results.\nimbalanced-learn documentation ↩︎\nAccuracy, Precision, Recall or F1? - Koo Ping Shung ↩︎\nscikit-learn documentation ↩︎\n","wordCount":"1438","inLanguage":"en","datePublished":"2022-05-30T16:49:00-03:00","dateModified":"2022-06-09T18:08:00-03:00","author":{"@type":"Person","name":"Jaqueline Souza Medeiros"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://devmedeiros.com/post/2022-05-30-churn-rate-challenge/"},"publisher":{"@type":"Organization","name":"Dev_Medeiros","logo":{"@type":"ImageObject","url":"https://devmedeiros.com/%3Clink%20/%20abs%20url%3E"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://devmedeiros.com/ accesskey=h title="Dev_Medeiros (Alt + H)">Dev_Medeiros</a>
<span class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button><ul class=lang-switch><li>|</li><li><a href=https://devmedeiros.com/pt/ title=Pt aria-label=Pt>Pt</a></li></ul></span></div><ul id=menu><li><a href=https://devmedeiros.com/about/ title=about><span>about</span></a></li><li><a href=https://devmedeiros.com/tags/ title=tags><span>tags</span></a></li><li><a href=https://devmedeiros.com/archives/ title=archives><span>archives</span></a></li><li><a href=https://devmedeiros.com/search/ title="search (Alt + /)" accesskey=/><span>search</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://devmedeiros.com/>Home</a>&nbsp;»&nbsp;<a href=https://devmedeiros.com/post/>Posts</a></div><h1 class=post-title>Data Science Challenge - Churn Rate</h1><div class=post-meta><span title='2022-05-30 16:49:00 -0300 -0300'>May 30, 2022</span>&nbsp;·&nbsp;<span title='2022-06-09 18:08:00 -0300 -0300'>Updated June 9, 2022</span>&nbsp;·&nbsp;7 min&nbsp;·&nbsp;Jaqueline Souza Medeiros&nbsp;|&nbsp;<ul class=i18n_list>Translations:<li><a href=https://devmedeiros.com/pt/post/2022-05-30-churn-rate-challenge/>Pt</a></li></ul>&nbsp;|&nbsp;<a href=https://github.com/devmedeiros.github.io/content/post/2022-05-30-churn-rate-challenge.en.md rel="noopener noreferrer" target=_blank>Suggest Changes</a></div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#first-week aria-label="First Week">First Week</a><ul><li><a href=#reading-the-dataset aria-label="Reading the dataset">Reading the dataset</a></li><li><a href=#missing-data aria-label="Missing data">Missing data</a></li><li><a href=#feature-encoding aria-label="Feature Encoding">Feature Encoding</a></li></ul></li><li><a href=#second-week aria-label="Second Week">Second Week</a><ul><li><a href=#data-analysis aria-label="Data Analysis">Data Analysis</a></li><li><a href=#the-churn-profile aria-label="The Churn Profile">The Churn Profile</a></li></ul></li><li><a href=#third-week aria-label="Third Week">Third Week</a><ul><li><a href=#preparing-the-dataset aria-label="Preparing the dataset">Preparing the dataset</a></li><li><a href=#test-and-training-dataset aria-label="Test and training dataset">Test and training dataset</a></li><li><a href=#model-evaluation aria-label="Model Evaluation">Model Evaluation</a></li><li><a href=#baseline aria-label=Baseline>Baseline</a></li><li><a href=#model-1---random-forest aria-label="Model 1 - Random Forest">Model 1 - Random Forest</a></li><li><a href=#model-2---linear-svc aria-label="Model 2 - Linear SVC">Model 2 - Linear SVC</a></li><li><a href=#model-3---multi-layer-perceptron aria-label="Model 3 - Multi-layer Perceptron">Model 3 - Multi-layer Perceptron</a></li><li><a href=#conclusion aria-label=Conclusion>Conclusion</a></li></ul></li></ul></div></details></div><div class=post-content><p><strong>Tools used:</strong> Python, seaborn, scikit-learn, imbalanced-learn</p><p><strong>Category:</strong> Data Analysis, Machine Learning</p><hr><p><img loading=lazy src=https://raw.githubusercontent.com/devmedeiros/Challenge-Data-Science/main/aluravoz.png#center alt="heart with an A inside and you can read &amp;lsquo;Alura Voz telecommunication company&amp;rsquo;"></p><p>I was challenged to take the role of a new data scientist hired at Alura Voz. This made-up company is a telecommunication company and it needs to reduce the Churn Rate.</p><p>The challenge is divided into four weeks. For the first week, the goal was to clean the dataset provided by an API. Next, we need to identify clients who are more likely to leave the company, using data exploration and analysis. Then, in the third week, we made machine learning models to predict the churn rate for Alura Voz. The last week is to show off what we made during the challenge and build our portfolio. In case you are interested in seeing the code for the challenge just head over to my GitHub <a href=https://github.com/devmedeiros/Challenge-Data-Science>repository</a>.</p><h1 id=first-week>First Week<a hidden class=anchor aria-hidden=true href=#first-week>#</a></h1><h2 id=reading-the-dataset>Reading the dataset<a hidden class=anchor aria-hidden=true href=#reading-the-dataset>#</a></h2><p>The dataset is available in a JSON file, at first glance it looked like a normal data frame.</p><p><img loading=lazy src=https://raw.githubusercontent.com/devmedeiros/Challenge-Data-Science/main/1%20-%20Data%20Cleaning/table_head.png#center alt="table head with the first five rows"></p><p>But, as we can see, <code>customer</code>, <code>phone</code>, <code>internet</code>, and <code>account</code> are their own separate table. So I had to normalize them separately and then I just concatenated all these tables into one.</p><h2 id=missing-data>Missing data<a hidden class=anchor aria-hidden=true href=#missing-data>#</a></h2><p>The first time I looked for missing data in this dataset I notice that apparently, that wasn&rsquo;t anything missing, but later on, I noticed that there was empty space and just space not being counted as <code>NaN</code>. So I corrected this, and now the dataset had 224 missing values for <code>Churn</code> and 11 missing for <code>Charges.Total</code>.</p><p>I decided to drop the missing <code>Churn</code> because this is going to be the object of our study and there isn&rsquo;t a point in studying something that doesn&rsquo;t exist. For the missing <code>Charges.Total</code>, I think it represents a customer that hasn&rsquo;t paid anything yet, because all of them had a tenure of 0, meaning that they had just become a client, so I just replaced the missing value for 0.</p><h2 id=feature-encoding>Feature Encoding<a hidden class=anchor aria-hidden=true href=#feature-encoding>#</a></h2><p>The feature <code>SeniorCitizen</code> was the only one that came with <code>0</code> and <code>1</code> instead of <code>Yes</code> and <code>No</code>. For now, I&rsquo;m changing it to yes and no, because it&rsquo;ll make the analysis simpler to read.</p><p><code>Charges.Monthly</code> and <code>Charges.Total</code> were renamed to lose the dot because the dot gets in the way when calling the feature in python.</p><h1 id=second-week>Second Week<a hidden class=anchor aria-hidden=true href=#second-week>#</a></h1><h2 id=data-analysis>Data Analysis<a hidden class=anchor aria-hidden=true href=#data-analysis>#</a></h2><p>In the first plot, we can see how much unbalanced our data set is. There&rsquo;re over 5000 clients that didn&rsquo;t leave the company and a little less than 2000 that left.</p><p><img loading=lazy src=https://raw.githubusercontent.com/devmedeiros/Challenge-Data-Science/main/2%20-%20Data%20Analysis/churn.jpg#center alt="bar plot with two bars, the first one is for &amp;rsquo;no&amp;rsquo; and the second is for &amp;lsquo;yes&amp;rsquo;, the first bar is over 5000 count and the second one is around 2000"></p><p>I experimented with oversampling the dataset to handle this imbalance, but it made the machine learning models worse. And undersampling isn&rsquo;t an option with this dataset size, so I just decided to leave it the way it is, and when it&rsquo;s time to split the training and test set I&rsquo;ll stratify the dataset by the <code>Churn</code> feature.</p><p>I also generated 16 plots for all the discrete data, to see all the plots check this <a href=https://github.com/devmedeiros/Challenge-Data-Science/blob/main/2%20-%20Data%20Analysis/data_analysis.ipynb>notebook</a>. I wanted to see if there was any behavior that made some clients more likely to leave the company. Is clear that all, except for <code>gender</code>, seems to play a role in determining if a client will leave the company or not. More specifically payment methods, contracts, online backup, tech support, and internet service.</p><p>In the <code>tenure</code> plot, I decided to make a distribution plot for the tenure, one plot for clients that didn&rsquo;t churn and another for the clients that did churn. We can see that clients that left the company tend to do so at the beginning of their time in the company.</p><p><img loading=lazy src=https://raw.githubusercontent.com/devmedeiros/Challenge-Data-Science/main/2%20-%20Data%20Analysis/tenure.jpg#center alt="there are two plots side-by-side, in the first one the title is &amp;lsquo;Churn = No&amp;rsquo; the data is along the tenure axis and is in a U shape. the second plot has the title &amp;lsquo;Churn = Yes&amp;rsquo; and starts high and drops fast along the tenure line"></p><p>The average monthly charge for clients that didn&rsquo;t churn is 61.27 monetary units, while clients that churn were paying 74.44. This is probably because of the type of contract they prefer, but either way is known that higher prices drive the customers away.</p><h2 id=the-churn-profile>The Churn Profile<a hidden class=anchor aria-hidden=true href=#the-churn-profile>#</a></h2><p><img loading=lazy src=https://64.media.tumblr.com/tumblr_lojvnhHFH91qlh1s6o1_400.gifv#center alt="person jumping through the window"></p><p>Considering everything that I could see through plots and measures. I came up with a profile for clients that are more likely to churn.</p><ul><li><p>New clients are more likely to churn than older clients.</p></li><li><p>Customers that use fewer services and products tend to leave the company. Also, when they aren&rsquo;t tied down to a longer contract they seem to be more likely to quit.</p></li><li><p>Regarding the payment method, clients that churn have a <strong>strong</strong> preference for electronic checks and usually are spending 13.17 monetary units more than the average client that didn&rsquo;t leave.</p></li></ul><h1 id=third-week>Third Week<a hidden class=anchor aria-hidden=true href=#third-week>#</a></h1><h2 id=preparing-the-dataset>Preparing the dataset<a hidden class=anchor aria-hidden=true href=#preparing-the-dataset>#</a></h2><p>We start by making dummies variables dropping the first, so we would have n-1 dummies for n categories. Then we move on to look at features correlation.</p><p><img loading=lazy src=https://raw.githubusercontent.com/devmedeiros/Challenge-Data-Science/main/3%20-%20Model%20Selection/corr_matrix.jpg#center alt="correlation matrix with all the features"></p><p>We can see that the <code>InternetService_No</code> feature has a lot of strong correlations with many other features, this is because these other features depend on the client having internet service. So I&rsquo;ll drop all features that are dependent on this one. The same thing happens with <code>PhoneService_Yes</code>.</p><p><code>tenure</code> and <code>ChargesTotal</code> also have a strong correlation, so I tried running the models without one of them and both, and it had a worse performance and took a long time to converge, so I decided to keep them as they are relevant as well.</p><p>After dropping the features I finish preparing the dataset by normalizing the numeric data, <code>ChargesTotal</code> and <code>tenure</code>.</p><h2 id=test-and-training-dataset>Test and training dataset<a hidden class=anchor aria-hidden=true href=#test-and-training-dataset>#</a></h2><p>I split the dataset into training and testing sets, 20% for testing and the rest for training. I stratified the data by the <code>Churn</code> feature and I shuffle the dataset before splitting. The same split is used by all the models. After splitting the dataset I decided to oversample the <strong>train</strong> data using SMOTE<sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup> because the dataset is imbalanced. The reason that I only used this technique on the training set is that I don&rsquo;t want to have a biased result, oversampling all the datasets would mean that I&rsquo;d be testing my models on the same data that I trained, and that&rsquo;s not the goal here.</p><h2 id=model-evaluation>Model Evaluation<a hidden class=anchor aria-hidden=true href=#model-evaluation>#</a></h2><p>I&rsquo;ll use a dummy classifier to have a baseline model for the accuracy score, and I&rsquo;ll also use the metrics: <code>precision</code>, <code>recall</code> and <code>f1 score</code><sup id=fnref:2><a href=#fn:2 class=footnote-ref role=doc-noteref>2</a></sup>. Although the dummy model won&rsquo;t have values for this metrics, I&rsquo;ll keep it for comparison on how much the models improved.</p><h2 id=baseline>Baseline<a hidden class=anchor aria-hidden=true href=#baseline>#</a></h2><p>I made the baseline model using a dummy classifier that guessed that every client behaved the same. It is always guessed that no client will leave the company. By using this approach we got a baseline accuracy score of <code>0.73456</code>.</p><p>All models moving forward will have the same random state.</p><h2 id=model-1---random-forest>Model 1 - Random Forest<a hidden class=anchor aria-hidden=true href=#model-1---random-forest>#</a></h2><p>I start by using a grid search with cross-validation to find the best parameters within a given pool of options using the <code>recall</code> as the strategy to evaluate the performance. The best model was:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>RandomForestClassifier(criterion<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;entropy&#39;</span>, max_depth<span style=color:#f92672>=</span><span style=color:#ae81ff>5</span>, max_leaf_nodes<span style=color:#f92672>=</span><span style=color:#ae81ff>70</span>, random_state<span style=color:#f92672>=</span><span style=color:#ae81ff>22</span>)
</span></span></code></pre></div><p>After fitting this model, the evaluating metrics were:</p><ul><li>Accuracy Score: 0.72534</li><li>Precision Score: 0.48922</li><li>Recall Score: 0.78877</li><li>F1 Score: 0.60389</li></ul><h2 id=model-2---linear-svc>Model 2 - Linear SVC<a hidden class=anchor aria-hidden=true href=#model-2---linear-svc>#</a></h2><p>For this model, I just used the default parameters and set the ceiling for the maximum of iterations to <code>900000</code>.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>LinearSVC(max_iter<span style=color:#f92672>=</span><span style=color:#ae81ff>900000</span>, random_state<span style=color:#f92672>=</span><span style=color:#ae81ff>22</span>)
</span></span></code></pre></div><p>After fitting this model, the evaluating metrics were:</p><ul><li>Accuracy Score: 0.71966</li><li>Precision Score: 0.48217</li><li>Recall Score: 0.75936</li><li>F1 Score: 0.58982</li></ul><h2 id=model-3---multi-layer-perceptron>Model 3 - Multi-layer Perceptron<a hidden class=anchor aria-hidden=true href=#model-3---multi-layer-perceptron>#</a></h2><p>Here I fixed the solver to LBFGS, because according to the documentation it has a better performance in smaller datasets<sup id=fnref:3><a href=#fn:3 class=footnote-ref role=doc-noteref>3</a></sup>, and used grid search with cross-validation to find a hidden layer size that would be the best. The best model was:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>MLPClassifier(hidden_layer_sizes<span style=color:#f92672>=</span>(<span style=color:#ae81ff>1</span>,), max_iter<span style=color:#f92672>=</span><span style=color:#ae81ff>9999</span>, random_state<span style=color:#f92672>=</span><span style=color:#ae81ff>22</span>, solver<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;lbfgs&#39;</span>)
</span></span></code></pre></div><p>After fitting this model, the evaluating metrics were:</p><ul><li>Accuracy Score: 0.72818</li><li>Precision Score: 0.49133</li><li>Recall Score: 0.68182</li><li>F1 Score: 0.57111</li></ul><h2 id=conclusion>Conclusion<a hidden class=anchor aria-hidden=true href=#conclusion>#</a></h2><p>After running the three models, all of them used the same random_state. I got the following accuracy scores and improvements (compared to the baseline model):</p><p><img loading=lazy src=https://raw.githubusercontent.com/devmedeiros/Challenge-Data-Science/main/3%20-%20Model%20Selection/results_table.png#center alt="results table"></p><p>In the end, the Random Forest had the best metrics overall. This model can <em>recall</em> a great portion of clients that churn correctly, still is not perfect but is certainly a starting point. The <em>accuracy</em> score is not as high as I&rsquo;d like, but in this particular problem, the goal is to keep clients from leaving the company and is better to use resources to keep a client that will not leave than to do nothing.</p><p>In the end, I liked this challenge, because I don&rsquo;t usually practice machine learning, but thanks to the challenge I got the chance to make a small project in this area that is so relevant and important. This was my first time working with neural networks and tunning hyper-parameters, and I&rsquo;m sure the next time I&rsquo;ll get even better results.</p><div class=footnotes role=doc-endnotes><hr><ol><li id=fn:1><p><a href=https://imbalanced-learn.org/stable/references/generated/imblearn.over_sampling.SMOTE.html>imbalanced-learn documentation</a>&#160;<a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:2><p><a href=https://towardsdatascience.com/accuracy-precision-recall-or-f1-331fb37c5cb9>Accuracy, Precision, Recall or F1? - Koo Ping Shung</a>&#160;<a href=#fnref:2 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:3><p><a href=https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html>scikit-learn documentation</a>&#160;<a href=#fnref:3 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li></ol></div></div><footer class=post-footer><ul class=post-tags><li><a href=https://devmedeiros.com/tags/storytelling/>storytelling</a></li><li><a href=https://devmedeiros.com/tags/python/>python</a></li><li><a href=https://devmedeiros.com/tags/data-analysis/>data analysis</a></li><li><a href=https://devmedeiros.com/tags/machine-learning/>machine learning</a></li><li><a href=https://devmedeiros.com/tags/neural-network/>neural network</a></li></ul><nav class=paginav><a class=prev href=https://devmedeiros.com/post/2022-06-14-feature-selection/><span class=title>« Prev Page</span><br><span>Feature Selection</span></a>
<a class=next href=https://devmedeiros.com/post/2022-04-29-ux-power-bi-dashboards/><span class=title>Next Page »</span><br><span>UX/UI for Business Intelligence Dashboards</span></a></nav><div class=share-buttons><a target=_blank rel="noopener noreferrer" aria-label="share Data Science Challenge - Churn Rate on twitter" href="https://twitter.com/intent/tweet/?text=Data%20Science%20Challenge%20-%20Churn%20Rate&url=https%3a%2f%2fdevmedeiros.com%2fpost%2f2022-05-30-churn-rate-challenge%2f&hashtags=storytelling%2cpython%2cdataanalysis%2cmachinelearning%2cneuralnetwork"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM195.519 424.544c135.939.0 210.268-112.643 210.268-210.268.0-3.218.0-6.437-.153-9.502 14.406-10.421 26.973-23.448 36.935-38.314-13.18 5.824-27.433 9.809-42.452 11.648 15.326-9.196 26.973-23.602 32.49-40.92-14.252 8.429-30.038 14.56-46.896 17.931-13.487-14.406-32.644-23.295-53.946-23.295-40.767.0-73.87 33.104-73.87 73.87.0 5.824.613 11.494 1.992 16.858-61.456-3.065-115.862-32.49-152.337-77.241-6.284 10.881-9.962 23.601-9.962 37.088.0 25.594 13.027 48.276 32.95 61.456-12.107-.307-23.448-3.678-33.41-9.196v.92c0 35.862 25.441 65.594 59.311 72.49-6.13 1.686-12.72 2.606-19.464 2.606-4.751.0-9.348-.46-13.946-1.38 9.349 29.426 36.628 50.728 68.965 51.341-25.287 19.771-57.164 31.571-91.8 31.571-5.977.0-11.801-.306-17.625-1.073 32.337 21.15 71.264 33.41 112.95 33.41z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Data Science Challenge - Churn Rate on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&url=https%3a%2f%2fdevmedeiros.com%2fpost%2f2022-05-30-churn-rate-challenge%2f&title=Data%20Science%20Challenge%20-%20Churn%20Rate&summary=Data%20Science%20Challenge%20-%20Churn%20Rate&source=https%3a%2f%2fdevmedeiros.com%2fpost%2f2022-05-30-churn-rate-challenge%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Data Science Challenge - Churn Rate on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fdevmedeiros.com%2fpost%2f2022-05-30-churn-rate-challenge%2f&title=Data%20Science%20Challenge%20-%20Churn%20Rate"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Data Science Challenge - Churn Rate on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fdevmedeiros.com%2fpost%2f2022-05-30-churn-rate-challenge%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Data Science Challenge - Churn Rate on whatsapp" href="https://api.whatsapp.com/send?text=Data%20Science%20Challenge%20-%20Churn%20Rate%20-%20https%3a%2f%2fdevmedeiros.com%2fpost%2f2022-05-30-churn-rate-challenge%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Data Science Challenge - Churn Rate on telegram" href="https://telegram.me/share/url?text=Data%20Science%20Challenge%20-%20Churn%20Rate&url=https%3a%2f%2fdevmedeiros.com%2fpost%2f2022-05-30-churn-rate-challenge%2f"><svg viewBox="2 2 28 28"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></div></footer></article></main><footer class=footer><span>&copy; 2021 - 2022 <a href=https://devmedeiros.com/terms/>Dev_Medeiros</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://git.io/hugopapermod rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerText="copy";function s(){t.innerText="copied!",setTimeout(()=>{t.innerText="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>