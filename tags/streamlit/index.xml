<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Streamlit on devmedeiros</title>
    <link>https://devmedeiros.com/tags/streamlit/</link>
    <description>Recent content in Streamlit on devmedeiros</description>
    <image>
      <title>devmedeiros</title>
      <url>https://devmedeiros.com/cover.png</url>
      <link>https://devmedeiros.com/cover.png</link>
    </image>
    <generator>Hugo -- gohugo.io</generator>
    <lastBuildDate>Sun, 24 Mar 2024 11:31:00 -0300</lastBuildDate><atom:link href="https://devmedeiros.com/tags/streamlit/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>[BTS] Nota Fiscal Goiana</title>
      <link>https://devmedeiros.com/post/bts-nota-fiscal-goiana/</link>
      <pubDate>Sun, 24 Mar 2024 11:31:00 -0300</pubDate>
      
      <guid>https://devmedeiros.com/post/bts-nota-fiscal-goiana/</guid>
      <description>A behind-the-scenes decision on the decision-making process of the project Nota Fiscal Goiana.</description>
      <content:encoded><![CDATA[<h2 id="introduction">Introduction</h2>
<p>The Nota Fiscal Goiana is an economic program implemented by the state of Goiás, Brazil. This program is designed to boost the collection of the Tax on Circulation of Goods and Services (ICMS) by encouraging people to ask for receipts when they buy goods and services within the state. Additionally, the program includes monthly raffles that distribute cash prizes to participating citizens.</p>
<p>As I regularly checked the Nota Fiscal Goiana portal and realized the website&rsquo;s simplicity, I decided to write a scraper to collect the raffle results. Initially, I aimed to simplify the process of checking if I had won, but as I delved deeper, I realized the potential to analyze the dataset more comprehensively.</p>
<h2 id="early-design-and-idea-phase">Early Design and Idea Phase</h2>
<p>The original idea was to scrape raffle results, store them in a database, and give people the option to access the data directly from the database or just view it through a good-looking Power BI dashboard.</p>
<p>This led me to start writing the code for scraping the website and parsing the tables. However, since there were approximately 50 published results not available on the website, I had to resort to checking the Diário Oficial do Estado de Goiás, where the official government documents of Goiás are published in Brazil.</p>
<h3 id="challenges-encountered">Challenges Encountered</h3>
<ul>
<li>
<p>Not all results were completely published on the Diário Oficial, leading to some results being unavailable.</p>
</li>
<li>
<p>The project doesn&rsquo;t have a budget, so automation can&rsquo;t require VM and online databases. Which Power BI requires both.</p>
</li>
</ul>
<h3 id="solutions">Solutions</h3>
<p>The results that I could find published on the Diário Oficial were saved in the database along with results available on the website.</p>
<p>Initially, I was using free options of online databases, and the backup of the old data was on an SQLite available on the project repository. However, I realized that when running the scraper I could save the data on the SQLite and commit the changes, when I did the information persisted on the SQLite file and I could just read it.</p>
<p>To help automate the dashboard I resorted to using Streamlit instead of Power BI. Because Streamlit allows me to read the SQLite file directly from the GitHub repository and every time someone opens the Streamlit dashboard it reads the current data, so I don&rsquo;t need to set up refreshes.</p>
<h2 id="current-state-of-the-project">Current State of the Project</h2>
<p>Currently, the project extracts monthly the raffle results from the Nota Fiscal Goiana and the state ICMS collection and saves the results in the SQLite file, using GitHub Actions. The Streamlit dashboard reads the SQLite file and displays the information on the Streamlit Community Cloud.</p>
<h3 id="lessons-learned">Lessons Learned</h3>
<p>While the Power BI dashboard was useful, I faced challenges with refreshing it easily. Initially using an SQLite database, I couldn&rsquo;t connect to the file on the repository, necessitating manual updates on my local machine. To overcome this, I explored alternatives like MySQL and PostgreSQL, enabling automated monthly scraping with GitHub Actions and updating the dashboard through the Power BI website.</p>
<p>The SQLite database is really helpful and depending on the size and scope of your project, you should look at it as a possible option to store your data.</p>
<h3 id="going-forward">Going forward</h3>
<p>I intend to maintain the project, making future patches to the scraping script if the website changes. And improving the code as my knowledge matures.</p>
<hr>
<h2 id="explore-further">Explore Further</h2>
<ul>
<li>
<p><strong>GitHub Repository:</strong> Access the project&rsquo;s source files and codes on our <a href="https://github.com/devmedeiros/nota-fiscal-goiana/tree/main">GitHub Repository</a>. Currently, everything is in Portuguese, but I plan to translate it in the future.</p>
</li>
<li>
<p><strong>Streamlit Dashboard:</strong> Explore the live Streamlit dashboard <a href="https://nota-fiscal-goiana.streamlit.app/">here</a> to interact with the project&rsquo;s data visualization directly.</p>
</li>
<li>
<p><strong>Academic Article:</strong> If you&rsquo;re interested in a detailed academic analysis of this project, check out our article <a href="http://periodicos.unifacef.com.br/resiget/article/view/2700/1876">here</a>. Please note that the article is in Portuguese.</p>
</li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>Credit Score Classification App</title>
      <link>https://devmedeiros.com/post/credit-score-classification-app/</link>
      <pubDate>Mon, 08 Aug 2022 17:17:00 -0300</pubDate>
      
      <guid>https://devmedeiros.com/post/credit-score-classification-app/</guid>
      <description>Using Streamlit to make a web app that classifies your credit score using Python</description>
      <content:encoded><![CDATA[<h2 id="project-overview">Project Overview</h2>
<p>This project showcase a data science life cycle, where I clean and prepare the dataset, use feature engineering, machine learning, deploy, and data visualization.</p>
<p><img loading="lazy" src="https://ik.imagekit.io/devmedeiros/data-science-cycle_QZwyHaXsP.png?ik-sdk-version=javascript-1.4.3&amp;updatedAt=1659975338736#center" alt="data science cycle of this project in a diagram"  />
</p>
<p>The dataset comes from <a href="https://www.kaggle.com/datasets/parisrohan/credit-score-classification?select=train.csv">kaggle</a>, it has a lot of information about a person&rsquo;s credit and bank details, but it also has a lot of typos, missing data, and censored data. This dataset needed cleaning and also needed some feature engineering, I needed to mutate some features, so they could be read by the model. Thus when presented with categorical data I needed to identify if it was ordinal or nominal, if it was an ordinal variable then it would be mapped to sequential numbers otherwise I&rsquo;d make a dummy. For <em>yes</em> and <em>no</em> variables I choose to make just one dummy, but for types of loans I made one dummy for each loan type and if someone didn&rsquo;t have a loan they simply get 0 on all loan type features. I talk about the process of cleaning and feature engineering on this dataset <a href="/post/data-cleaning-credit-score/">here</a>.</p>
<p>Then I needed a machine learning model that I could predict a person&rsquo;s credit score based on some features. To decide which features I was going to use I based my decision on what is commonly used among real companies, and I also choose variables that I thought made sense. I ended up with the following features:</p>
<ul>
<li>Age</li>
<li>Annual income</li>
<li>Number of bank accounts</li>
<li>Number of credit cards</li>
<li>Number of delayed payments</li>
<li>Credit card utilization ratio</li>
<li>Total EMI paid monthly</li>
<li>Credit history age in months</li>
<li>Loans</li>
<li>Missed any payment in the last 12 months</li>
<li>Paid minimum amount on at least one credit card</li>
</ul>
<p>With the features ready, I moved on to making the model, I decided to use a simple Random Forest, for now, I do intend to work on making this model better, but in this first instance, I wanted to focus on making the streamlit app.</p>
<p>After I finished the model I serialized it and the scaler using the <code>pickle</code> package. To deploy the model and build a visualization I used <a href="https://streamlit.io/">streamlit</a>.</p>
<p><img loading="lazy" src="https://user-images.githubusercontent.com/33239902/183321842-be97fb04-f00b-4b62-8e6e-2b53d25335a0.gif" alt="a gif showing how the streamlit credit score app works"  />
</p>
<p>In this app, you can fill out a form or just select one of the three default profiles given to see how the model evaluates each person&rsquo;s credit score. It also presents how certain the model was by displaying a pie graph with the probability (in percentage) of each credit score group the answers fit. It also shows how much each feature counts towards your credit score, according to this model. You can see the app live <a href="https://devmedeiros-credit-score-classification-appstreamlit-app-fcakrl.streamlitapp.com/">here</a>.</p>
<hr>
<p>All of the code is available at my GitHub <a href="https://github.com/devmedeiros/credit-score-classification-app">repository</a>. Besides the code, there you&rsquo;ll find the documentation, the original and treated data (all the stages of treatment), all the requirements for building this project, and how to run it locally.</p>
]]></content:encoded>
    </item>
    
  </channel>
</rss>
