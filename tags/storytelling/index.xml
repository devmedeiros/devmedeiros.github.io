<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>storytelling on devmedeiros</title>
    <link>https://devmedeiros.com/tags/storytelling/</link>
    <description>Recent content in storytelling on devmedeiros</description>
    <image>
      <title>devmedeiros</title>
      <url>https://devmedeiros.com/cover.png</url>
      <link>https://devmedeiros.com/cover.png</link>
    </image>
    <generator>Hugo -- gohugo.io</generator>
    <lastBuildDate>Tue, 04 Apr 2023 22:30:00 -0300</lastBuildDate><atom:link href="https://devmedeiros.com/tags/storytelling/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>How to use Word clouds?</title>
      <link>https://devmedeiros.com/post/how-to-use-word-clouds/</link>
      <pubDate>Tue, 04 Apr 2023 22:30:00 -0300</pubDate>
      
      <guid>https://devmedeiros.com/post/how-to-use-word-clouds/</guid>
      <description>Word clouds are a popular data visualization tool for analyzing texts, but they need to be presented clearly for the message to be understood.</description>
      <content:encoded><![CDATA[<p>Word clouds are a popular data visualization tool for analyzing texts, comment boxes, and reviews. And it&rsquo;s not popular for no reason, it is an efficient visualization to show the opinion of a group of people in a summarized way, but it is not always presented effectively. So, here are some points to pay attention to when presenting the best word cloud you can.</p>
<h2 id="1--removing-common-and-irrelevant-words">1- Removing common and irrelevant words</h2>
<p>One of the most common words in the English language is the article &ldquo;the&rdquo;. This is not surprising, given that it is essential for forming sentences, but despite being fundamental, when we think in terms of how it helps to explain the theme or idea of a sentence, it does not contribute to it at all. These articles and other common words are called <strong>stopwords</strong>.</p>
<p>So, whenever we work with word clouds, it&rsquo;s important to remove these stopwords to make it clear what your theme is and to reduce information pollution.</p>
<h2 id="2--simplicity-over-complexity">2- Simplicity over complexity</h2>
<p>When working with visual presentations, it&rsquo;s common to want to do something extravagant that catches people&rsquo;s attention, but sometimes complex visualizations can distract people from the story you want to tell and make it difficult to understand your message.</p>
<h2 id="3--segment-your-data">3- Segment your data</h2>
<p>One of the most common uses of word clouds is in customer reviews and company evaluations (such as NPS), in these cases, where we have different &ldquo;types&rdquo; of customers, it&rsquo;s expected that they have very different behaviors, a satisfied or happy customer will have different comments from one who is dissatisfied. So, if you make only one-word cloud without segmenting your data, you may find a word cloud that means nothing more than &ldquo;many people mentioned this&rdquo;.</p>
<p>By segmenting the data by customer ratings or by detractors and promoters, you can identify what customers who are happy with your service are saying and contrast it with what those who are dissatisfied are saying, and identify areas for improvement.</p>
]]></content:encoded>
    </item>
    
    <item>
      <title>Making Better Graphs</title>
      <link>https://devmedeiros.com/post/choose-the-best-graph/</link>
      <pubDate>Tue, 15 Nov 2022 08:01:00 -0300</pubDate>
      
      <guid>https://devmedeiros.com/post/choose-the-best-graph/</guid>
      <description>Choosing the best graph or plot can be a hard task, but it doesn&amp;rsquo;t need to be.</description>
      <content:encoded><![CDATA[<p>Learning how to make better visualizations come with study and practice, today I want to talk about sources you can use to guide you into making better graphs and other data visualizations in general.</p>
<h2 id="from-data-to-viz">From Data to Viz</h2>
<p><img loading="lazy" src="https://i.imgur.com/416qeWj.png" alt="infograph showing different possibilities for choosing a plot"  />
</p>
<p><a href="https://www.data-to-viz.com/">From Data to Viz</a> brings a lot of tools to help navigate the data visualization world. In the website <code>Explore</code> tab you are faced with an interactive infographic that helps you pick a plot, you can choose between: <code>numeric</code>, <code>categoric</code>, <code>numeric&amp;categoric</code>, <code>maps</code>, <code>network</code>, and <code>time series</code>. But it doesn&rsquo;t only show plots that would be good for your type of data, it also shows a description explaining the plot and common mistakes people make when using that specific plot.</p>
<p>At the end of each branch from the infographic, there is also a <code>story</code> link, where you can read about an implementation of that type of plot using real-world data.</p>
<p>And in the case, you are just starting and don&rsquo;t know many terms or are unsure if your data contains <code>too many</code> or <code>few</code> points when you rest your mouse over the options nodes you&rsquo;ll be able to see a description with an example table representing what that means.</p>
<p>By heading to the <code>Caveats</code> tab you&rsquo;re able to see a collection of caveats, you can filter them by:</p>
<ul>
<li>
<p><strong>Improvement:</strong> all caveats that will suggest some sort of improvement on plots.</p>
</li>
<li>
<p><strong>Misleading:</strong> some plots may be misleading by omitting some information, may it be sample size or how proportionally different two categories are. By reading this you&rsquo;ll be able to spot bad practices that lead to confusion or manipulation.</p>
</li>
</ul>
<p>This website also provides links to <strong>Python</strong> and <strong>R</strong> galleries where you can get code snippets to reproduce these plots.</p>
<p>If you prefer to learn by looking at what not to do, you should check out <a href="https://viz.wtf/">WTF Visualizations</a>. It showcases bad visualizations that don&rsquo;t make sense or are somehow misleading.</p>
<p>There you&rsquo;ll find several pie plots that don&rsquo;t add up to 100%, area graphs where the numbers don&rsquo;t match the area, and just straight-up manipulation like the example below.</p>
<figure class="align-center ">
    <img loading="lazy" src="https://64.media.tumblr.com/d8bcf748d564125312aeba4001b96df5/tumblr_qzg4n3nfJY1sgh0voo1_640.jpg#center"
         alt="a donut graph showing &#39;yes&#39; as a majority while it represents 10%, and &#39;no&#39; as a minority while it represents 90%"/> <figcaption>
            <p>piechart from <a href="https://viz.wtf/">WTF Visualizations</a></p>
        </figcaption>
</figure>

]]></content:encoded>
    </item>
    
    <item>
      <title>Data Science Challenge - Churn Rate</title>
      <link>https://devmedeiros.com/post/2022-05-30-churn-rate-challenge/</link>
      <pubDate>Mon, 30 May 2022 16:49:00 -0300</pubDate>
      
      <guid>https://devmedeiros.com/post/2022-05-30-churn-rate-challenge/</guid>
      <description>Alura hosted a four-week Data Science Challenge using an imbalanced dataset of Churn Rate of a company Alura Voz</description>
      <content:encoded><![CDATA[<p>I was challenged to take the role of a new data scientist hired at Alura Voz. This made-up company is a telecommunication company and it needs to reduce the Churn Rate.</p>
<p>The challenge is divided into four weeks. For the first week, the goal was to clean the dataset provided by an API. Next, we need to identify clients who are more likely to leave the company, using data exploration and analysis. Then, in the third week, we made machine learning models to predict the churn rate for Alura Voz. The last week is to show off what we made during the challenge and build our portfolio. In case you are interested in seeing the code for the challenge just head over to my GitHub <a href="https://github.com/devmedeiros/Challenge-Data-Science">repository</a>.</p>
<h2 id="first-week">First Week</h2>
<h3 id="reading-the-dataset">Reading the dataset</h3>
<p>The dataset is available in a JSON file, at first glance it looked like a normal data frame.</p>
<p><img loading="lazy" src="https://raw.githubusercontent.com/devmedeiros/Challenge-Data-Science/main/1%20-%20Data%20Cleaning/table_head.png#center" alt="table head with the first five rows"  />
</p>
<p>But, as we can see, <code>customer</code>, <code>phone</code>, <code>internet</code>, and <code>account</code> are their own separate table. So I had to normalize them separately and then I just concatenated all these tables into one.</p>
<h3 id="missing-data">Missing data</h3>
<p>The first time I looked for missing data in this dataset I notice that apparently, that wasn&rsquo;t anything missing, but later on, I noticed that there was empty space and just space not being counted as <code>NaN</code>. So I corrected this, and now the dataset had 224 missing values for <code>Churn</code> and 11 missing for <code>Charges.Total</code>.</p>
<p>I decided to drop the missing <code>Churn</code> because this is going to be the object of our study and there isn&rsquo;t a point in studying something that doesn&rsquo;t exist. For the missing <code>Charges.Total</code>, I think it represents a customer that hasn&rsquo;t paid anything yet, because all of them had a tenure of 0, meaning that they had just become a client, so I just replaced the missing value for 0.</p>
<h3 id="feature-encoding">Feature Encoding</h3>
<p>The feature <code>SeniorCitizen</code> was the only one that came with <code>0</code> and <code>1</code> instead of <code>Yes</code> and <code>No</code>. For now, I&rsquo;m changing it to yes and no, because it&rsquo;ll make the analysis simpler to read.</p>
<p><code>Charges.Monthly</code> and <code>Charges.Total</code> were renamed to lose the dot because the dot gets in the way when calling the feature in python.</p>
<h2 id="second-week">Second Week</h2>
<h3 id="data-analysis">Data Analysis</h3>
<p>In the first plot, we can see how much unbalanced our data set is. There&rsquo;re over 5000 clients that didn&rsquo;t leave the company and a little less than 2000 that left.</p>
<p><img loading="lazy" src="https://raw.githubusercontent.com/devmedeiros/Challenge-Data-Science/main/2%20-%20Data%20Analysis/churn.jpg#center" alt="bar plot with two bars, the first one is for &amp;rsquo;no&amp;rsquo; and the second is for &amp;lsquo;yes&amp;rsquo;, the first bar is over 5000 count and the second one is around 2000"  />
</p>
<p>I experimented with oversampling the dataset to handle this imbalance, but it made the machine learning models worse. And undersampling isn&rsquo;t an option with this dataset size, so I just decided to leave it the way it is, and when it&rsquo;s time to split the training and test set I&rsquo;ll stratify the dataset by the <code>Churn</code> feature.</p>
<p>I also generated 16 plots for all the discrete data, to see all the plots check this <a href="https://github.com/devmedeiros/Challenge-Data-Science/blob/main/2%20-%20Data%20Analysis/data_analysis.ipynb">notebook</a>. I wanted to see if there was any behavior that made some clients more likely to leave the company. Is clear that all, except for <code>gender</code>, seems to play a role in determining if a client will leave the company or not. More specifically payment methods, contracts, online backup, tech support, and internet service.</p>
<p>In the <code>tenure</code> plot, I decided to make a distribution plot for the tenure, one plot for clients that didn&rsquo;t churn and another for the clients that did churn. We can see that clients that left the company tend to do so at the beginning of their time in the company.</p>
<p><img loading="lazy" src="https://raw.githubusercontent.com/devmedeiros/Challenge-Data-Science/main/2%20-%20Data%20Analysis/tenure.jpg#center" alt="there are two plots side-by-side, in the first one the title is &amp;lsquo;Churn = No&amp;rsquo; the data is along the tenure axis and is in a U shape. the second plot has the title &amp;lsquo;Churn = Yes&amp;rsquo; and starts high and drops fast along the tenure line"  />
</p>
<p>The average monthly charge for clients that didn&rsquo;t churn is 61.27 monetary units, while clients that churn were paying 74.44. This is probably because of the type of contract they prefer, but either way is known that higher prices drive the customers away.</p>
<h3 id="the-churn-profile">The Churn Profile</h3>
<p><img loading="lazy" src="https://64.media.tumblr.com/tumblr_lojvnhHFH91qlh1s6o1_400.gifv#center" alt="person jumping through the window"  />
</p>
<p>Considering everything that I could see through plots and measures. I came up with a profile for clients that are more likely to churn.</p>
<ul>
<li>
<p>New clients are more likely to churn than older clients.</p>
</li>
<li>
<p>Customers that use fewer services and products tend to leave the company. Also, when they aren&rsquo;t tied down to a longer contract they seem to be more likely to quit.</p>
</li>
<li>
<p>Regarding the payment method, clients that churn have a <strong>strong</strong> preference for electronic checks and usually are spending 13.17 monetary units more than the average client that didn&rsquo;t leave.</p>
</li>
</ul>
<h2 id="third-week">Third Week</h2>
<h3 id="preparing-the-dataset">Preparing the dataset</h3>
<p>We start by making dummies variables dropping the first, so we would have n-1 dummies for n categories. Then we move on to look at features correlation.</p>
<p><img loading="lazy" src="https://raw.githubusercontent.com/devmedeiros/Challenge-Data-Science/main/3%20-%20Model%20Selection/corr_matrix.jpg#center" alt="correlation matrix with all the features"  />
</p>
<p>We can see that the <code>InternetService_No</code> feature has a lot of strong correlations with many other features, this is because these other features depend on the client having internet service. So I&rsquo;ll drop all features that are dependent on this one. The same thing happens with <code>PhoneService_Yes</code>.</p>
<p><code>tenure</code> and <code>ChargesTotal</code> also have a strong correlation, so I tried running the models without one of them and both, and it had a worse performance and took a long time to converge, so I decided to keep them as they are relevant as well.</p>
<p>After dropping the features I finish preparing the dataset by normalizing the numeric data, <code>ChargesTotal</code> and <code>tenure</code>.</p>
<h3 id="test-and-training-dataset">Test and training dataset</h3>
<p>I split the dataset into training and testing sets, 20% for testing and the rest for training. I stratified the data by the <code>Churn</code> feature and I shuffle the dataset before splitting. The same split is used by all the models. After splitting the dataset I decided to oversample the <strong>train</strong> data using SMOTE<sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup> because the dataset is imbalanced. The reason that I only used this technique on the training set is that I don&rsquo;t want to have a biased result, oversampling all the datasets would mean that I&rsquo;d be testing my models on the same data that I trained, and that&rsquo;s not the goal here.</p>
<h3 id="model-evaluation">Model Evaluation</h3>
<p>I&rsquo;ll use a dummy classifier to have a baseline model for the accuracy score, and I&rsquo;ll also use the metrics: <code>precision</code>, <code>recall</code> and <code>f1 score</code><sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>. Although the dummy model won&rsquo;t have values for this metrics, I&rsquo;ll keep it for comparison on how much the models improved.</p>
<h3 id="baseline">Baseline</h3>
<p>I made the baseline model using a dummy classifier that guessed that every client behaved the same. It is always guessed that no client will leave the company. By using this approach we got a baseline accuracy score of <code>0.73456</code>.</p>
<p>All models moving forward will have the same random state.</p>
<h3 id="model-1---random-forest">Model 1 - Random Forest</h3>
<p>I start by using a grid search with cross-validation to find the best parameters within a given pool of options using the <code>recall</code> as the strategy to evaluate the performance. The best model was:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>RandomForestClassifier(criterion<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;entropy&#39;</span>, max_depth<span style="color:#f92672">=</span><span style="color:#ae81ff">5</span>, max_leaf_nodes<span style="color:#f92672">=</span><span style="color:#ae81ff">70</span>, random_state<span style="color:#f92672">=</span><span style="color:#ae81ff">22</span>)
</span></span></code></pre></div><p>After fitting this model, the evaluating metrics were:</p>
<ul>
<li>Accuracy Score: 0.72534</li>
<li>Precision Score: 0.48922</li>
<li>Recall Score: 0.78877</li>
<li>F1 Score: 0.60389</li>
</ul>
<h3 id="model-2---linear-svc">Model 2 - Linear SVC</h3>
<p>For this model, I just used the default parameters and set the ceiling for the maximum of iterations to <code>900000</code>.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>LinearSVC(max_iter<span style="color:#f92672">=</span><span style="color:#ae81ff">900000</span>, random_state<span style="color:#f92672">=</span><span style="color:#ae81ff">22</span>)
</span></span></code></pre></div><p>After fitting this model, the evaluating metrics were:</p>
<ul>
<li>Accuracy Score: 0.71966</li>
<li>Precision Score: 0.48217</li>
<li>Recall Score: 0.75936</li>
<li>F1 Score: 0.58982</li>
</ul>
<h3 id="model-3---multi-layer-perceptron">Model 3 - Multi-layer Perceptron</h3>
<p>Here I fixed the solver to LBFGS, because according to the documentation it has a better performance in smaller datasets<sup id="fnref:3"><a href="#fn:3" class="footnote-ref" role="doc-noteref">3</a></sup>, and used grid search with cross-validation to find a hidden layer size that would be the best. The best model was:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>MLPClassifier(hidden_layer_sizes<span style="color:#f92672">=</span>(<span style="color:#ae81ff">1</span>,), max_iter<span style="color:#f92672">=</span><span style="color:#ae81ff">9999</span>, random_state<span style="color:#f92672">=</span><span style="color:#ae81ff">22</span>, solver<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;lbfgs&#39;</span>)
</span></span></code></pre></div><p>After fitting this model, the evaluating metrics were:</p>
<ul>
<li>Accuracy Score: 0.72818</li>
<li>Precision Score: 0.49133</li>
<li>Recall Score: 0.68182</li>
<li>F1 Score: 0.57111</li>
</ul>
<h3 id="conclusion">Conclusion</h3>
<p>After running the three models, all of them used the same random_state. I got the following accuracy scores and improvements (compared to the baseline model):</p>
<p><img loading="lazy" src="https://raw.githubusercontent.com/devmedeiros/Challenge-Data-Science/main/3%20-%20Model%20Selection/results_table.png#center" alt="results table"  />
</p>
<p>In the end, the Random Forest had the best metrics overall. This model can <em>recall</em> a great portion of clients that churn correctly, still is not perfect but is certainly a starting point. The <em>accuracy</em> score is not as high as I&rsquo;d like, but in this particular problem, the goal is to keep clients from leaving the company and is better to use resources to keep a client that will not leave than to do nothing.</p>
<p>In the end, I liked this challenge, because I don&rsquo;t usually practice machine learning, but thanks to the challenge I got the chance to make a small project in this area that is so relevant and important. This was my first time working with neural networks and tunning hyper-parameters, and I&rsquo;m sure the next time I&rsquo;ll get even better results.</p>
<div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p><a href="https://imbalanced-learn.org/stable/references/generated/imblearn.over_sampling.SMOTE.html">imbalanced-learn documentation</a>&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2">
<p><a href="https://towardsdatascience.com/accuracy-precision-recall-or-f1-331fb37c5cb9">Accuracy, Precision, Recall or F1? - Koo Ping Shung</a>&#160;<a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:3">
<p><a href="https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html">scikit-learn documentation</a>&#160;<a href="#fnref:3" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>
]]></content:encoded>
    </item>
    
    <item>
      <title>UX/UI for Business Intelligence Dashboards</title>
      <link>https://devmedeiros.com/post/2022-04-29-ux-power-bi-dashboards/</link>
      <pubDate>Fri, 29 Apr 2022 11:14:00 -0300</pubDate>
      
      <guid>https://devmedeiros.com/post/2022-04-29-ux-power-bi-dashboards/</guid>
      <description>Defining UX/UI in the context of Bussiness Intelligence for Dashboards with examples</description>
      <content:encoded><![CDATA[<h2 id="what-is-uxui">What is UX/UI?</h2>
<p>UX is the acronym for User Experience, it is a recent concept about making design decisions while thinking about the user experience. The UX designer needs to be concerned about whether their product is easy to use and intuitive, making changes to it whenever necessary to suit the user&rsquo;s needs.</p>
<p>UI stands for User Interface. It is everything involved in the interaction of the user and the product. The UI designer is responsible for developing interfaces, not only limited to the visual aspects but also ensuring that the interfaces are functional and usable and generally contribute to a good user experience.</p>
<h2 id="how-to-improve-the-experience-of-bi-dashboards">How to Improve the Experience of BI Dashboards?</h2>
<p>Many people see BI dashboards as web pages. This comes with some expectations. For example, most sites that have some navigation system use a top menu with buttons, a side menu (most common in Brazil being on the left, but in some countries, it is on the right), or a hamburger menu (the one we click on it and the options appear).</p>
<figure>
    <img loading="lazy" src="https://ik.imagekit.io/devmedeiros/site_layout_C0BWt0-Rh.png?ik-sdk-version=javascript-1.4.3&amp;updatedAt=1651180441785"
         alt="Image with three website layouts, the first with a top navigation bar, the second with a left-side menu and the third with a sandwich menu"/> <figcaption>
            <p>Jaqueline Medeiros - All rights reserved</p>
        </figcaption>
</figure>

<p>With this, a majority of people who use the dashboards expect to find navigation buttons and data segmentation (filter) in these places, in addition to other information such as logo and title.</p>
<h3 id="data-segmentation">Data Segmentation</h3>
<p>Also commonly called a data filter, it is a fundamental part of several dashboards, its positioning needs to be defined carefully, because if it is in a place that the user does not expect, it can prevent your dashboard from being used efficiently, in addition to maintaining a visual standard for everyone its filters help people more easily recognize what is and isn&rsquo;t a filter.</p>
<figure>
    <img loading="lazy" src="https://ik.imagekit.io/devmedeiros/base_7_vbzsb9L.png?ik-sdk-version=javascript-1.4.3&amp;updatedAt=1651183332760"
         alt="Image with six filters, distributed in two columns, each one with three. One filter is for a city, the other is for a restaurant and the last one is for reservations? The only difference between them is the design, on the left they are consistent and on the right, each one has a unique design."/> 
</figure>

<p>You can and should use and experiment with different themes in your projects. What matters, when making it easier for users, is consistency, pick a model for your filters with the desired colors and all the graphic specifications that are of interest to you and use it in all filters, as this will help people quickly find it and recognize that it&rsquo;s a filter.</p>
<p><img loading="lazy" src="https://ik.imagekit.io/devmedeiros/borracha_Xwi_TYTHB.png?ik-sdk-version=javascript-1.4.3&amp;updatedAt=1651183773246#center" alt="Three filters with, city, restaurant and accept reservation? Where the city shows an eraser next to the title and the text Clear selections"  />
</p>
<p>It&rsquo;s common for people when they are reading something on the computer to position the mouse pointer where they are reading. For Power BI, this makes it easier for the user to find the clear data segmentation button because when hovering the mouse over the filter name, a rubber appears on the left side where the filter name is. If you use Power BI, you probably already knew that, but we can improve this by using something that the user already knows, which is the <strong>rubber</strong>, and create a button using it as an icon and make this button clear all filters at the same time. If you want to know how to make this button <a href="https://community.powerbi.com/t5/Desktop/Clear-All-Slicers-by-one-button-in-power-bi-desktop/m-p/494518">here</a>, in the Power BI forum, explains how.</p>
<p>This feature needs may be unnoticed at first. Regularly dashboard users don&rsquo;t realize which filters they have used or they use so many that they just want to be able to clear the selection faster and more efficiently, so this button makes the process easier.</p>
<h3 id="page-navigation">Page Navigation</h3>
<p>Power BI&rsquo;s native page navigation is not intuitive for most people and you may want to direct the navigation in a specific flow that facilitates understanding and contributes to the intended storytelling. In this case, we have the option to hide all the report tabs, except for the opening/home page. But what&rsquo;s the best way to direct the user to the other pages? Suppose your report is simple, you have an overview tab and another tab with a breakdown, a simple button would solve your problem, but if your report is extensive it may be unfeasible to put a button for each tab on all pages.</p>
<figure>
    <img loading="lazy" src="https://ik.imagekit.io/devmedeiros/infografico_kQJe_yhQD.png?ik-sdk-version=javascript-1.4.3&amp;updatedAt=1651241421864"
         alt="A circle with four rectangles around it, a line connects the rectangles to the circle" width="100%"/> <figcaption>
            <p>Jaqueline Medeiros - All rights reserved</p>
        </figcaption>
</figure>

<p>In this case, it might be interesting to consider having a home page that leads to all the other pages and placing a <code>home</code> or <code>back</code> button on them.</p>
<h2 id="how-to-improve-dashboards-interface">How to Improve Dashboards Interface?</h2>
<h3 id="prototyping-programs">Prototyping Programs</h3>
<p>Using a specific program to prototype your dashboard allows for greater artistic freedom compared to what Business Intelligence applications typically provide. Figma is a great tool for this, you can create advanced backgrounds and prototypes with amazing quality to use in your BI dashboards.</p>
<p>Here&rsquo;s an example of a dashboard I made a few months ago:</p>
<figure>
    <img loading="lazy" src="https://raw.githubusercontent.com/devmedeiros/Alura-Challenge-BI-2/main/Alura%20Food/alura%20food%20resume.png"
         alt="In the picture you can read the text: Overview of Indian Restaurants Market Dev_Medeiros City Restaurant Accepts Reservations? 19.21 % Restaurants with Online Delivery Average Rating 3.72 R$ 39.48 Average Price per Person 9577 Total Restaurants 5 Most Popular Cities Restaurants 5 Most Popular Cuisine"/> <figcaption>
            Panel with data
        </figcaption>
</figure>

<p>The background of this panel was done completely in Figma, even some of the titles of the BI visuals.</p>
<figure>
    <img loading="lazy" src="https://raw.githubusercontent.com/devmedeiros/Alura-Challenge-BI-2/main/Alura%20Food/Alura%20Food.png"
         alt=" In the image you can read the text: Overview of Indian Restaurants Market Dev_Medeiros City Restaurant Accepts Reservations? % of Restaurants with Online Delivery Average Rating Average Price per Person Total Restaurants 5 Cities with the most Restaurants 5 Most Popular Cuisine"/> <figcaption>
            Background made in Figma
        </figcaption>
</figure>

<p>You can check out more <a href="/post/alura-challenge-bi-2">Power BI Dashboards</a> I made for Alura Challenge BI.</p>
<h3 id="figures-and-icons">Figures and Icons</h3>
<figure class="align-center ">
    <img loading="lazy" src="https://img.freepik.com/vetores-gratis/employees-dando-as-maos-e-ajudando-os-colegas-a-cliir-as-escadas_74855-5236.jpg?w%20=1380&amp;t=st=1651239796~exp=1651240396~hmac=1b2ab902322bbf8ce26e3bc83f602e69075fcc2e18f0afdc91a328c7942ba746#center"
         alt="Vector created by pch.vector - br.freepik.com"/> <figcaption>
            <p>Vector created by pch.vector - br.freepik.com</p>
        </figcaption>
</figure>

<p>Figures and icons when used correctly help make the panel stand out, and make it more eye-catching and beautiful. There are several ways to get images, if you or your team can&rsquo;t create them yourself there is the option of using online platforms that provide vectorized images. On these platforms, there are free options, which require some type of attribution, and premium (paid) options, which often do not need to attribute the author and have a higher quality.</p>
<figure class="align-center ">
    <img loading="lazy" src="https://img.icons8.com/dusk/344/cash-register.png#center"
         alt="Cash register icon"/> <figcaption>
            <p>ICONS8 icon</p>
        </figcaption>
</figure>

]]></content:encoded>
    </item>
    
    <item>
      <title>Storytelling with CEAP</title>
      <link>https://devmedeiros.com/post/2022-04-17-storytelling-with-ceap/</link>
      <pubDate>Sun, 17 Apr 2022 10:08:00 -0300</pubDate>
      
      <guid>https://devmedeiros.com/post/2022-04-17-storytelling-with-ceap/</guid>
      <description>The storytelling of how much each deputy/senator spends in Brazil</description>
      <content:encoded><![CDATA[<h2 id="the-dataset">The Dataset</h2>
<p>CEAP stands for <em>Cota para o Exercício da Atividade Parlamentar</em>, in English, Exercise of Parliamentary Activity from Brazil. The quota is a monthly amount that can be used by the deputy/senator to defray typical expenses of the exercise of the parliamentary mandate.</p>
<p>The monthly amounts don&rsquo;t accumulate over the months. Another important thing is Senators in Brazil have an 8-year mandate, and there are elections every 4 years.</p>
<p>The main dataset is provided by the Brazilian government through the transparency portal.<sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup> I gathered additional data to link the quota for each senator and federal unit.<sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup><sup id="fnref:3"><a href="#fn:3" class="footnote-ref" role="doc-noteref">3</a></sup></p>
<h2 id="data-analysis">Data Analysis</h2>
<h3 id="total-amount-refunded-by-year">Total Amount Refunded by Year</h3>
<p><img loading="lazy" src="https://raw.githubusercontent.com/devmedeiros/7DaysOfCode/main/img/refunded_year.jpg#center" alt="Bar plot with blue bars and green bars indicating election years, shows the total amount refunded by senators in Brazil"  />
</p>
<p>On the image above we can see how much was refunded by year. The year 2022 is the smallest because is not over yet. The data was collected on April 16, 2022.</p>
<p>There seems to be a trend to spend slightly less money on election years.</p>
<p>The first three years were the three lowest spending years, <strong>11.52M</strong>, <strong>11.61M</strong>, and <strong>10.64M</strong> from 2008 to 2010, respectively. This was probably because in the following years the expenses that could be refunded increased in 2011.</p>
<h3 id="top-highest-and-lowest-average-refunded-percentages-by-senators">Top Highest and Lowest Average Refunded Percentages by Senators</h3>
<p>These percentages represent the quota percentage a given senator refunded on average.</p>
<p><img loading="lazy" src="https://raw.githubusercontent.com/devmedeiros/7DaysOfCode/main/img/senator_refund_highest_mean.jpg#center" alt="Horizontal bar plot with the 10 highest refundees, on average, senators"  />
</p>
<p>In the image above, it&rsquo;s seen highest refunded senators note that they’re all closed together, the highest refundee, on average, is João Capiberibe, using <strong>99.78%</strong> of his quota. On the lowest side (image below), it&rsquo;s presented the lowest use of the refund quota, on average. Nailde Panta, is the lowest refundee, on average, using just <strong>4.1%</strong> of her quota.</p>
<p><img loading="lazy" src="https://raw.githubusercontent.com/devmedeiros/7DaysOfCode/main/img/senator_refund_lowest_mean.jpg#center" alt="Horizontal bar plot with the 10 lowest refundees, on average, senators"  />
</p>
<h3 id="expense-types-over-the-years">Expense Types Over the Years</h3>
<p>Here we can see that in the years 2008 to 2010 there wasn’t any refund by transport and private security services.</p>
<p><img loading="lazy" src="https://raw.githubusercontent.com/devmedeiros/7DaysOfCode/main/img/expense_type_year.jpg#center" alt="Bar plot of expenses type refunded by the years 2008 to 2022"  />
</p>
<p>In 2008, the main expenses were publication of parliamentary activity (over <strong>R$ 7,000.00</strong>) and locomotion/accommodation (almost <strong>R$ 6,500.00</strong>). In the subsequent years, those expenses dropped in half, for the publications and to less than <strong>R$ 1,000.00</strong> for locomotion.</p>
<p>In 2013 and 2014 the expense of private security was the highest ever with over <strong>R$ 4,000.00</strong>. Probably due to protests in those years.</p>
<h2 id="to-learn-more">To learn more</h2>
<p>To learn how The Chamber of Deputies from Brazil works click <a href="https://www2.camara.leg.br/english">here</a>.</p>
<p>This is presentation is for learning purposes, from a challenge proposed by Alura #7DaysOfCode. In case you want to know what else can be done with the data, check the <a href="https://serenata.ai/en/">Serenata de Amor</a>. It&rsquo;s a Brazilian project that uses AI to track refunded requests by deputies.</p>
<p>See my code at my GitHub <a href="https://github.com/devmedeiros/7DaysOfCode">repo</a>, there you can also find a slide presentation, learn about how the data was cleaned, and know more about the next task from the challenge.</p>
<h3 id="references">References</h3>
<div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p><a href="https://www12.senado.leg.br/transparencia/dados-abertos-transparencia/dados-abertos-ceaps">https://www12.senado.leg.br/transparencia/dados-abertos-transparencia/dados-abertos-ceaps</a>&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2">
<p><a href="https://pt.wikipedia.org/wiki/Lista_de_senadores_do_Brasil#Nova_Rep%C3%BAblica_(1987%E2%80%932023)">https://pt.wikipedia.org/wiki/Lista_de_senadores_do_Brasil#Nova_Rep%C3%BAblica_(1987%E2%80%932023)</a>&#160;<a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:3">
<p><a href="https://www2.camara.leg.br/legin/int/atomes/2009/atodamesa-43-21-maio-2009-588364-publicacaooriginal-112820-cd-mesa.html">https://www2.camara.leg.br/legin/int/atomes/2009/atodamesa-43-21-maio-2009-588364-publicacaooriginal-112820-cd-mesa.html</a>&#160;<a href="#fnref:3" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>
]]></content:encoded>
    </item>
    
    <item>
      <title>Sentiment Analysis of Fake News vs Real News</title>
      <link>https://devmedeiros.com/post/2021-10-12-fakenews-sentiment/</link>
      <pubDate>Tue, 12 Oct 2021 19:47:00 -0300</pubDate>
      
      <guid>https://devmedeiros.com/post/2021-10-12-fakenews-sentiment/</guid>
      <description>Sentiment Analysis comparing Fake News and Real News using R</description>
      <content:encoded><![CDATA[<p>I want to tackle sentiment analysis using R in a simple way, just to get me started. With this in mind we begin loading all the packages we&rsquo;ll be using.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-r" data-lang="r"><span style="display:flex;"><span><span style="color:#a6e22e">library</span>(readr)
</span></span><span style="display:flex;"><span><span style="color:#a6e22e">library</span>(dplyr)
</span></span><span style="display:flex;"><span><span style="color:#a6e22e">library</span>(tidytext)
</span></span><span style="display:flex;"><span><span style="color:#a6e22e">library</span>(tokenizers)
</span></span><span style="display:flex;"><span><span style="color:#a6e22e">library</span>(stopwords)
</span></span><span style="display:flex;"><span><span style="color:#a6e22e">library</span>(ggplot2)
</span></span></code></pre></div><p>Then we need to load our dataset. This data comes from Kaggle <a href="https://www.kaggle.com/clmentbisaillon/fake-and-real-news-dataset"><em>Fake and real news dataset</em></a>.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-r" data-lang="r"><span style="display:flex;"><span>Fake <span style="color:#f92672">&lt;-</span> <span style="color:#a6e22e">read_csv</span>(<span style="color:#e6db74">&#39;~/fakenews/Fake.csv&#39;</span>)
</span></span><span style="display:flex;"><span>True <span style="color:#f92672">&lt;-</span> <span style="color:#a6e22e">read.csv</span>(<span style="color:#e6db74">&#39;~/fakenews/True.csv&#39;</span>)
</span></span></code></pre></div><p>I want to merge the two datasets, but first we need to create a new column that will tell me where the data came from.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-r" data-lang="r"><span style="display:flex;"><span>Fake<span style="color:#f92672">$</span>news <span style="color:#f92672">&lt;-</span> <span style="color:#e6db74">&#39;fake&#39;</span>
</span></span><span style="display:flex;"><span>True<span style="color:#f92672">$</span>news <span style="color:#f92672">&lt;-</span> <span style="color:#e6db74">&#39;real&#39;</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>data <span style="color:#f92672">&lt;-</span> <span style="color:#a6e22e">rbind</span>(Fake,True)
</span></span></code></pre></div><p>Now we can start the data cleaning. In this first moment, we&rsquo;ll do a simple tokenization on the <strong>title</strong> and <strong>text</strong> variables. Then we&rsquo;ll be removing the stopwords according to the <em>snowball</em> source from the <em>stopwords</em> package.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-r" data-lang="r"><span style="display:flex;"><span>title <span style="color:#f92672">&lt;-</span> <span style="color:#a6e22e">tibble</span>(news <span style="color:#f92672">=</span> data<span style="color:#f92672">$</span>news, text <span style="color:#f92672">=</span> data<span style="color:#f92672">$</span>title)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>corpus <span style="color:#f92672">&lt;-</span> <span style="color:#a6e22e">tibble</span>(news <span style="color:#f92672">=</span> data<span style="color:#f92672">$</span>news, corpus <span style="color:#f92672">=</span> data<span style="color:#f92672">$</span>text)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>tidy_title <span style="color:#f92672">&lt;-</span> title <span style="color:#f92672">%&gt;%</span>
</span></span><span style="display:flex;"><span>  <span style="color:#a6e22e">unnest_tokens</span>(word, text, token <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;words&#39;</span>) <span style="color:#f92672">%&gt;%</span>
</span></span><span style="display:flex;"><span>  <span style="color:#a6e22e">filter</span>(<span style="color:#f92672">!</span>(word <span style="color:#f92672">%in%</span> <span style="color:#a6e22e">stopwords</span>(source <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;snowball&#39;</span>)))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>tidy_corpus <span style="color:#f92672">&lt;-</span> corpus <span style="color:#f92672">%&gt;%</span>
</span></span><span style="display:flex;"><span>  <span style="color:#a6e22e">unnest_tokens</span>(word, corpus, token <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;words&#39;</span>) <span style="color:#f92672">%&gt;%</span>
</span></span><span style="display:flex;"><span>  <span style="color:#a6e22e">filter</span>(<span style="color:#f92672">!</span>(word <span style="color:#f92672">%in%</span> <span style="color:#a6e22e">stopwords</span>(source <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;snowball&#34;</span>)))
</span></span></code></pre></div><p>With the tidy data we can select the ten most frequent words from which title news&rsquo; group.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-r" data-lang="r"><span style="display:flex;"><span>p0 <span style="color:#f92672">&lt;-</span> tidy_title <span style="color:#f92672">%&gt;%</span>
</span></span><span style="display:flex;"><span>  <span style="color:#a6e22e">group_by</span>(news, word) <span style="color:#f92672">%&gt;%</span>
</span></span><span style="display:flex;"><span>  <span style="color:#a6e22e">summarise</span>(n <span style="color:#f92672">=</span> <span style="color:#a6e22e">n</span>()) <span style="color:#f92672">%&gt;%</span>
</span></span><span style="display:flex;"><span>  <span style="color:#a6e22e">arrange</span>(<span style="color:#a6e22e">desc</span>(n)) <span style="color:#f92672">%&gt;%</span>
</span></span><span style="display:flex;"><span>  <span style="color:#a6e22e">slice</span>(<span style="color:#ae81ff">1</span><span style="color:#f92672">:</span><span style="color:#ae81ff">10</span>)
</span></span></code></pre></div><p>Fake news titles mention <em>video</em> and <em>trump</em> by a large margin, 8477 and 7874 respectively. On the real news titles, <em>trump</em> is also one of the most mentioned, coming on first with 4883 appearances, followed by <em>u.s</em>, 4187, and <em>says</em> with 2981.</p>
<p><img loading="lazy" src="https://ik.imagekit.io/devmedeiros/10_popular_titles_H0dG3ljPN.png?updatedAt=1634083210303" alt="10 most frequent words by fake news titles and real news"  title="Top 10 words by fake news and real news"  />
</p>
<p>Now we prepare the data to the sentiment analysis. I&rsquo;m interested in classifing the data into sentiments of joy, anger, fear or surprise, for example. So I&rsquo;ll be using the <code>nrc</code> dataset from <a href="http://saifmohammad.com/WebPages/NRC-Emotion-Lexicon.htm">Saif Mohammad and Peter Turney</a>.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-r" data-lang="r"><span style="display:flex;"><span>p1 <span style="color:#f92672">&lt;-</span> tidy_title <span style="color:#f92672">%&gt;%</span>
</span></span><span style="display:flex;"><span>  <span style="color:#a6e22e">inner_join</span>(<span style="color:#a6e22e">get_sentiments</span>(<span style="color:#e6db74">&#39;nrc&#39;</span>)) <span style="color:#f92672">%&gt;%</span>
</span></span><span style="display:flex;"><span>  <span style="color:#a6e22e">group_by</span>(sentiment, news) <span style="color:#f92672">%&gt;%</span>
</span></span><span style="display:flex;"><span>  <span style="color:#a6e22e">summarise</span>(n <span style="color:#f92672">=</span> <span style="color:#a6e22e">n</span>()) <span style="color:#f92672">%&gt;%</span>
</span></span><span style="display:flex;"><span>  <span style="color:#a6e22e">mutate</span>(prop <span style="color:#f92672">=</span> n<span style="color:#f92672">/</span><span style="color:#a6e22e">sum</span>(n))
</span></span></code></pre></div><p><img loading="lazy" src="https://ik.imagekit.io/devmedeiros/title_sentiment_lkDe-y_p97.png?updatedAt=1634083978487" alt="sentiment from news titles"  title="Sentiment from news titles"  />
</p>
<p><strong>Disgust</strong> seems to be the most common sentiment around fake news titles while <strong>trust</strong> is the lowest, even though it still is more than 50%. Overall fake news titles seems to have more &ldquo;sentiment&rdquo; than real news in this particular dataset. Even positive sentiments like <strong>joy</strong> and <strong>surprise</strong>.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-r" data-lang="r"><span style="display:flex;"><span>p2 <span style="color:#f92672">&lt;-</span> tidy_corpus <span style="color:#f92672">%&gt;%</span>
</span></span><span style="display:flex;"><span><span style="color:#a6e22e">inner_join</span>(<span style="color:#a6e22e">get_sentiments</span>(<span style="color:#e6db74">&#39;nrc&#39;</span>)) <span style="color:#f92672">%&gt;%</span>
</span></span><span style="display:flex;"><span><span style="color:#a6e22e">group_by</span>(sentiment, news) <span style="color:#f92672">%&gt;%</span>
</span></span><span style="display:flex;"><span><span style="color:#a6e22e">summarise</span>(n <span style="color:#f92672">=</span> <span style="color:#a6e22e">n</span>()) <span style="color:#f92672">%&gt;%</span>
</span></span><span style="display:flex;"><span><span style="color:#a6e22e">mutate</span>(prop <span style="color:#f92672">=</span> n<span style="color:#f92672">/</span><span style="color:#a6e22e">sum</span>(n))
</span></span></code></pre></div><p>For the news&rsquo; corpus we can see the same sentiments are prevalent, but the proportion is lower compared to the title. A fake news article loses <strong>trust</strong> when the reader takes more time to read it. It also becames less <strong>negative</strong> and shows less <strong>fear</strong>.</p>
<p><img loading="lazy" src="https://ik.imagekit.io/devmedeiros/corpus_sentiment_302EKlTtO.png?updatedAt=1634083978312" alt="sentiment from news text"  title="Sentiment from news corpus text"  />
</p>
<p>An improvement we could do here is to use our own stopwords and change the way we made the tokens. We had instances were <em>trump</em> and <em>trump&rsquo;s</em> didn&rsquo;t correspond to the same thing and if we had used this data to train a model this could become problematic.</p>
]]></content:encoded>
    </item>
    
  </channel>
</rss>
