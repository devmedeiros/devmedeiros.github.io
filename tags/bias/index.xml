<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>bias on devmedeiros</title>
    <link>https://devmedeiros.com/tags/bias/</link>
    <description>Recent content in bias on devmedeiros</description>
    <image>
      <title>devmedeiros</title>
      <url>https://devmedeiros.com/cover.png</url>
      <link>https://devmedeiros.com/cover.png</link>
    </image>
    <generator>Hugo -- gohugo.io</generator>
    <lastBuildDate>Sat, 01 Oct 2022 16:14:00 -0300</lastBuildDate><atom:link href="https://devmedeiros.com/tags/bias/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Ethics in Data Science</title>
      <link>https://devmedeiros.com/post/ethics-data-science/</link>
      <pubDate>Sat, 01 Oct 2022 16:14:00 -0300</pubDate>
      
      <guid>https://devmedeiros.com/post/ethics-data-science/</guid>
      <description>Data Science is an exciting field, but it&amp;rsquo;s also full of ethical dilemmas.</description>
      <content:encoded><![CDATA[<h2 id="introduction">Introduction</h2>
<p>Data science is an exciting field, but it’s also full of ethical pitfalls. If you want to avoid getting drawn into a situation that could damage your company or client, it’s important to understand the basics of data ethics and how they can affect your work.</p>
<h2 id="learning-from-the-past">Learning from the past</h2>
<p>Learning from the past is important because it can help us avoid making ethical mistakes that others already committed. The following examples are a sampling of some of the most popular data science ethics issues over the years:</p>
<ul>
<li><strong>Google&rsquo;s Street View privacy concerns (2011)</strong></li>
</ul>
<p>In 2011, Google came under fire for collecting personal data from unsecured wireless networks while taking pictures for its Street View service. The company said it was an accident and that it had not intended to collect any information from these Wi-Fi networks. However, computer security experts discovered a code in the software used by Google&rsquo;s cars that indicated that they were designed to do just that.</p>
<p>Google said it had collected data like emails and passwords, but it wasn&rsquo;t sure how much. The fact that Google didn&rsquo;t know what data was collected raised concerns about its commitment to user privacy and security.</p>
<ul>
<li><strong>Facebook&rsquo;s Cambridge Analytica scandal (2018)</strong></li>
</ul>
<p>In March 2018, Facebook revealed that it had been the victim of a massive data breach involving tens of millions of users. The company said that Cambridge Analytica, a political consultancy firm with ties to the Trump presidential campaign, had gained access to information from up to 87 million.</p>
<p>Users were unaware that the app was collecting data from them, and Facebook did not do enough to prevent it. The incident led to intense scrutiny of Facebook&rsquo;s use of user data and its responsibility for protecting users&rsquo; privacy. In April 2018, Facebook CEO Mark Zuckerberg testified before American Congress on the matter. He apologized for his company&rsquo;s mistakes in handling user data and outlined some steps he would take to ensure that similar breaches didn&rsquo;t occur again.</p>
<ul>
<li><strong>IBM’s Photo-scraping scandal (2019)</strong></li>
</ul>
<p>IBM faced a photo-scraping controversial scandal in 2019 where the controversy focused on 1 million pictures of human faces that IBM scrapped from Flickr, the online photo-hosting site.</p>
<p>This scandal brought to light how their data is being used. People aren’t consenting to have their details used for profit.</p>
<ul>
<li><strong>Predictive Policing Software</strong></li>
</ul>
<p>It would be nice to be able to predict crime or be able to estimate where law enforcement needs to be to prevent crime, but many predictions made by this kind of software don’t come true. This happens because these tools are fed bad data.</p>
<blockquote>
<p>“Data collected by police is notoriously bad, easily manipulated, glaringly incomplete, and too often undermined by racial bias.” – <a href="https://www.aclu.org/news/criminal-law-reform/predictive-policing-software-more-accurate">Ezekiel Edwards, ACLU</a></p>
</blockquote>
<p>This polluted data makes the software produce equally contaminated results, leading the software to be better at predicting policing instead of predicting crime, becoming a self-fulfilling prophecy.</p>
<h2 id="conclusion">Conclusion</h2>
<p>These examples show that there are many ways to make mistakes when it comes to handling user data, but they also serve as excellent educational resources for anyone interested in learning more about <em>how</em> these kinds of problems can be avoided in future projects.</p>
<p>There are lots of resources to learn more about this theme and how to combat this issue when working with machine learning models and user data. I&rsquo;ll list some, but remember this is a broad and deep topic, and my research may not find everything there is to it.</p>
<ul>
<li><a href="https://geomblog.github.io/fairness/">A free course on Fairness, Accountability, and Transparency sponsored by GIAN.</a></li>
<li><a href="https://www.youtube.com/watch?v=bXitS_PMyFQ">Why Mathematicians Won&rsquo;t Help Cops - VSauce2</a></li>
<li><a href="https://www.gov.uk/government/publications/data-ethics-framework/data-ethics-framework-2020">Data Ethics Framework - UK Government</a></li>
</ul>
]]></content:encoded>
    </item>
    
  </channel>
</rss>
