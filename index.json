[{"content":"What is UX/UI? UX is the acronym for User Experience, it is a recent concept about making design decisions while thinking about the user experience. The UX designer needs to be concerned about whether their product is easy to use and intuitive, making changes to it whenever necessary to suit the user\u0026rsquo;s needs.\nUI stands for User Interface. It is everything involved in the interaction of the user and the product. The UI designer is responsible for developing interfaces, not only limited to the visual aspects but also ensuring that the interfaces are functional and usable and generally contribute to a good user experience.\nHow to Improve the Experience of BI Dashboards? Many people see BI dashboards as web pages. This comes with some expectations. For example, most sites that have some navigation system use a top menu with buttons, a side menu (most common in Brazil being on the left, but in some countries, it is on the right), or a hamburger menu (the one we click on it and the options appear).\n\r\rJaqueline Medeiros - All rights reserved\n\r\rWith this, a majority of people who use the dashboards expect to find navigation buttons and data segmentation (filter) in these places, in addition to other information such as logo and title.\nData Segmentation Also commonly called a data filter, it is a fundamental part of several dashboards, its positioning needs to be defined carefully, because if it is in a place that the user does not expect, it can prevent your dashboard from being used efficiently, in addition to maintaining a visual standard for everyone its filters help people more easily recognize what is and isn\u0026rsquo;t a filter.\n\r\rYou can and should use and experiment with different themes in your projects. What matters, when making it easier for users, is consistency, pick a model for your filters with the desired colors and all the graphic specifications that are of interest to you and use it in all filters, as this will help people quickly find it and recognize that it\u0026rsquo;s a filter.\nIt\u0026rsquo;s common for people when they are reading something on the computer to position the mouse pointer where they are reading. For Power BI, this makes it easier for the user to find the clear data segmentation button because when hovering the mouse over the filter name, a rubber appears on the left side where the filter name is. If you use Power BI, you probably already knew that, but we can improve this by using something that the user already knows, which is the rubber, and create a button using it as an icon and make this button clear all filters at the same time. If you want to know how to make this button here, in the Power BI forum, explains how.\nThis feature needs may be unnoticed at first. Regularly dashboard users don\u0026rsquo;t realize which filters they have used or they use so many that they just want to be able to clear the selection faster and more efficiently, so this button makes the process easier.\nPage Navigation Power BI\u0026rsquo;s native page navigation is not intuitive for most people and you may want to direct the navigation in a specific flow that facilitates understanding and contributes to the intended storytelling. In this case, we have the option to hide all the report tabs, except for the opening/home page. But what\u0026rsquo;s the best way to direct the user to the other pages? Suppose your report is simple, you have an overview tab and another tab with a breakdown, a simple button would solve your problem, but if your report is extensive it may be unfeasible to put a button for each tab on all pages.\n\r\rJaqueline Medeiros - All rights reserved\n\r\rIn this case, it might be interesting to consider having a home page that leads to all the other pages and placing a home or back button on them.\nHow to Improve Dashboards Interface? Prototyping Programs Using a specific program to prototype your dashboard allows for greater artistic freedom compared to what Business Intelligence applications typically provide. Figma is a great tool for this, you can create advanced backgrounds and prototypes with amazing quality to use in your BI dashboards.\nHere\u0026rsquo;s an example of a dashboard I made a few months ago:\n\r\rPanel with data\r\r\rThe background of this panel was done completely in Figma, even some of the titles of the BI visuals.\n\r\rBackground made in Figma\r\r\rYou can check out more Power BI Dashboards I made for Alura Challenge: Alura Films, Alura Skimo, and Alura Food.\nFigures and Icons \rVector created by pch.vector - br.freepik.com\n\r\rFigures and icons when used correctly help make the panel stand out, and make it more eye-catching and beautiful. There are several ways to get images, if you or your team can\u0026rsquo;t create them yourself there is the option of using online platforms that provide vectorized images. On these platforms, there are free options, which require some type of attribution, and premium (paid) options, which often do not need to attribute the author and have a higher quality.\n\rICONS8 icon\n\r\r","permalink":"https://devmedeiros.com/post/2022-04-29-ux-power-bi-dashboards/","summary":"What is UX/UI? UX is the acronym for User Experience, it is a recent concept about making design decisions while thinking about the user experience. The UX designer needs to be concerned about whether their product is easy to use and intuitive, making changes to it whenever necessary to suit the user\u0026rsquo;s needs.\nUI stands for User Interface. It is everything involved in the interaction of the user and the product.","title":"UX/UI for Business Intelligence Dashboards"},{"content":"The Dataset CEAP stands for Cota para o Exercício da Atividade Parlamentar, in English, Exercise of Parliamentary Activity from Brazil. The quota is a monthly amount that can be used by the deputy/senator to defray typical expenses of the exercise of the parliamentary mandate.\nThe monthly amounts don\u0026rsquo;t accumulate over the months. Another important thing is Senators in Brazil have an 8-year mandate, and there are elections every 4 years.\nThe main dataset is provided by the Brazilian government through the transparency portal.1 I gathered additional data to link the quota for each senator and federal unit.23\nData Analysis Total Amount Refunded by Year On the image above we can see how much was refunded by year. The year 2022 is the smallest because is not over yet. The data was collected on April 16, 2022.\nThere seems to be a trend to spend slightly less money on election years.\nThe first three years were the three lowest spending years, 11.52M, 11.61M, and 10.64M from 2008 to 2010, respectively. This was probably because in the following years the expenses that could be refunded increased in 2011.\nTop Highest and Lowest Average Refunded Percentages by Senators These percentages represent the quota percentage a given senator refunded on average.\nIn the image above, it\u0026rsquo;s seen highest refunded senators note that they’re all closed together, the highest refundee, on average, is João Capiberibe, using 99.78% of his quota. On the lowest side (image below), it\u0026rsquo;s presented the lowest use of the refund quota, on average. Nailde Panta, is the lowest refundee, on average, using just 4.1% of her quota.\nExpense Types Over the Years Here we can see that in the years 2008 to 2010 there wasn’t any refund by transport and private security services.\nIn 2008, the main expenses were publication of parliamentary activity (over R$ 7,000.00) and locomotion/accommodation (almost R$ 6,500.00). In the subsequent years, those expenses dropped in half, for the publications and to less than R$ 1,000.00 for locomotion.\nIn 2013 and 2014 the expense of private security was the highest ever with over R$ 4,000.00. Probably due to protests in those years.\nTo learn more To learn how The Chamber of Deputies from Brazil works click here.\nThis is presentation is for learning purposes, from a challenge proposed by Alura #7DaysOfCode. In case you want to know what else can be done with the data, check the Serenata de Amor. It\u0026rsquo;s a Brazilian project that uses AI to track refunded requests by deputies.\nSee my code at my GitHub repo, there you can also find a slide presentation, learn about how the data was cleaned, and know more about the next task from the challenge.\nReferences   https://www12.senado.leg.br/transparencia/dados-abertos-transparencia/dados-abertos-ceaps\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n https://pt.wikipedia.org/wiki/Lista_de_senadores_do_Brasil#Nova_Rep%C3%BAblica_(1987%E2%80%932023)\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n https://www2.camara.leg.br/legin/int/atomes/2009/atodamesa-43-21-maio-2009-588364-publicacaooriginal-112820-cd-mesa.html\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n   ","permalink":"https://devmedeiros.com/post/2022-04-17-storytelling-with-ceap/","summary":"The Dataset CEAP stands for Cota para o Exercício da Atividade Parlamentar, in English, Exercise of Parliamentary Activity from Brazil. The quota is a monthly amount that can be used by the deputy/senator to defray typical expenses of the exercise of the parliamentary mandate.\nThe monthly amounts don\u0026rsquo;t accumulate over the months. Another important thing is Senators in Brazil have an 8-year mandate, and there are elections every 4 years.","title":"Storytelling with CEAP"},{"content":"What is the p-value? In statistics, we have hypothesis tests, which are made to decide whether to reject or not reject the null hypothesis. Some examples of hypothesis testing are Neyman-Pearson, Shapiro-Wilk, Student\u0026rsquo;s T-test, and others.\nAll hypothesis tests have a test statistic specific to them. And this statistic is used to evaluate the test, but this task can be tiring even with the use of computers because most software does not return the comparison statistic as an output, only the sample statistic. In this case, the p-value comes to facilitate this comparison, as it is already a representation of this test statistic. It represents the probability of obtaining a test statistic equal to or more extreme than the one calculated in your sample, considering the null hypothesis to be true.\nThis makes it easier because by knowing the confidence level that you want to test your hypothesis, you just need to compare if the p-value is smaller or greater than your confidence level, while if you were to use the test statistic it would still be necessary to calculate the statistic for each different confidence level than you would compare. So suppose you want to compare 1%, 5%, and 10%, you would have to calculate three different test statistics to compare your sample statistic.\nHow to use it? It is very common when we are learning statistics on our own, to read that if the p-value is less than 5% we reject the hypothesis or that if it is higher we \u0026ldquo;accept\u0026rdquo; the hypothesis.\nThe value with which we compare the p-value must be defined together with people from the business area you are working with, it is very common in some sectors to use a very small p-value such as 1% or even 0.1% and in others use values ​​greater than 5%.\nNote that a p-value of 0.02 would be rejected if we consider α = 1%, but not if α = 5%. When in doubt about which one to use, it is first recommended to make this decision before taking the test. Second, keep in mind that an α = 1% will have a confidence of 99% (1-α), whereas if it were 5% it would be only 95%. It might seem like it\u0026rsquo;s best to take the value that gives you the most \u0026ldquo;confidence\u0026rdquo;, but a very small p-value can lead to more rejections of your hypothesis.\nThe truth is that hypothesis tests only give us the rejection information, when a null hypothesis is not rejected, it means that no evidence was found that contradicts what it claims, it does not mean that we have proved it to be correct. So you have to be very careful when using the p-value.\nInterpreting the measure A very common way of interpreting the measure is \u0026ldquo;The null hypothesis is rejected, with α% confidence\u0026rdquo;, in the case of rejection (where the p-value \u0026gt; α) and the case of no rejection (p-value \u0026lt; α) \u0026ldquo;Not enough evidence was found to reject the null hypothesis, with α% confidence\u0026rdquo;.\n","permalink":"https://devmedeiros.com/post/2022-04-12-comprehending-the-p-value/","summary":"What is the p-value? In statistics, we have hypothesis tests, which are made to decide whether to reject or not reject the null hypothesis. Some examples of hypothesis testing are Neyman-Pearson, Shapiro-Wilk, Student\u0026rsquo;s T-test, and others.\nAll hypothesis tests have a test statistic specific to them. And this statistic is used to evaluate the test, but this task can be tiring even with the use of computers because most software does not return the comparison statistic as an output, only the sample statistic.","title":"Comprehending the P Value"},{"content":"Tools used: Power BI, Figma, SQL\nCategory: Dashboard\n This dashboard is the last out of three from the second Alura Challenge BI.\n Alura Skimo is interested in analyzing their sales data, to help with this I made this dashboard. It’s composed of three pages. The first one presents a summary of all the main measures you’ll find in the dashboard, filtered for the most recent month. On this first page, you can also rest your mouse cursor over a measure and it’ll show a small plot with the historical series with a tendency line.\nMoving on to the next page, you can see all the information about the products sold by Alura Skimo. You can filter the data by ice cream flavor, type of package, category, and product cost. It displayed the basic information about the products, along with a rank of the best-selling products and a couple of plots showing the sales by flavor and sales by category.\nOn the last page, you can find information about the salesperson from our company. It shows when they joined the company, how much is their cut, how much revenue they got and how many sales each one made, everything in the last year (2018).\nThis dashboard is complex next to the other two because our dataset came from SQL files. First, I had to create the database and load each SQL file into it, then all I had to do was load it on Power BI. At last, I did all the data cleaning and processing on it.\nIf you want to look at the project files and code you can find it in my github repository.\n","permalink":"https://devmedeiros.com/post/2022-03-08-alura-skimo-powerbi/","summary":"Tools used: Power BI, Figma, SQL\nCategory: Dashboard\n This dashboard is the last out of three from the second Alura Challenge BI.\n Alura Skimo is interested in analyzing their sales data, to help with this I made this dashboard. It’s composed of three pages. The first one presents a summary of all the main measures you’ll find in the dashboard, filtered for the most recent month. On this first page, you can also rest your mouse cursor over a measure and it’ll show a small plot with the historical series with a tendency line.","title":"Alura Skimo - Power BI Dashboard"},{"content":"Tools used: Power BI, Google Sheets, Figma\nCategory: Dashboard\n This dashboard is the second of three that I'm making in the following weeks as part of the second Alura Challenge BI.\n Alura Food is interested in expanding its business by entering the Indian market. To do so, the company asked to calculate measures to help them make a better decision.\nFirst of all, I merged the datasets and cleaned the data through Power BI, translated a few texts from English to Portuguese through Google Sheets, and converted the meals price from their respective currency value to BRL (Brazillian Currency). Finally, I used Figma to make the background, including images and titles.\nIn this project, I opted to used a single page to display all the requested information as I believe it makes it easier to analyze the data.\nMost restaurants in this Indian market don't offer online delivery, with just 19.21% having it. The average rating for the restaurants is 3.72 out of a maximum of 5, the rating is also presented as text, with a Muito Bom (Very Good) being a score over 4.\nThe average price of a meal per person is around R$ 39.48 (around USD 7.65). And there are 9577 different restaurants in the dataset, of which 3968, specialize in North Indian cuisine. New Delhi is, by far, the most popular choice to open a restaurant with 5473, the second closest city is Gurgaon, with 1118 restaurants.\nIf you want to look at the project files and code you can find it in my github repository.\n","permalink":"https://devmedeiros.com/post/2022-02-26-alura-food-powerbi/","summary":"Tools used: Power BI, Google Sheets, Figma\nCategory: Dashboard\n This dashboard is the second of three that I'm making in the following weeks as part of the second Alura Challenge BI.\n Alura Food is interested in expanding its business by entering the Indian market. To do so, the company asked to calculate measures to help them make a better decision.\nFirst of all, I merged the datasets and cleaned the data through Power BI, translated a few texts from English to Portuguese through Google Sheets, and converted the meals price from their respective currency value to BRL (Brazillian Currency).","title":"Alura Food - Power BI Dashboard"},{"content":"Tools used: Power BI, Google Sheets, Figma\nCategory: Dashboard\n This dashboard is the first of three that I'll be making in the following weeks as part of the second Alura Challenge BI.\n The objective of this dashboard is to help find the best selection for an upcoming movie.\nWith this in mind, I explored in the first tab a little summary about the movies in the database, with this you can get a general view of our data. The second tab is displayed the IMDB and Meta Score of the movies. I also created a measure to show how much both measurements agree. A score of 100 means doesn't agree at all and 0 means completely agree, the agreeable score was 9.48.\nOur third tab shows information about the movie stars, the top 10 actors with the highest movie gross revenue, and the top 10 actors with the biggest movie count.\nThe fourth tab presents the distribution of gross revenue by how many different genres a movie has. It also shows how many movies have a given genre, for example, Drama is the most popular choice for movie genre, with 72% of movies in the dataset. At last, this tab also shows the mean gross revenue by genre.\nThe fifth and last tab shows a bit of information about the Brazillian Parental Guide and shows the average Meta Score for movies by the parental guide certification. It also shows how much gross revenue, on average, each parental guide certification earned.\n","permalink":"https://devmedeiros.com/post/2022-02-17-alura-films-powerbi/","summary":"Tools used: Power BI, Google Sheets, Figma\nCategory: Dashboard\n This dashboard is the first of three that I'll be making in the following weeks as part of the second Alura Challenge BI.\n The objective of this dashboard is to help find the best selection for an upcoming movie.\nWith this in mind, I explored in the first tab a little summary about the movies in the database, with this you can get a general view of our data.","title":"Alura Films - Power BI Dashboard"},{"content":"The show is presented by Tyler Renelle from Depth, since 2021 it has been redone to update the content. The podcast offers a resource list where you can find all of the books, courses, and sites mentioned during the podcast. It\u0026rsquo;s a great podcast for anyone who is learning about machine learning, it can be useful for a complete beginner, an enthusiast, or someone who is looking to further even more on this topic.\nThe show starts by talking about what is data science, how it relates to machine learning and artificial intelligence. It also talks about the first attempts humans had made to create AI, with some examples coming from as early as the 13th century.\nI like that the show talks about concepts of machine learning in a different way than I\u0026rsquo;m used to. When people are talking about machine learning, as a Statistician, I always feel that I understand what people are talking about, except I never knew the keywords they were using. What machine learning engineers might call features I was taught as variable, so I got confused when talking to friends about machine learning when they came from a different background as me (e.g. computer science). This is one of the reasons why I like the podcast so much because it talks about deep details about every machine learning concept so if it is talking about something that I already know I can now make that connection.\n","permalink":"https://devmedeiros.com/post/2022-01-24-podcast-review-mlg/","summary":"The show is presented by Tyler Renelle from Depth, since 2021 it has been redone to update the content. The podcast offers a resource list where you can find all of the books, courses, and sites mentioned during the podcast. It\u0026rsquo;s a great podcast for anyone who is learning about machine learning, it can be useful for a complete beginner, an enthusiast, or someone who is looking to further even more on this topic.","title":"Podcast Review - Machine Learning Guide"},{"content":"Recently I finished an Alura course named Python for Data Science and I want to put what I learned into practice, to do so I\u0026rsquo;ll make a descriptive analysis on this dataset Amazon Top 50 Bestselling Books 2009 - 2019. It contains 550 books and the data has been categorized as fiction and non-fiction by Goodreads. All of the code can be found here.\nI started checking the first five observations from the dataset.\n   Name Author User Rating Reviews Price Year Genre     10-Day Green Smoothie Cleanse JJ Smith 4.7 17350 8 2016 Non Fiction   11/22/63: A Novel Stephen King 4.6 2052 22 2011 Fiction   12 Rules for Life: An Antidote to Chaos Jordan B. Peterson 4.7 18979 15 2018 Non Fiction   1984 (Signet Classics) George Orwell 4.7 21424 6 2017 Fiction   5,000 Awesome Facts (About Everything!) (Natio\u0026hellip; National Geographic Kids 4.8 7665 12 2019 Non Fiction    Here it\u0026rsquo;s possible to see that the data has the Year in which the book was on the top 50 list, it\u0026rsquo;s Price, the average User Rating, total Reviews, Author, Name and lastly, Genre.\nThere are no null values in the dataset. And from 550 books there are 248 unique authors, so let\u0026rsquo;s see which authors have had more books in the top 50 bestselling during this period.\n   Author Number of Books     Jeff Kinney 12   Gary Chapman 11   Rick Riordan 11   Suzanne Collins 11   American Psychological Association 10   Dr. Seuss 9   Gallup 9   Rob Elliott 8   Stephen R. Covey 7   Stephenie Meyer 7   Dav Pilkey 7   Bill O\u0026rsquo;Reilly 7   Eric Carle 7    The author with more books in the top 50 list was Jeff Kinney, tied at second, with 11 books, was Gary Chapman, Rick Riordan, and Suzanne Collins. Tied at 9th is Stephen R. Covey, Stephenie Meyer, Dav Pilkey, Bill O\u0026rsquo;Reilly, and Eric Carle, with 7 books.\nWith the violing plot, we can see how the user rating is concentrated and because our data is composed of bestsellers it makes sense that the user rating is mostly concentrated around 4.5 and 4.75.\nThis boxplot of reviews count by year shows that the variability increases through the years, having its peak at 2014 and gradually stabilizing. We can also see that in the first years, 2010 and 2011, there were more outliers in the data.\nI wanted to look at the user rating and price by book genre. So I calculated these average values.\n   Genre User Rating Price     Fiction 4.65 10.85   Non Fiction 4.60 14.84    The user rating average by genre seems to be similar just 0.05 difference, but the price has a bigger difference 10.85 for fiction and 14.84 for non-fiction books. To be sure that these differences are statistically significant I\u0026rsquo;ll use the Mann-Whitney test.\nThe Mann-Whitney null hypothesis is that the samples have the same distribution, and in both cases, we reject the null hypothesis with a 95% confidence level. The p-value for the price data was 8.34e-08 and the user rating was 1.495e-07.\nTo visually show how different their distribution is we can take a look at the following plots.\nThe distribution for the price of fiction books is heavily inclined to the left and consistently diminishes as the price goes up. While the non-fiction books price starts high and becomes even higher, 120 and almost 140 occurrences in the first two categories, then it rapidly diminishes.\nThe distribution for the user rating by the fiction genre slowly increases, having its peek at around 4.8. And the distribution of the non-fiction genre has its peak at a little over 4.6.\n","permalink":"https://devmedeiros.com/post/2021-12-28-amazon-top-50-books/","summary":"Recently I finished an Alura course named Python for Data Science and I want to put what I learned into practice, to do so I\u0026rsquo;ll make a descriptive analysis on this dataset Amazon Top 50 Bestselling Books 2009 - 2019. It contains 550 books and the data has been categorized as fiction and non-fiction by Goodreads. All of the code can be found here.\nI started checking the first five observations from the dataset.","title":"Descriptive Analysis of Amazon Top 50 Bestselling Books 2009 - 2019"},{"content":"Introduction I\u0026rsquo;m learning data visualization in Python and I see myself as a \u0026lsquo;hands on\u0026rsquo; learner, so I\u0026rsquo;ll be reproducing some basic plots using seaborn package that you can use as a reference everytime you need to fresh up your memory.\nAt first is required that the packages are properly imported, after that I load the iris dataset.\nimport pandas as pd import seaborn as sns import matplotlib.pyplot as plt  url = \u0026#34;https://git.io/JXciW\u0026#34;  iris = pd.read_csv(url) If you\u0026rsquo;re not familiar with the iris dataset, you can see its first five rows below:\n   sepal_length sepal_width petal_length petal_width species     5.1 3.5 1.4 0.2 setosa   4.9 3.0 1.4 0.2 setosa   4.7 3.2 1.3 0.2 setosa   4.6 3.1 1.5 0.2 setosa   5.0 3.6 1.4 0.2 setosa    Barplots To create simple barplots.\nsns.barplot(x=\u0026#34;species\u0026#34;, y=\u0026#34;petal_width\u0026#34;, data=iris) Making a horizontal barplot.\nsns.barplot(x=\u0026#34;petal_width\u0026#34;, y=\u0026#34;species\u0026#34;, data=iris) Custom bar order.\nsns.barplot(  x=\u0026#34;species\u0026#34;,  y=\u0026#34;petal_width\u0026#34;,  data=iris,  order=[\u0026#34;virginica\u0026#34;, \u0026#34;setosa\u0026#34;, \u0026#34;versicolor\u0026#34;]) Add caps to error bars.\nsns.barplot(x=\u0026#34;species\u0026#34;, y=\u0026#34;petal_width\u0026#34;, data=iris, capsize=.2) Barplot withough error bar.\nsns.barplot(x=\u0026#34;species\u0026#34;, y=\u0026#34;petal_width\u0026#34;, data=iris, ci=None) Scatterplots A simple scatterplot.\nsns.scatterplot(x=\u0026#34;sepal_width\u0026#34;, y=\u0026#34;petal_width\u0026#34;, data=iris) Mapping groups to scatterplot.\nsns.scatterplot(x=\u0026#34;sepal_width\u0026#34;, y=\u0026#34;petal_width\u0026#34;, data=iris, hue=\u0026#34;species\u0026#34;) Mapping groups and scalling scatterplot.\nsns.scatterplot(  x=\u0026#34;sepal_width\u0026#34;,  y=\u0026#34;petal_width\u0026#34;,  data=iris,  hue=\u0026#34;sepal_length\u0026#34;,  size=\u0026#34;sepal_length\u0026#34;) Legend and Axes To change the plot legend to the outside of the plot area, you can use bbox_to_anchor = (1,1), loc=2. The following plot has a custom title, a new x axis label, and a y axis label.\nsns.scatterplot(x=\u0026#34;sepal_width\u0026#34;, y=\u0026#34;petal_width\u0026#34;, data=iris, hue=\u0026#34;species\u0026#34;) plt.legend(  title=\u0026#34;Species\u0026#34;,  bbox_to_anchor = (1,1),  loc=2) plt.xlabel(\u0026#34;Sepal Width\u0026#34;) plt.ylabel(\u0026#34;Petal Width\u0026#34;) plt.title(\u0026#34;Sepal Width x Petal Width\u0026#34;) ","permalink":"https://devmedeiros.com/post/2021-11-07-seaborn-package-guide/","summary":"Introduction I\u0026rsquo;m learning data visualization in Python and I see myself as a \u0026lsquo;hands on\u0026rsquo; learner, so I\u0026rsquo;ll be reproducing some basic plots using seaborn package that you can use as a reference everytime you need to fresh up your memory.\nAt first is required that the packages are properly imported, after that I load the iris dataset.\nimport pandas as pd import seaborn as sns import matplotlib.pyplot as plt  url = \u0026#34;https://git.","title":"Python seaborn Package Guide"},{"content":"Tools used: SQL, R\nCategory: Data Analysis\n Introduction COVID-19 has changed many things in our day-to-day life, from ordering more delivery, working from home, or even just getting a new pet. And now I want to see if it has impacted traffic crashes in California. The dataset that I’ll be using comes from Kaggle. It covers collisions from January 2001 up to December 2020 from California. 1 The following are the tables present in the dataset:\n case_id collisions parties victims  The case_id contains the case_id and db_year. The collisions table contains all the information about each collision. While the parties table contains information about all the parties involved in the collisions, in this case, parties can be drivers, pedestrians, cyclists, and parked vehicles. The victims table contain information about all the victims, it also includes passengers.\nTo answer if the pandemic has impacted the collisions I need to separate my data. Knowing that the first case of COVID was on January 26, 2020, in California. 2 I’ll be separating the dataset as before COVID for every crash that happened before the first case and after COVID for every crash on the same day and forward.\nTo compare before COVID and after COVID I’ll looking at a couple of things, Proportion of DUIs and Fatality of Crashes.\nTo do this I’ll be querying the database with SQLite through R. For the complete code go to my GitHub repo.\nProportion of DUIs Alcohol use has changed in the US during the pandemic 3 and with all the lockdowns and working from home we may wonder if this alcohol use has caused more crashes or not. With this in mind we take a look at our data about California.\nAs we can see from the following table, the percentages before and after COVID are very similar. Only 7.3% and 8.96% of violations occurred from DUIs.\n    Before COVID  After COVID      Violation Qnty Perc Qnty Perc   DUI 653467 7.3 42169 8.96   Other 8300399 92.7 428299 91.04    Fatality of Crashes Moving on to our next task, to see if crashes after COVID became more or less fatal. The analysis is going to look at how many people died from collisions, how many got some injury, and no injury at all.\nLooking at the table below you can see that the percentage of collisions that resulted in someone dying was 0.74% before the pandemic and is now 1.29%, is not a big difference, but when you look at no injury, you can see that before COVID 45.24% of collisions didn\u0026rsquo;t result in someone having an injury and after the pandemic, this number drops to 18.57%. This could indicate some influence of the pandemic, but further research is needed.\n    Before COVID  After COVID      Degree of Injury Qnty Perc Qnty Perc   Death 68875 0.74 4131 1.29   Some injury 5033610 54.02 257057 80.14   No injury 4216085 45.24 59576 18.57    References   Alexander Gude and California Highway Patrol, “California Traffic Collision Data from SWITRS.” Kaggle, 2021, doi: 10.34740/KAGGLE/DSV/2569326.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n Timeline of Coronavirus US\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n Pollard MS, Tucker JS, Green HD. Changes in Adult Alcohol Use and Consequences During the COVID-19 Pandemic in the US. JAMA Netw Open. 2020;3(9):e2022942. doi:10.1001/jamanetworkopen.2020.22942\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n  ","permalink":"https://devmedeiros.com/post/2021-11-02-sql-california-traffic/","summary":"\u003cp\u003e\u003cstrong\u003eTools used:\u003c/strong\u003e SQL, R\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eCategory:\u003c/strong\u003e Data Analysis\u003c/p\u003e\n\u003chr\u003e","title":"Have COVID Impacted California Traffic Collisions?"},{"content":"The data.table package is one of the fastest packages for data manipulation, currently, it is even faster than pandas and dplyr 1. data.table syntax is dt[i, j, by], where:\n i is used to subset rows j is used to subset columns by is used to subset groups, like GROUP BY from SQL  You can read it out loud as2:\n Take dt, subset/reorder rows using i, then calculate j, grouped by by.\n A data.table is also a data.frame and all of the basic data manipulations you can use in data.frames applies to data.table. Like ncol(), nrow(), names(), summary(). But it has more possibilities, for instance in data.table there is a special variable .N which is an integer that contains the row number in the group. If you use dt[.N] you\u0026rsquo;ll get the last row of your data.table.\nAnother cool feature of data.table is that if you want filter/subset a column you don\u0026rsquo;t need to use df$x[df$x == 1] you can simple use dt[x == 1] which make your code much more readable and clean.\nYou also get to use special operators: %like%, %in% and %between%. These operators work like SQL operators, LIKE, IN, and BETWEEN, respectively.\nIf you are familiar with SQL there is this one thing the package offers that will catch your eye. It\u0026rsquo;s called chaining, which allows you to perform a sequence of operations in a data.table, you just need to use dt[][] and chain multiple operations \u0026ldquo;[]\u0026rdquo;.\nBut that\u0026rsquo;s not all with the operator := you can alter data without making a new copy in the memory.\nIf you want to start using the package I suggest you use the cheatsheet. It\u0026rsquo;s really useful if you already have a basic knowledge about data.frames.\n  https://h2oai.github.io/db-benchmark/\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n https://cran.r-project.org/web/packages/data.table/vignettes/datatable-intro.html\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n   ","permalink":"https://devmedeiros.com/post/2021-10-27-data-table/","summary":"The data.table package is one of the fastest packages for data manipulation, currently, it is even faster than pandas and dplyr 1. data.table syntax is dt[i, j, by], where:\n i is used to subset rows j is used to subset columns by is used to subset groups, like GROUP BY from SQL  You can read it out loud as2:\n Take dt, subset/reorder rows using i, then calculate j, grouped by by.","title":"An Overview on data.table R Package"},{"content":"Tools used: R, ggplot, Shiny\nCategory: Simulation\n I got inspired to simulate a blackjack game, so I decided to make a series of functions to emulate the dealer\u0026rsquo;s behavior, a newbie player, a cautious player, and a strategist. With this set of functions, you can run a single game with p players, d decks, any combination of players archetypes. And you can also run it n times.\nI also made a shiny app to display how the simulation works. In the app, you are limited by the number of players, but if you wish to run the code with more players you can check the GitHub repository. There you can find the rules considered for the simulation and the complete code.\nIf you are familiar with the R language you can also run the app locally, you just need to run the library(shiny) and runGitHub(\u0026quot;blackjack-simulation\u0026quot;, \u0026quot;devmedeiros\u0026quot;, ref = \u0026quot;main\u0026quot;).\nThe app is composed of a sidebar with a space to choose the archetypes, the number of decks to use, and how many rounds you want to see simulated. Depending on how many rounds you choose it can get slow as it\u0026rsquo;ll simulate according to your choices every time you hit the RUN SIMULATION RUN.\nIn the plot tab, we have the evolution of the lose rate through the rounds.\nA 1 means the player lost that round and a 0 means the player won.\nThe game setup tab shows all the cards dealt in the simulation, each card was dealt from left to right and a blank cell means the player didn\u0026rsquo;t ask for another card (hit).\nAnd lastly, the lose rate tab shows us the same data from the plot tab, but now as a table. This is useful if you want to take a deeper look at how one strategy was better than the other.\n","permalink":"https://devmedeiros.com/post/2021-10-24-blackjack-simulation/","summary":"\u003cp\u003e\u003cstrong\u003eTools used:\u003c/strong\u003e R, ggplot, Shiny\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eCategory:\u003c/strong\u003e Simulation\u003c/p\u003e\n\u003chr\u003e","title":"Blackjack Simulation"},{"content":"I want to tackle sentiment analysis using R in a simple way, just to get me started. With this in mind we begin loading all the packages we\u0026rsquo;ll be using.\nlibrary(readr) library(dplyr) library(tidytext) library(tokenizers) library(stopwords) library(ggplot2) Then we need to load our dataset. This data comes from Kaggle Fake and real news dataset.\nFake \u0026lt;- read_csv(\u0026#39;~/fakenews/Fake.csv\u0026#39;) True \u0026lt;- read.csv(\u0026#39;~/fakenews/True.csv\u0026#39;) I want to merge the two datasets, but first we need to create a new column that will tell me where the data came from.\nFake$news \u0026lt;- \u0026#39;fake\u0026#39; True$news \u0026lt;- \u0026#39;real\u0026#39;  data \u0026lt;- rbind(Fake,True) Now we can start the data cleaning. In this first moment, we\u0026rsquo;ll do a simple tokenization on the title and text variables. Then we\u0026rsquo;ll be removing the stopwords according to the snowball source from the stopwords package.\ntitle \u0026lt;- tibble(news = data$news, text = data$title)  corpus \u0026lt;- tibble(news = data$news, corpus = data$text)  tidy_title \u0026lt;- title %\u0026gt;%  unnest_tokens(word, text, token = \u0026#39;words\u0026#39;) %\u0026gt;%  filter(!(word %in% stopwords(source = \u0026#39;snowball\u0026#39;)))  tidy_corpus \u0026lt;- corpus %\u0026gt;%  unnest_tokens(word, corpus, token = \u0026#39;words\u0026#39;) %\u0026gt;%  filter(!(word %in% stopwords(source = \u0026#34;snowball\u0026#34;))) With the tidy data we can select the ten most frequent words from which title news\u0026rsquo; group.\np0 \u0026lt;- tidy_title %\u0026gt;%  group_by(news, word) %\u0026gt;%  summarise(n = n()) %\u0026gt;%  arrange(desc(n)) %\u0026gt;%  slice(1:10) Fake news titles mention video and trump by a large margin, 8477 and 7874 respectively. On the real news titles, trump is also one of the most mentioned, coming on first with 4883 appearances, followed by u.s, 4187, and says with 2981.\nNow we prepare the data to the sentiment analysis. I\u0026rsquo;m interested in classifing the data into sentiments of joy, anger, fear or surprise, for example. So I\u0026rsquo;ll be using the nrc dataset from Saif Mohammad and Peter Turney.\np1 \u0026lt;- tidy_title %\u0026gt;%  inner_join(get_sentiments(\u0026#39;nrc\u0026#39;)) %\u0026gt;%  group_by(sentiment, news) %\u0026gt;%  summarise(n = n()) %\u0026gt;%  mutate(prop = n/sum(n)) Disgust seems to be the most common sentiment around fake news titles while trust is the lowest, even though it still is more than 50%. Overall fake news titles seems to have more \u0026ldquo;sentiment\u0026rdquo; than real news in this particular dataset. Even positive sentiments like joy and surprise.\np2 \u0026lt;- tidy_corpus %\u0026gt;% inner_join(get_sentiments(\u0026#39;nrc\u0026#39;)) %\u0026gt;% group_by(sentiment, news) %\u0026gt;% summarise(n = n()) %\u0026gt;% mutate(prop = n/sum(n)) For the news\u0026rsquo; corpus we can see the same sentiments are prevalent, but the proportion is lower compared to the title. A fake news article loses trust when the reader takes more time to read it. It also becames less negative and shows less fear.\nAn improvement we could do here is to use our own stopwords and change the way we made the tokens. We had instances were trump and trump\u0026rsquo;s didn\u0026rsquo;t correspond to the same thing and if we had used this data to train a model this could become problematic.\n","permalink":"https://devmedeiros.com/post/2021-10-12-fakenews-sentiment/","summary":"I want to tackle sentiment analysis using R in a simple way, just to get me started. With this in mind we begin loading all the packages we\u0026rsquo;ll be using.\nlibrary(readr) library(dplyr) library(tidytext) library(tokenizers) library(stopwords) library(ggplot2) Then we need to load our dataset. This data comes from Kaggle Fake and real news dataset.\nFake \u0026lt;- read_csv(\u0026#39;~/fakenews/Fake.csv\u0026#39;) True \u0026lt;- read.csv(\u0026#39;~/fakenews/True.csv\u0026#39;) I want to merge the two datasets, but first we need to create a new column that will tell me where the data came from.","title":"Sentiment Analysis of Fake News vs Real News"},{"content":"Hi! My name is Jaqueline Medeiros and I am a Data Scientist from Brazil. I have a degree in Statistics and most of my experience has been with data analysis and visualization. Even so, I\u0026rsquo;ve been showing interest in data engineering, I\u0026rsquo;ve been participating in projects involving this branch of data science. I\u0026rsquo;m currently working on implementing PySpark and Apache Hop to make data cleaning faster and more efficient.\nEven though I want to focus on data engineering, I\u0026rsquo;m still interested in machine learning, data analysis, and other things under the data science umbrella. So I\u0026rsquo;ll still be posting about data science in general.\nYou can find me at:\n Linkedin: medeiros-jaqueline Github: devmedeiros  Resume Data Scientist who is a member of the Innovation Laboratory of the Regional Electoral Court of Goiás. On my day-to-day, I work with data processing and cleaning, data ingestion from several sources, making views, and Business Intelligence applications.\nEducation Federal University of Goiás - UFG - Goiânia, GO\nSpecialization - Postgraduate Degree in Database with Big Data (Apr 2020 - present)\nFederal University of Goiás - UFG - Goiânia, GO\nBachelor in Statistics (Completed in December 2020)\n  Courses: Linear Regression, Logistic Regression, Time Series, Nonparametric Statistics, Statistical Inference, Multivariate Analysis, and Probability.   Experience Regional Electoral Court of Goiás - Goiânia, GO\nStatistician - Data Scientist (Jun 2020 – present)\n  I use analytical and technical expertise to generate insights that help improve the court\u0026rsquo;s governance, productivity, and transparency, leveraging its CNJ Quality Award score from 70.28% in 2020 to 80.72% in 2021. Identify bottlenecks in the electoral process. Data ingestion from multiple sources using a combination of SQL, R, and Python. Generating views to be used in Power BI dashboards.   Projects, Internships, and College Positions GRUPOM Consulting and Research - Goiânia, GO\nOpinion Researcher Internship (Apr 2019 – May 2020)\n  Planning and implementation of electoral, opinion, and market surveys. Data processing and cleaning using Excel and R. Preparation and presentation of static and interactive Business Intelligence reports.   University Monitoring 3rd\nComputational Statistics I (1st Semester 2018)\n  Assist the teacher in classes, assemble answer templates for lists of exercises for the teacher\u0026rsquo;s exercises. Helping students with the content presented in class. Exercise resolution classes using the R software.   Volunteer Undergraduate Research Project\nGuided Study on Control Charts (2nd Semester 2016 - 1st Semester 2017)\n  Exploratory analysis through control charts of inflation rates in Brazil during the period of 2002 - 2016.   University Monitoring 2nd\nProbability and Statistics (2nd Semester 2015)\n  Make templates for lists of exercises and resolutions of exercises in the discipline. Answering students\u0026rsquo; doubts about the content taught in class.   University Monitoring 1st\nProbability and Statistics (1st Semester 2015)\n  Assemble templates for exercise lists and exercise resolutions for the discipline. Advise students of the discipline with doubts about the content taught in class.   Skills   Statistical Analysis and Data Preparation Statistical Modeling and Machine Learning Programming languages: Python and R Agile Methodologies Query languages: SQL and Power Query Development of interactive BI dashboards (using Power BI)   Courses and Certificates   The Complete SQL Bootcamp: Go From Zero to Hero Python for Data Science Business Intelligence: Introduction to Business Intelligence Agile Fundamentals: Your First Steps towards Agile Transformation Linux I: Knowing and Using the Terminal   A PDF version is available here.\n","permalink":"https://devmedeiros.com/about/","summary":"about","title":"About"}]