[{"content":"What is Multivariate Analysis? Multivariate analysis is a branch of statistical methods that allows analyzing the distribution of two or more variables. In statistics it can be used to reduce the dimensionality of data, simplifying variability, or for inference techniques.\nPrincipal Component Analysis Principal Component Analysis, also known as PCA, constitutes a multivariate analysis method whose purpose is to reduce the dimensionality of data. This reduction occurs by reducing the number of columns or variables while maintaining a significant percentage of the variability present in the data.\nThe use of this technique becomes interesting when dealing with a large number of variables of interest that you want to group and correlate. For example, a telecommunications company may have various information about its customers, such as age, income, profession, length of service with the company, and products/services purchased, among others. Often, the analyst wants to take advantage of all this information, whilst avoiding overfitting. In this context, the application of PCA emerges as a valuable tool, allowing dimensionality reduction while preserving the intrinsic variability of these variables.\nCluster Analysis Cluster analysis, also called clustering, is a technique that aims to group individuals or variables with similar characteristics. There are several algorithms for clustering, but the most common and most used is K-means.\nClusters can be used to create metrics and indices that can be used to evaluate a business or even to build forecasting models.\nThese tools are fundamental for scientists and analysts seeking to extract valuable insights, avoid overfitting, and promote a deeper understanding of the structure of multivariate data.\n","permalink":"https://devmedeiros.com/post/multivariate-analysis/","summary":"The two most important multivariate analysis techniques for data science.","title":"Multivariate Analysis for Data Science"},{"content":"Introduction to Data Testing Data testing comes from software testing in computer science, when it is tested various parts of the code to assess if it\u0026rsquo;s fit for use. In this sense, when testing your data you want to evaluate if the data is accurate, completed, and consistent.\nSo you want to make sure what you are extracting and sending is what you expect, making sure you are not losing data by changing data formats and transforming the data. When moving your data from point A to point B, you want to guarantee that the data is the same and has the same information.\nOne way to do this is by using common database structures, setting data types, and defining table constraints. But if this is not enough you can set more complex rules by using other tools, such as Great Expectations, and assertr.\nCommon Database Structure When creating or altering a table on a database, you need to specify the column\u0026rsquo;s data types and you can set multiple constraints. These data types determine how the values look, so imagine you have a table listing employees\u0026rsquo; basic info, like their names, salary, and date of birth.\nName Salary Date of Birth Ana 3000.00 1988-02-02 Bob 4000.00 1970-04-12 Carl 2000.00 1999-07-05 The salary column would be of type numeric, double digits because that is how we format currency. The date of birth would be of a \u0026ldquo;date\u0026rdquo; type format and the name a simple string. These data types are a kind of constraint, if you create a table with these data types you cannot insert a new row with different data types.\nBut you can also add more constraints, you could set a column to never accept null/empty values, or set a column to be composed of unique values.\nSetting data types and constraints is vital to maintaining data integrity because whenever someone or something (like an automatic process) tries to add or alter a piece of information on a table that has constraints it will raise an error. These errors help users avoid typos or inserting invalid options that may break other applications.\nImagine you have a system that sends a happy birthday present to an employee automatically, if that employee doesn\u0026rsquo;t have a date of birth registered they\u0026rsquo;ll never get their happy birthday present. So making it impossible to leave the birthday empty helps users pay attention use the database correctly and keep everything working as intended.\nLimitations When we usually think about data unit testing, or testing your data to make sure that nothing is out of the ordinary, is easy to imagine how relying only on data constraints is a good idea. It covers most use cases that we normally use, but if we stop to think about it, we can find more uses that we don\u0026rsquo;t even know we need.\nIn our employee example table, sure setting the salary to be a numeric data type with decimals usually stops a lot of potential mistakes, but it doesn\u0026rsquo;t stop people from having a negative salary or even an absurd salary amount. Sure you can set how big a number can be, but it only defines the digits, so if your maximal salary is 10000, you will have to set it to be 9,999 or 99,999.\nOf course, it helps, but it doesn\u0026rsquo;t fully solve the problem.\nAnother example of a limitation would be complex rules, for instance, if the employees have seniority levels like I, II, and III, and these come with salary range, you can\u0026rsquo;t have a custom rule saying \u0026ldquo;from level I the salary range is 1000 to 2000\u0026rdquo;.\nBeyond Constraints If you need to set more custom and/or business-specific data validation rules you can use Great Expectations, and assertr. Both are great for defining rules and validation parameters to test your data.\nYou can write tests setting min and max values, check unique values, check if the values are within a given set and many more options.\nIn conclusion, navigating data testing involves a careful balance between relying on conventional database structures and using specialized tools like Great Expectations and assertr. While constraints play a crucial role in maintaining data integrity, recognizing their limitations highlights the need for more nuanced and business-specific validation rules. By embracing these tools, data engineers and analysts can ensure not only the accuracy and completeness of their data but also navigate the complexities of real-world scenarios.\n","permalink":"https://devmedeiros.com/post/navigating-data-testing/","summary":"What is Data Testing and why you should care about it?","title":"Navigating Data Testing"},{"content":"Every day new research regarding Machine Learning is being published, and it may seem impossible to keep up with all the new models and applications that people are using. So I want to talk about my two favorite ways to keep up with new research and know about what people are working on.\n1 - Two Minute Papers This YouTube channel is my favorite one to keep up with new research regarding Machine Learning. They are always releasing new videos talking about papers showing what researchers have just published, focusing mainly on the results, but always linking the original paper so you can read more about it.\nTwo Minute Papers have been covering the OpenAI discoveries for quite some time now, I remember seeing videos about what ChatGPT could do, before it was open to the public, and it compared it with previous versions. So you can see the progress being made, even if you miss the older videos, they always make a comparison with previous versions when available.\nThe videos are kept in an approachable language, so even beginners can understand the main points. Additionally, the channel also makes the videos entertaining, so it\u0026rsquo;s easy to keep watching and stack on more knowledge.\n2 - Papers with Code Papers with Code is more than a website that just indexes papers. It makes it easier to find papers with code about Machine Learning, linking the papers, the source code, and even datasets.\nIf you want to learn about the state-of-the-art of a specific topic in Machine Learning you can browse between tasks, for example, if you are looking for more information about Natural Language Processing, you can choose a specific task, like Named Entity Recognition, then you\u0026rsquo;ll be able to see the current benchmarks, according to database, the best model, the model paper and source code. It also links the databases used to research the specific task you choose and it links libraries that you can use to apply the models.\n","permalink":"https://devmedeiros.com/post/keeping-up-machine-learning/","summary":"How to keep up with the latest discoveries in Machine Learning Models and academic research.","title":"Keeping Up with the Latest in Machine Learning Models"},{"content":"This tutorial is focused on Hugo, but it can easily be adapted for other static website frameworks.\nI\u0026rsquo;ve found many alternatives to add claps or likes to your website, but none was customizable as I wanted it to be, or it would require me to change my website hosting.\nI recently found out about Lyket and started using it. I\u0026rsquo;ll guide you on how to set it up on your website.\nSetting Lyket API The Lyket API provides a free tier, with no credit card needed to sign-up. To use it on a static site you\u0026rsquo;ll need to add the following tag to your website header.\n1 \u0026lt;script src=\u0026#34;https://unpkg.com/@lyket/widget@latest/dist/lyket.js?apiKey=[YOUR-API-KEY]\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; You can get an API Key on their website when you sign-up \u0026laquo; https://lyket.dev/ \u0026raquo;. You can use your public API token without fearing other people using it, you just need to set the allowed websites to your domain.\nThen you need to add the button tag where you want it to show, in my case I wanted it to appear on every blog post, but not on the other pages. So I added it to my post footer on my hugo project it\u0026rsquo;s located in layouts/_default/single.html file, on pasted the tag inside the \u0026lt;footer class=\u0026quot;post-footer\u0026quot;\u0026gt; tag.\nThe html button template will look something like this:\n1 2 3 4 5 6 7 \u0026lt;div data-lyket-type=\u0026#34;clap\u0026#34; data-lyket-namespace=\u0026#34;namespace\u0026#34; data-lyket-id=\u0026#34;everybody-clap-now\u0026#34; data-lyket-template=\u0026#34;medium\u0026#34; data-lyket-color-text=\u0026#34;currentColor\u0026#34; /\u0026gt; You can change the data-lyket-type to clap, like, updown, and rate. On data-lyket-namespace you can choose whatever name you like, you can use it to separate and organize your pages if you use the same API token on multiple websites, or if you separate your website content into categories you could use \u0026ldquo;blog\u0026rdquo; and \u0026ldquo;projects\u0026rdquo;, for example.\nOn data-lyket-template you can choose the template for your button, each button type has a different button template, this is optional, if you don\u0026rsquo;t add it it\u0026rsquo;ll turn to the default option, you can see the available options you can see it here: \u0026laquo; https://lyket.dev/templates \u0026raquo;.\nThe data-lyket-color-text allows you to personalize the text color of your button (the color of the numbers), you can set one color, but if you have a website that has a dark and light mode, you may prefer to use \u0026lsquo;currentColor\u0026rsquo;.\nThe data-lyket-id set the counter name, if you want two pages to share the like count it\u0026rsquo;ll need to have the same id. On my Hugo website, I used Hugo to get the page slug to use it as an id, you can use {{ replace (path.Base .RelPermalink) \u0026quot; /\u0026quot; \u0026quot;-\u0026quot; }} template as your id, so even if you translate your content all versions will keep their like count.\n1 2 3 4 5 6 7 \u0026lt;div data-lyket-type=\u0026#34;clap\u0026#34; data-lyket-namespace=\u0026#34;namespace\u0026#34; data-lyket-id=\u0026#34;{{ replace (path.Base .RelPermalink) \u0026#34; /\u0026#34; \u0026#34;-\u0026#34; }}\u0026#34; data-lyket-template=\u0026#34;medium\u0026#34; data-lyket-color-text=\u0026#34;currentColor\u0026#34; /\u0026gt; Bonus Tip You can add a Hugo parameter to hide the like button on pages you don\u0026rsquo;t want to show. Simply add to your page frontmatter a parameter, in this case noLike, and set it to true, so your page would look like this:\n1 2 3 4 5 --- title: \u0026#34;Your Awesome Page Title\u0026#34; layout: \u0026#34;single\u0026#34; noLike: true --- Then on your single.html file, you add the if clause to hide the button, in the end, it\u0026rsquo;ll look like this:\n1 2 3 4 5 6 7 8 9 {{- if not .Params.noLike }} \u0026lt;div data-lyket-type=\u0026#34;clap\u0026#34; data-lyket-namespace=\u0026#34;namespace\u0026#34; data-lyket-id=\u0026#34;{{ replace (path.Base .RelPermalink) \u0026#34; /\u0026#34; \u0026#34;-\u0026#34; }}\u0026#34; data-lyket-template=\u0026#34;medium\u0026#34; data-lyket-color-text=\u0026#34;currentColor\u0026#34; /\u0026gt; {{- end }} ","permalink":"https://devmedeiros.com/post/like-button-static-website-lyket/","summary":"Learn how to add a like/clap button to your static website using Lyket.","title":"How to Add Like/Clap Buttons on your Static Website with Lyket"},{"content":"Have you ever had a purchase blocked by your credit card issuer without you taking any action? If this happened to you, your credit card issuer probably had detected that your purchase didn\u0026rsquo;t match your normal spending habit, maybe you were at a different location or spend a lot of money suddenly. Anyway, the credit card company was using anomaly detection techniques to prevent fraudulent activities.\nWhat is Anomaly Detection? Anomaly detection refers to the problem of finding patterns in data that do not conform to expected behavior. 1\nIn statistics there is a famous distribution that is called \u0026ldquo;The Normal Distribution\u0026rdquo;, this name is not in vain. A lot of random things are normally distributed, for instance, if you select 100 random people and measure their height, and made a distribution plot, you\u0026rsquo;d end up with something looking like a symmetrical bell curve.\nEven though abnormal results are possible, it is expected that most people or things will fall within a normal distribution. When something is divergent from this normality it is said to be an outlier or an anomaly.\nWhen dealing with something like credit card fraud you can create an expectation of a person expanding habits, so if your daily routine consists of waking up and driving to your workplace and buying a coffee, then lunch in New York. Your credit card company would find it weird if you suddenly made a physical purchase in Rio de Janeiro.\nIn simple terms, that is what anomaly detection is trying to do, understand how something works so it can identify if it starts acting weird.\nAnomaly Detection x Class Imbalance in Datasets If you are aware of what class imbalance is, you may be wondering \u0026ldquo;can\u0026rsquo;t I use anomaly detection to solve this class imbalance problem?\u0026rdquo; - sometimes you can, but it\u0026rsquo;s not appropriate.\nClass imbalance occurs when one class has significantly more samples than the other class(es). This can lead to biased models that favor the majority class and perform poorly on minority classes. While anomaly detection focuses on identifying rare or abnormal data points that deviate significantly from the normal patterns in the dataset.\nEven though it has some overlap on what is trying to do, unless you have a class imbalance problem that the minority class can be considered an abnormal behavior, you won\u0026rsquo;t be able to use anomaly detection.\nI talked about class imbalance before on a churn rate problem in a fictitious telecommunication company, if you are interested you can check it here.\nChandola, V., Banerjee, A., and Kumar, V. 2009. Anomaly detection: A survey. ACM Comput. Surv. 41, 3, Article 15 (July 2009), 58 pages. DOI = 10.1145/1541880.1541882\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://devmedeiros.com/post/anomaly-detection/","summary":"How can your credit card issuer know if a purchase is made by you or a cloned version of your credit card?","title":"What is Anomaly Detection?"},{"content":"Recently I’ve been working with a data scraping project that works with a small amount of data, small enough that free resources/tier from the most popular cloud engines are enough to allocate my data, but I don’t like having the risk of being billed over this personal project. To solve this I’ve been looking for a free alternative that I can share and that runs automatically with Github Actions.\nIf you want to check out the repo that contains the code discussed in this post, follow this link.\nI\u0026rsquo;ll illustrate how to integrate SQLite Databases with Github Actions using Python, but if you know how to modify a file using another programming language this post is still relevant to you.\nWriting your Data Generator/Scrapper First, your project needs to be on a repository, in my case, I’m using Github. I wrote a Python code that scrapes a webpage and saves the data to a SQLite database, on this example I’ll illustrate this with a much simpler code.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 # Import libraries import pandas as pd import random from sqlalchemy import create_engine from datetime import datetime # Create random data people = [\u0026#39;Ana\u0026#39;, \u0026#39;Bob\u0026#39;, \u0026#39;Charles\u0026#39;, \u0026#39;Daiana\u0026#39;] values = [random.random(), random.random(), random.random(), random.random()] # Create db connection engine = create_engine(\u0026#39;sqlite:///data.db\u0026#39;) # Create the dataframe data = pd.DataFrame({\u0026#39;people\u0026#39;: people, \u0026#39;values\u0026#39;: values, \u0026#39;load_date\u0026#39;: datetime.now()}) # Save the data frame data.to_sql(\u0026#39;data\u0026#39;, if_exists=\u0026#39;append\u0026#39;, con=engine, index=False) Setting the Workflow If you run the above code multiple times on a local machine it’ll work, but you’ll notice that on Github it’ll not persist the changes, that is because you need to commit the changes. To do this you’ll need to create a workflow, on your repo create a yaml file on .github/workflow. This file is going to be your workflow, you can choose any name you want.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 # name your workflow name: random data # definy the frequency it\u0026#39;ll run on: schedule: - cron: \u0026#34;0 * * * *\u0026#34; # hourly workflow_dispatch: env: ACTIONS_ALLOW_UNSECURE_COMMANDS: true # create the jobs jobs: generate-latest: runs-on: ubuntu-latest steps: - name: Checkout repo uses: actions/checkout@v3 - name: Set up Python uses: actions/setup-python@v4 with: python-version: \u0026#39;3.10\u0026#39; # setting python version to 3.10 - name: Install requirements run: pip3 install -r requirements.txt # setting the environment - name: Run random data run: python main.py - name: Commit changes run: | git config --global user.name \u0026#34;github-actions\u0026#34; git config --global user.email \u0026#34;action@github.com\u0026#34; git add -A git commit -m \u0026#34;add more data\u0026#34; - name: Push changes uses: ad-m/github-push-action@master with: github_token: ${{ secrets.GITHUB_TOKEN }} Don\u0026rsquo;t forget to enable workflow permissions, on your repo go to Settings \u0026gt; Actions \u0026gt; General, and select Read and write permissions.\nConclusion This can be a good free alternative in case you want to be able to share the data you are scraping or generating. But you still need to keep an eye on Github\u0026rsquo;s limitations when using the free version. See the current usage limits on their official website.\nIf you would like to see a real-like application of this you can go to this repo. Where I\u0026rsquo;ve implemented a monthly scrapper that saves the data to an SQLite database that is available to everyone.\n","permalink":"https://devmedeiros.com/post/sqlite-database-github-actions/","summary":"Discover a free alternative to cloud engines for data scraping on GitHub Actions!","title":"How to Use SQLite Database with Github Actions"},{"content":"In the increasingly data-driven business world, machine learning has gained prominence as a powerful tool to drive business success. However, a crucial question arises: is machine learning really necessary in all cases? In this post, we\u0026rsquo;ll delve into that discussion, exploring the applications, benefits, and limitations of machine learning. In addition, we will provide useful criteria to help you decide whether this approach is essential for your business.\nWhat is Machine Learning? Machine Learning (ML) is a term used to describe algorithms that can be modeled to predict or explain something. You can make machine learning models that can predict monthly sales, customer churn, recommendation systems, etc.\nTo make these models you\u0026rsquo;ll need a lot of data, but not only that. You\u0026rsquo;ll need good data, unbiased and clean.\nThese models can be written in many different programming languages, the most popular ones are Python and R, but you could also use Julia, Scala, GO, and many others.\nMachine Learning Cases Recommendation Systems If you ever wonder how your favorite streaming service always has a new song or show recommended to you, this is thanks to recommendation systems. They have many different algorithms to guess what a user would like.\nOne of these algorithms creates clusters of similar users and uses things the other users enjoy to recommend to you. A simpler approach would just recommend the most popular items.\nCustomer Churn This kind of model is really popular amongst companies like internet providers and banks. These models look at customer behavior when interacting with the company to try to identify what causes a customer to give up working with them.\nIs useful to find problems in a company regarding customer experience and the quality of the services provided.\nThe benefits of Machine Learning With Machine Learning companies can make informed decisions about their business. For example, a restaurant can predict how many meals are going to be ordered weekly and then buy enough ingredients to avoid wasting food; or a company can calculate how likely a certain customer is to churn and then offer discounts or better deals to try to keep the customer.\nThese processes can be automated and - if the model is well built - it\u0026rsquo;ll only get better with time, increasing the accuracy and overall efficiency.\nLimitations Even though Machine Learning is incredible and sometimes looks like it came out of a sci-fi movie, it\u0026rsquo;s not a solution that fits all. Some business problems don\u0026rsquo;t have enough quality data to build a model, and even when you do have it, the computing power and expertise needed make it an expensive tool.\nSometimes you can achieve a close result to an ML model by just taking a moving average, it\u0026rsquo;s not perfect, but when you take into consideration the investment and the return of both approaches, it may make more sense to choose the cheaper option first, before moving into more complex models.\nHow to decide if Machine Learning is Needed Before deciding if an ML model is necessary check if you already accomplished the following things:\nyou have a quality data pipeline your company is data-driven you have the budget to invest in a data science team or to hire a company to do it your problem can be solved with ML have you already tested/tried simpler options available These aren\u0026rsquo;t rules written in stone, just something that I think is important to keep in mind before choosing to invest in Machine Learning. You don\u0026rsquo;t want to spend time working on something just to give up because then you realized that ML doesn\u0026rsquo;t fit your business needs or it doesn\u0026rsquo;t deliver extraordinary results.\n","permalink":"https://devmedeiros.com/post/unlocking-machine-learning/","summary":"Find out if the use of machine learning is essential in all business cases. Evaluate benefits and limitations.","title":"Is Machine Learning Necessary for Your Business?"},{"content":"Word clouds are a popular data visualization tool for analyzing texts, comment boxes, and reviews. And it\u0026rsquo;s not popular for no reason, it is an efficient visualization to show the opinion of a group of people in a summarized way, but it is not always presented effectively. So, here are some points to pay attention to when presenting the best word cloud you can.\n1- Removing common and irrelevant words One of the most common words in the English language is the article \u0026ldquo;the\u0026rdquo;. This is not surprising, given that it is essential for forming sentences, but despite being fundamental, when we think in terms of how it helps to explain the theme or idea of a sentence, it does not contribute to it at all. These articles and other common words are called stopwords.\nSo, whenever we work with word clouds, it\u0026rsquo;s important to remove these stopwords to make it clear what your theme is and to reduce information pollution.\n2- Simplicity over complexity When working with visual presentations, it\u0026rsquo;s common to want to do something extravagant that catches people\u0026rsquo;s attention, but sometimes complex visualizations can distract people from the story you want to tell and make it difficult to understand your message.\n3- Segment your data One of the most common uses of word clouds is in customer reviews and company evaluations (such as NPS), in these cases, where we have different \u0026ldquo;types\u0026rdquo; of customers, it\u0026rsquo;s expected that they have very different behaviors, a satisfied or happy customer will have different comments from one who is dissatisfied. So, if you make only one-word cloud without segmenting your data, you may find a word cloud that means nothing more than \u0026ldquo;many people mentioned this\u0026rdquo;.\nBy segmenting the data by customer ratings or by detractors and promoters, you can identify what customers who are happy with your service are saying and contrast it with what those who are dissatisfied are saying, and identify areas for improvement.\n","permalink":"https://devmedeiros.com/post/how-to-use-word-clouds/","summary":"Word clouds are a popular data visualization tool for analyzing texts, but they need to be presented clearly for the message to be understood.","title":"How to use Word clouds?"},{"content":"SQL stands for Structured Query Language. A query language is a specialized programming language that you can use to search and change the contents of a database.\nLike English, SQL has different dialects with special keywords. Most keywords on different dialects are the same, these are the standard SQL, but there are more dialects specific to some databases like:\nPostgreSQL SQL Server Oracle SQL When a keyword is specific to a dialect, usually, you can find an equivalent in another dialect. So you can focus on learning your preferred dialect and when you need to use another database you can easily look up how to do what you want in the other dialect, for instance, oracle sql nvl on postgre.\nWhich should you learn? The knowledge from learning any SQL dialect is transferable, so you can choose whichever you like. If you have contact with a database take advantage of that and learn it, but if you don\u0026rsquo;t have any connection any choice will be good enough.\n","permalink":"https://devmedeiros.com/post/what-is-sql/","summary":"SQL stands for Structured Query Language. A query language is a specialized programming language that you can use to search and change the contents of a database.","title":"What is SQL?"},{"content":"In the last months, once again AI became a hot topic, after several papers regarding art made by artificial intelligence, now is the time for text written by AI to shine.\nMachine learning models were already helping people for a long time, such as grammar checkers, tools to rewrite text to avoid plagiarism, and even tools that do it all - as long as you know how to ask. Tools like ChatGPT aren\u0026rsquo;t new, they just exploded in popularity recently, and you certainly have heard about many ways to use these artificial intelligence tools, but I want to propose a new way in which we can take advantage of them.\nLooking stuff up on the internet can be hard, and sometimes you need a lot of information regarding what you\u0026rsquo;re looking for, which makes your journey slow. In these cases, using AI tools can be helpful. Unlike search engines like Google, artificial intelligence tools are capable to understand a question and give you an elaborated answer without the need to use special \u0026ldquo;codes\u0026rdquo; or filters, you just need to use natural language.\nRecently I was looking for how to find circles on images using Python. All the search results on the first page of Google returned an answer containing the package OpenCV, which is one of the main packages in the area of Digital Image Processing, but I wanted something different.\nEven though I changed the parameters of my search, the search engines kept insisting on showing me pages with the package opencv, even when I explicitly wrote \u0026ldquo;without using opencv\u0026rdquo;, then I decided to test ChatGPT. First I try asking you to write a function to find the circles.\nIt did write a functioning function, pretty similar to what I had written, but it still was using OpenCV, then I decided to be more specific and ask for other ways to do this, instead of just asking for the code.\nI don\u0026rsquo;t know if all the methods it suggested work or even exist, but these answers help me break a creative block I had, regarding how to solve this issue, and it made it possible for me to get back on making progress on this project. Now I can learn more about scikit-image, which I didn\u0026rsquo;t even know, and test new packages. It was satisfactory to be able to find new ways to solve the same problem. Using this I can assure you I\u0026rsquo;m exploring many different paths and I\u0026rsquo;m not stuck on just what I already know and use.\n","permalink":"https://devmedeiros.com/post/using-ai-in-your-favor/","summary":"How to use AI tools, like ChatGPT, in your favor.","title":"How to use AI in your favor?"},{"content":"Learning how to make better visualizations come with study and practice, today I want to talk about sources you can use to guide you into making better graphs and other data visualizations in general.\nFrom Data to Viz From Data to Viz brings a lot of tools to help navigate the data visualization world. In the website Explore tab you are faced with an interactive infographic that helps you pick a plot, you can choose between: numeric, categoric, numeric\u0026amp;categoric, maps, network, and time series. But it doesn\u0026rsquo;t only show plots that would be good for your type of data, it also shows a description explaining the plot and common mistakes people make when using that specific plot.\nAt the end of each branch from the infographic, there is also a story link, where you can read about an implementation of that type of plot using real-world data.\nAnd in the case, you are just starting and don\u0026rsquo;t know many terms or are unsure if your data contains too many or few points when you rest your mouse over the options nodes you\u0026rsquo;ll be able to see a description with an example table representing what that means.\nBy heading to the Caveats tab you\u0026rsquo;re able to see a collection of caveats, you can filter them by:\nImprovement: all caveats that will suggest some sort of improvement on plots.\nMisleading: some plots may be misleading by omitting some information, may it be sample size or how proportionally different two categories are. By reading this you\u0026rsquo;ll be able to spot bad practices that lead to confusion or manipulation.\nThis website also provides links to Python and R galleries where you can get code snippets to reproduce these plots.\nIf you prefer to learn by looking at what not to do, you should check out WTF Visualizations. It showcases bad visualizations that don\u0026rsquo;t make sense or are somehow misleading.\nThere you\u0026rsquo;ll find several pie plots that don\u0026rsquo;t add up to 100%, area graphs where the numbers don\u0026rsquo;t match the area, and just straight-up manipulation like the example below.\npiechart from WTF Visualizations\n","permalink":"https://devmedeiros.com/post/choose-the-best-graph/","summary":"Choosing the best graph or plot can be a hard task, but it doesn\u0026rsquo;t need to be.","title":"Making Better Graphs"},{"content":"Introduction Data science is an exciting field, but it’s also full of ethical pitfalls. If you want to avoid getting drawn into a situation that could damage your company or client, it’s important to understand the basics of data ethics and how they can affect your work.\nLearning from the past Learning from the past is important because it can help us avoid making ethical mistakes that others already committed. The following examples are a sampling of some of the most popular data science ethics issues over the years:\nGoogle\u0026rsquo;s Street View privacy concerns (2011) In 2011, Google came under fire for collecting personal data from unsecured wireless networks while taking pictures for its Street View service. The company said it was an accident and that it had not intended to collect any information from these Wi-Fi networks. However, computer security experts discovered a code in the software used by Google\u0026rsquo;s cars that indicated that they were designed to do just that.\nGoogle said it had collected data like emails and passwords, but it wasn\u0026rsquo;t sure how much. The fact that Google didn\u0026rsquo;t know what data was collected raised concerns about its commitment to user privacy and security.\nFacebook\u0026rsquo;s Cambridge Analytica scandal (2018) In March 2018, Facebook revealed that it had been the victim of a massive data breach involving tens of millions of users. The company said that Cambridge Analytica, a political consultancy firm with ties to the Trump presidential campaign, had gained access to information from up to 87 million.\nUsers were unaware that the app was collecting data from them, and Facebook did not do enough to prevent it. The incident led to intense scrutiny of Facebook\u0026rsquo;s use of user data and its responsibility for protecting users\u0026rsquo; privacy. In April 2018, Facebook CEO Mark Zuckerberg testified before American Congress on the matter. He apologized for his company\u0026rsquo;s mistakes in handling user data and outlined some steps he would take to ensure that similar breaches didn\u0026rsquo;t occur again.\nIBM’s Photo-scraping scandal (2019) IBM faced a photo-scraping controversial scandal in 2019 where the controversy focused on 1 million pictures of human faces that IBM scrapped from Flickr, the online photo-hosting site.\nThis scandal brought to light how their data is being used. People aren’t consenting to have their details used for profit.\nPredictive Policing Software It would be nice to be able to predict crime or be able to estimate where law enforcement needs to be to prevent crime, but many predictions made by this kind of software don’t come true. This happens because these tools are fed bad data.\n“Data collected by police is notoriously bad, easily manipulated, glaringly incomplete, and too often undermined by racial bias.” – Ezekiel Edwards, ACLU\nThis polluted data makes the software produce equally contaminated results, leading the software to be better at predicting policing instead of predicting crime, becoming a self-fulfilling prophecy.\nConclusion These examples show that there are many ways to make mistakes when it comes to handling user data, but they also serve as excellent educational resources for anyone interested in learning more about how these kinds of problems can be avoided in future projects.\nThere are lots of resources to learn more about this theme and how to combat this issue when working with machine learning models and user data. I\u0026rsquo;ll list some, but remember this is a broad and deep topic, and my research may not find everything there is to it.\nA free course on Fairness, Accountability, and Transparency sponsored by GIAN. Why Mathematicians Won\u0026rsquo;t Help Cops - VSauce2 Data Ethics Framework - UK Government ","permalink":"https://devmedeiros.com/post/ethics-data-science/","summary":"Data Science is an exciting field, but it\u0026rsquo;s also full of ethical dilemmas.","title":"Ethics in Data Science"},{"content":"What is it like working as an Analytics Engineer? Analytics Engineer refers to a Data Science professional focused on transforming data into information that is easy to access to the end-user. They provide static and dynamic reports that empower the business team without them needing to think about the complexity behind data analysis.\nIn this case study, I want to talk about what would be common tasks that an Analytics Engineer would need to perform and how I\u0026rsquo;d navigate them.\nIn this scenario, the Analytics Engineer works for Bankio a digital bank from Brazil. Like most digital banks in Brazil, Bankio offers free transfers for every bank account in the country. It also has many products like an investment account, a savings account, an individual bank account, a credit card without an annual fee, and many more.\nTask 1: SQL Query A Bussiness Analyst from Bankio asks for your assistance writing a SQL query to get all the account\u0026rsquo;s monthly balance between January 2020 to December 2020.\nSQL Query Solution (click to expand) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 SELECT a.*, -- Here I calculate the cumulative sum of total deposits for each customer ordering it -- by month, if its null I change the value to 0 then I subtract the cumulative sum of -- total withdrawals for each customer ordering it by month, if its null I change the -- value to 0 and I save this as ours account_monthly_balance NVL(SUM(total_transfer_in) OVER (PARTITION BY customer_id ORDER BY action_month), 0) - NVL(SUM(total_transfer_out) OVER (PARTITION BY customer_id ORDER BY action_month), 0) AS account_monthly_balance FROM -- total transactions in/out subquery ( SELECT * FROM -- total deposits subquery ( SELECT action_month, customer_id, SUM(amount) AS total_transfer_in FROM ( SELECT * FROM -- regular deposits subquery ( SELECT d_month.action_month, accounts.customer_id, SUM(transfer_ins.amount) AS amount FROM d_time INNER JOIN transfer_ins ON transfer_ins.transaction_completed_at = d_time.time_id LEFT JOIN d_month USING(month_id) LEFT JOIN accounts USING(account_id) WHERE transfer_ins.status = \u0026#39;completed\u0026#39; GROUP BY d_month.action_month, accounts.customer_id ) transfer_in UNION ALL SELECT * FROM -- pix deposits subquery ( SELECT d_month.action_month, accounts.customer_id, SUM(pix_movements.pix_amount) AS amount FROM d_time INNER JOIN pix_movements ON pix_movements.pix_completed_at = d_time.time_id LEFT JOIN d_month USING(month_id) LEFT JOIN accounts USING(account_id) WHERE pix_movements.status = \u0026#39;completed\u0026#39; AND pix_movements.in_or_out = \u0026#39;pix_in\u0026#39; GROUP BY d_month.action_month, accounts.customer_id ) ) GROUP BY action_month, customer_id ) FULL JOIN ( SELECT action_month, customer_id, SUM(amount) AS total_transfer_out FROM -- total withdrawals subquery ( SELECT * FROM -- regular withdrawal subquery ( SELECT d_month.action_month, accounts.customer_id, SUM(transfer_outs.amount) AS amount FROM d_time INNER JOIN transfer_outs ON transfer_outs.transaction_completed_at = d_time.time_id LEFT JOIN d_month USING(month_id) LEFT JOIN accounts USING(account_id) WHERE transfer_outs.status = \u0026#39;completed\u0026#39; GROUP BY d_month.action_month, accounts.customer_id ) UNION ALL SELECT * FROM -- pix withdrawal subquery ( SELECT d_month.action_month, accounts.customer_id, SUM(pix_movements.pix_amount) AS amount FROM d_time INNER JOIN pix_movements ON pix_movements.pix_completed_at = d_time.time_id LEFT JOIN d_month USING(month_id) LEFT JOIN accounts USING(account_id) WHERE pix_movements.status = \u0026#39;completed\u0026#39; AND pix_movements.in_or_out = \u0026#39;pix_out\u0026#39; GROUP BY d_month.action_month, accounts.customer_id ) ) GROUP BY action_month, customer_id ) USING (action_month, customer_id) ) a; Task 2: Key Performance Indicators Photo by Stephen Dawson on Unsplash\nAnother collegue from Bankio is interested in analysing the success of the company product PIX on a business and technical level. So they asked you to come up with some key indicators to measure this.\nMean processing time of PIX transactions (click to expand) This can be obtained using the time a customer requests a PIX transaction and when it is completed, we calculate this for all the PIX transactions then we take the mean value. PIX is supposed to be instantaneous, so this metric should be as small as possible. The proportion of PIX failures (click to expand) This indicator is important because it’s inconvenient for the client to have their transaction fail. We can calculate this by dividing the sum of failed PIX transactions by the total PIX transactions. This measure should be minimized. The proportion of transactions using PIX (click to expand) PIX success can be measured by the proportion of transaction movements using PIX over normal transactions. So just count how many completed transactions using PIX were made and divide it by the total amount of completed transactions. A bigger measurement reflects PIX success over regular transactions.\nAlternatively instead of just counting the transactions we can evaluate how much money each transaction type is moving.\nThe proportion of in/out of PIX (click to expand) This measure is good to analyze if customers are using their PIX to receive more money or to send money. It would be better if more customers are receiving more money than they are sending. Because Bankio already had free transactions for any bank, before PIX came around, others still had to pay fees to send money to your Bankio account. For this reason, it is better to count how many transactions are coming in through PIX and divide it by all PIX transactions. The higher the better.\nIn this case we could also sum the balance of deposit and withdrawals from the Bankio account using PIX and compare it with regular transactions.\nTask 3: Daily Investment Return Bankio has a customer banking account that allows you to invest on a fixed rate income product. Consider that this product provides customers with a daily return of 0.01% according to their daily invested balance amount. Calculate how much each customer has on their bankio account during the year 2020.\nThis return is calculated daily after all withdrawals and/or deposits made on a given day. And every day, even weekends, generate some return.\nThe following example describes customer A who begins investing in this fixed income product on day 16 of the first month. The prior balance was zero since this consumer is making a first-time deposit into the investment. His initial deposit was 1,000, and at the end of the day, it produced a daily income rate of 0.01% of his balance. The same product is still being consumed by this client at different times throughout the month. Remember that this is just a dummy sample of the transaction log with daily calculations applied. Note that the income for that day should be set to zero in the event of negative Movements.\nDay Month Account ID Deposit Withdrawal End of Day Income Account Daily Balance 16 1 A 1000 0 0.1 1000.10 20 1 A 500 0 0.15 1500.55 2 2 A 0 200 0.13 1302.48 19 2 A 1000 200 0.21 2104.78 Movements = Previous Day Balance + Deposit - Withdrawal\nEnd of Day Income = Movements * Income Rate\nAccount Daily Balance = Movements + End of Day Income\nGlossary Account Monthly Balance It is the amount of money a customer had in their account at the end of a given month.\nAccount info (branch, number and check-digit) In Brazil, a bank account can be uniquely identifiable by three numbers. The branch code, which indicates which bank branch the accounts were opened in, comes first. The second is the account number that a branch uses to identify accounts. The check-digit, which is only used for error detection, is the last.\nCPF It is the Brazilian individual taxpayer registry identification.\nPIX In Brazil, this is the most recent method of money transmission. It\u0026rsquo;s unpaid. It is immediate, and all that is required to complete a transaction is the Pix-Key associated with the account.\nNon PIX transfers These are the conventional methods for transferring money between bank accounts. This type of transaction requires the CPF, the branch code, the account number, and the check digit of the account that will receive the funds to be provided. Most banks charge a fee in these transactions, and the confirmation of the transaction typically takes several hours to days.\n","permalink":"https://devmedeiros.com/post/case-study-analytics-engineer/","summary":"Project exploring common tasks an Analytics Engineer needs to perform on a daily day.","title":"Case Study Analytics Engineer"},{"content":"Project Overview This project showcase a data science life cycle, where I clean and prepare the dataset, use feature engineering, machine learning, deploy, and data visualization.\nThe dataset comes from kaggle, it has a lot of information about a person\u0026rsquo;s credit and bank details, but it also has a lot of typos, missing data, and censored data. This dataset needed cleaning and also needed some feature engineering, I needed to mutate some features, so they could be read by the model. Thus when presented with categorical data I needed to identify if it was ordinal or nominal, if it was an ordinal variable then it would be mapped to sequential numbers otherwise I\u0026rsquo;d make a dummy. For yes and no variables I choose to make just one dummy, but for types of loans I made one dummy for each loan type and if someone didn\u0026rsquo;t have a loan they simply get 0 on all loan type features. I talk about the process of cleaning and feature engineering on this dataset here.\nThen I needed a machine learning model that I could predict a person\u0026rsquo;s credit score based on some features. To decide which features I was going to use I based my decision on what is commonly used among real companies, and I also choose variables that I thought made sense. I ended up with the following features:\nAge Annual income Number of bank accounts Number of credit cards Number of delayed payments Credit card utilization ratio Total EMI paid monthly Credit history age in months Loans Missed any payment in the last 12 months Paid minimum amount on at least one credit card With the features ready, I moved on to making the model, I decided to use a simple Random Forest, for now, I do intend to work on making this model better, but in this first instance, I wanted to focus on making the streamlit app.\nAfter I finished the model I serialized it and the scaler using the pickle package. To deploy the model and build a visualization I used streamlit.\nIn this app, you can fill out a form or just select one of the three default profiles given to see how the model evaluates each person\u0026rsquo;s credit score. It also presents how certain the model was by displaying a pie graph with the probability (in percentage) of each credit score group the answers fit. It also shows how much each feature counts towards your credit score, according to this model. You can see the app live here.\nAll of the code is available at my GitHub repository. Besides the code, there you\u0026rsquo;ll find the documentation, the original and treated data (all the stages of treatment), all the requirements for building this project, and how to run it locally.\n","permalink":"https://devmedeiros.com/post/credit-score-classification-app/","summary":"Using Streamlit to make a web app that classifies your credit score using Python","title":"Credit Score Classification App"},{"content":" Disclaimer: I\u0026rsquo;ll be talking about how to come up with the python code, if you want to read the actual code please go to this repo.\nMeet the Credit Score Classification Dataset The dataset that we\u0026rsquo;ll clean comes from kaggle, which is the train.csv dataset, but this could be used for the test.csv as well.\nThere are 28 columns and 100k rows in this dataset. I compiled a feature description table that you can see below.\nFeature Description ID Represents a unique identification of an entry Customer_ID Represents a unique identification of a person Month Represents the month of the year Name Represents the name of a person Age Represents the age of the person SSN Represents the social security number of a person Occupation Represents the occupation of the person Annual_Income Represents the annual income of the person Monthly_Inhand_Salary Represents the monthly base salary of a person Num_Bank_Accounts Represents the number of bank accounts a person holds Num_Credit_Card Represents the number of other credit cards held by a person Interest_Rate Represents the interest rate on a credit card Num_of_Loan Represents the number of loans taken from the bank Type_of_Loan Represents the types of loan taken by a person Delay_from_due_date Represents the average number of days delayed from the payment date Num_of_Delayed_Payment Represents the average number of payments delayed by a person Changed_Credit_Limit Represents the percentage change in credit card limit Num_Credit_Inquiries Represents the number of credit card inquiries Credit_Mix Represents the classification of the mix of credits Outstanding_Debt Represents the remaining debt to be paid (in USD) Credit_Utilization_Ratio Represents the utilization ratio of credit cards Credit_History_Age Represents the age of credit history of the person Payment_of_Min_Amount Represents whether only the minimum amount was paid by the person Total_EMI_per_month Represents the monthly EMI payments (in USD) Amount_invested_monthly Represents the monthly amount invested by the customer (in USD) Payment_Behaviour Represents the payment behavior of the customer (in USD) Monthly_Balance Represents the monthly balance amount of the customer (in USD) Credit_Score Represents the bracket of credit score (Poor, Standard, Good) Even though we have 100k rows, within these rows that are only 12,500 different customers, each customer appears 8 times (from January to August). So basically we can select a particular customer and look at their information and easily find incorrect data and be able to adjust it.\nCleaning Typos and Outliers In this dataset that is a lot of typos or just straight-up nonsense. You\u0026rsquo;ll find some values to be: _, !@9#%8, __10000__, NM or _______. I believe these typos are in the dataset to represent the improbability that you may find when dealing with real-world data and most of them mean that this is a null value.\nFor a moment I thought __10000__ would just be a typo, but there is no amount invested monthly that is over 200 dollars.\n1 2 3 4 5 6 7 8 9 10 11 12 __10000__ 4305 0.0 169 80.41529543900253 1 36.66235139442514 1 89.7384893604547 1 ... 36.541908593249026 1 93.45116318631192 1 140.80972223052834 1 38.73937670100975 1 167.1638651610451 1 Name: Amount_invested_monthly, Length: 91049, dtype: int64 Following this logic, I looked for nonsense in the data frame and I started to replace them with numpy nan\u0026rsquo;s. I also looked for outliers by looking at the distribution of values, if there was a value that only appeared once and was isolated I substitute it for a null value. I based this decision not only on this but also when I looked for customers that had this outlier and I observed all the data from this particular customer, I\u0026rsquo;d see weird things like:\nBy looking at this customer is clear that he didn\u0026rsquo;t make this much money annually only one month of the year.\nWhen you finish this search for typos and outliers don\u0026rsquo;t forget to assign the correct data type to your features. Some features like age started with string characters among the age values and because of this, it\u0026rsquo;s uploaded as an object instead of int or float.\nFilling Null Values After dealing with all the outliers and typos, we ended up with a lot of null values, as you can see:\n1 df.isna().sum()[df.isna().sum() \u0026gt; 0] 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 Age 2776 Occupation 7062 Annual_Income 993 Monthly_Inhand_Salary 15002 Num_Credit_Card 2271 Interest_Rate 2034 Num_of_Loan 4348 Type_of_Loan 11408 Num_of_Delayed_Payment 7002 Changed_Credit_Limit 2091 Num_Credit_Inquiries 1965 Credit_Mix 20195 Credit_History_Age 9030 Payment_of_Min_Amount 12007 Amount_invested_monthly 8784 Payment_Behaviour 7600 Monthly_Balance 1200 dtype: int64 Instead of just dropping all these null values I first try to fill them using the information I already have. Remember that I said that a customer has historical data for 8 months? We can just use this historical data to fill the null values using an aggregation measurement of our choice filtering for the customer, this will be more accurate than just calculating the mean value of the database.\nI decided to use the average values for the following columns:\n1 2 3 4 5 mean_columns = [ \u0026#39;Num_of_Delayed_Payment\u0026#39;, \u0026#39;Changed_Credit_Limit\u0026#39;, \u0026#39;Num_Credit_Inquiries\u0026#39;, \u0026#39;Amount_invested_monthly\u0026#39;, \u0026#39;Monthly_Balance\u0026#39;, \u0026#39;Num_of_Loan\u0026#39;, \u0026#39;Num_Credit_Card\u0026#39;, \u0026#39;Interest_Rate\u0026#39;, \u0026#39;Annual_Income\u0026#39;, \u0026#39;Monthly_Inhand_Salary\u0026#39; ] And the last non-empty value for these:\n1 last_columns = [\u0026#39;Age\u0026#39;, \u0026#39;Occupation\u0026#39;, \u0026#39;Type_of_Loan\u0026#39;, \u0026#39;Credit_Mix\u0026#39;] The reason for not using the mean for all my values is that I didn\u0026rsquo;t want to have someone be 20.5 years old and Occupation, Type_of_Loan, and Credit_Mix are discrete data.\nFeature Engineering With the clean data, we can proceed to feature engineering. In this case, we first want to change the Type_of_Loan, because that are some occurrences that it has all the loans in one value, as you can see:\n1 2 3 4 5 6 Not Specified, Mortgage Loan, Auto Loan, and Payday Loan 8 Payday Loan, Mortgage Loan, Debt Consolidation Loan, and Student Loan 8 Debt Consolidation Loan, Auto Loan, Personal Loan, Debt Consolidation Loan, Student Loan, and Credit-Builder Loan 8 Student Loan, Auto Loan, Student Loan, Credit-Builder Loan, Home Equity Loan, Debt Consolidation Loan, and Debt Consolidation Loan 8 Personal Loan, Auto Loan, Mortgage Loan, Student Loan, and Student Loan 8 Name: Type_of_Loan, Length: 5380, dtype: int64 So I\u0026rsquo;ll save all the different loan types in one vector, by splitting the loans every time there is a , or , and.\n1 2 3 4 5 6 loan_types = [] for index in df.index: temp = df.Type_of_Loan[index].replace(\u0026#39;and \u0026#39;, \u0026#39;\u0026#39;).split(\u0026#39;, \u0026#39;) for i in temp: #loan in temp array if i not in loan_types: #if loan is not in loan_types loan_types.append(i) #add it Now we can create dummy variables of these loan_types, so a customer will receive the number 1 if they have this loan or a 0 if they don\u0026rsquo;t.\n1 2 3 4 5 6 for loan in loan_types: df[loan] = 0 #create the loan column in the df with 0 for index in df.index: temp = df.Type_of_Loan[index].replace(\u0026#39;and \u0026#39;, \u0026#39;\u0026#39;).split(\u0026#39;, \u0026#39;) if loan in temp: df.loc[index, loan] = 1 Now I want to keep working on this dataset to make it ready for training a machine learning model. For this reason, I need to transform my discrete data into numeric.\nThe feature Credit_History_Age has the values as strings \u0026ldquo;22 Years and 5 Months\u0026rdquo;, this pattern repeats itself, so we can take advantage of this and select the year multiplied by 12 and sum the month, resulting in a new feature with the credit history age in months. When we are done with this, there are still going to be null values, to fill them I choose to interpolate the values, this works great when the missing value is in February up until July because it interpolates with the customer\u0026rsquo;s credit history age, but it becomes a bad guessed when the missing value is in January or August.\nThe months\u0026rsquo; names are going to be replaced by their number counterpart, so January is 1, February is 2, and so on. credit_mix and credit_score have 3 sequential categories, I choose to go with -1, 0, and 1, but you can use 1, 2, 3 and it\u0026rsquo;ll produce the same result.\nDon\u0026rsquo;t forget to check the GitHub Repository for the complete code mentioned here and the cleaned dataset.\n","permalink":"https://devmedeiros.com/post/data-cleaning-credit-score/","summary":"How to come up with ways to clean a dataset using Python","title":"Cleaning Credit Score Classification Dataset"},{"content":"The Alura Challenge BI 2 was a four-week event where we\u0026rsquo;re challenged to make Business Intelligence dashboards according to the request of three made-up companies. During the event I made this three dashboard using Power BI, you can look at the source file of all dashboards at my repo.\nAlura Films The objective of this dashboard is to help find the best selection for an upcoming movie.\nWith this in mind, I explored in the first page a little summary about the movies in the database, with this you can get a general view of our data. The second page is displayed the IMDB and Meta Score of the movies. I also created a measure to show how much both measurements agree. A score of 100 means doesn\u0026rsquo;t agree at all and 0 means completely agree, the agreeable score was 9.48.\nOur third page shows information about the movie stars, the top 10 actors with the highest movie gross revenue, and the top 10 actors with the biggest movie count.\nThe fourth page presents the distribution of gross revenue by how many different genres a movie has. It also shows how many movies have a given genre, for example, Drama is the most popular choice for movie genre, with 72% of movies in the dataset. At last, this page also shows the mean gross revenue by genre.\nThe fifth and last page shows a bit of information about the Brazillian Parental Guide and shows the average Meta Score for movies by the parental guide certification. It also shows how much gross revenue, on average, each parental guide certification earned.\nAlura Food Alura Food is interested in expanding its business by entering the Indian market. To do so, the company asked to calculate measures to help them make a better decision.\nFirst of all, I merged the datasets and cleaned the data through Power BI, translated a few texts from English to Portuguese through Google Sheets, and converted the meals price from their respective currency value to BRL (Brazillian Currency). Finally, I used Figma to make the background, including images and titles.\nIn this project, I opted to used a single page to display all the requested information as I believe it makes it easier to analyze the data.\nMost restaurants in this Indian market don\u0026rsquo;t offer online delivery, with just 19.21% having it. The average rating for the restaurants is 3.72 out of a maximum of 5, the rating is also presented as text, with a Muito Bom(Very Good) being a score over 4.\nThe average price of a meal per person is around R$ 39.48 (around USD 7.65). And there are 9577 different restaurants in the dataset, of which 3968, specialize in North Indian cuisine. New Delhi is, by far, the most popular choice to open a restaurant with 5473, the second closest city is Gurgaon, with 1118 restaurants.\nAlura Skimó Alura Skimo is interested in analyzing their sales data, to help with this I made this dashboard.\nIt’s composed of three pages. The first one presents a summary of all the main measures you’ll find in the dashboard, filtered for the most recent month. On this first page, you can also rest your mouse cursor over a measure and it’ll show a small plot with the historical series with a tendency line.\nMoving on to the next page, you can see all the information about the products sold by Alura Skimo. You can filter the data by ice cream flavor, type of package, category, and product cost. It displayed the basic information about the products, along with a rank of the best-selling products and a couple of plots showing the sales by flavor and sales by category.\nOn the last page, you can find information about the salesperson from our company. It shows when they joined the company, how much is their cut, how much revenue they got and how many sales each one made, everything in the last year (2018).\nThis dashboard is complex next to the other two because our dataset came from SQL files. First, I had to create the database and load each SQL file into it, then all I had to do was load it on Power BI. At last, I did all the data cleaning and processing on it.\n","permalink":"https://devmedeiros.com/post/alura-challenge-bi-2/","summary":"Alura hosted a four-week Challenge BI, where participants needed to make 3 dashboards","title":"Alura Challenge BI 2"},{"content":"What is feature selection? Feature selection is the process of finding pertinent features and eliminating unnecessary, redundant, or noisy data. It can be useful to help the training process be faster and it can also avoid overfitting. According to Munson and Caruana, the goal of feature selection is to identify the feature set that best balances the risks of having too few features with the risks of having too many features. It can also help improve the prediction accuracy and the interpretation of models.1 2\nI chose three methods for feature selection to show, but these are not all available methods, in the scikit-learn package that are 17 algorithms implemented at the time of writing. 3\nCorrelation among features Correlation among features can bring unnecessary noise to your model. Because a high correlation means that when one feature increases the other increases as well (or decreases if is negative). So if two features have a high correlation they aren’t bringing any new information to our model, in this case, is best to choose one of them to keep and drop the other.\nLook at the following correlation matrix, for example:\nWe can see that height and weight are heavily correlated with BMI, we don’t need to keep these three features, we can drop height and weight and keep just the BMI, or depending on what you want with the model you just drop the BMI.\nLow Variance Removing features with low variance is a simple method of feature selection. You can set a minimal variance a feature needs to determine if it continues in the model or not. The reasoning behind this is that a low variance means it has a small effect on the final model.\nAlthough there isn\u0026rsquo;t concrete information on what is a \u0026ldquo;low variance\u0026rdquo;, the scikit-learn default option for this method is 0, but you can test other values for your dataset and see what works best for you.\nRecursive feature elimination The purpose of recursive feature elimination (RFE) is to pick features by iteratively considering smaller and smaller sets of features given an external estimator that assigns weights to features. The importance of each feature is first determined through any particular attribute, and the estimator is then trained on the initial set of features. The least crucial features are then removed from the present list of features. On the pruned set, that process is continued recursively until the target number of features to select is eventually attained. 4\nMUNSON, M. Arthur; CARUANA, Rich. On feature selection, bias-variance, and bagging. In: Joint European Conference on Machine Learning and Knowledge Discovery in Databases. Springer, Berlin, Heidelberg, 2009. p. 144-159.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nKUMAR, Vipin; MINZ, Sonajharia. Feature selection: a literature review. SmartCR, v. 4, n. 3, p. 211-229, 2014.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nAPI Reference — scikit-learn 1.1.1 documentation\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n1.13. Feature selection — scikit-learn 1.1.1 documentation\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://devmedeiros.com/post/2022-06-14-feature-selection/","summary":"Defining feature selection and showing how you can select features in Machine Learning","title":"Feature Selection"},{"content":"I was challenged to take the role of a new data scientist hired at Alura Voz. This made-up company is a telecommunication company and it needs to reduce the Churn Rate.\nThe challenge is divided into four weeks. For the first week, the goal was to clean the dataset provided by an API. Next, we need to identify clients who are more likely to leave the company, using data exploration and analysis. Then, in the third week, we made machine learning models to predict the churn rate for Alura Voz. The last week is to show off what we made during the challenge and build our portfolio. In case you are interested in seeing the code for the challenge just head over to my GitHub repository.\nFirst Week Reading the dataset The dataset is available in a JSON file, at first glance it looked like a normal data frame.\nBut, as we can see, customer, phone, internet, and account are their own separate table. So I had to normalize them separately and then I just concatenated all these tables into one.\nMissing data The first time I looked for missing data in this dataset I notice that apparently, that wasn\u0026rsquo;t anything missing, but later on, I noticed that there was empty space and just space not being counted as NaN. So I corrected this, and now the dataset had 224 missing values for Churn and 11 missing for Charges.Total.\nI decided to drop the missing Churn because this is going to be the object of our study and there isn\u0026rsquo;t a point in studying something that doesn\u0026rsquo;t exist. For the missing Charges.Total, I think it represents a customer that hasn\u0026rsquo;t paid anything yet, because all of them had a tenure of 0, meaning that they had just become a client, so I just replaced the missing value for 0.\nFeature Encoding The feature SeniorCitizen was the only one that came with 0 and 1 instead of Yes and No. For now, I\u0026rsquo;m changing it to yes and no, because it\u0026rsquo;ll make the analysis simpler to read.\nCharges.Monthly and Charges.Total were renamed to lose the dot because the dot gets in the way when calling the feature in python.\nSecond Week Data Analysis In the first plot, we can see how much unbalanced our data set is. There\u0026rsquo;re over 5000 clients that didn\u0026rsquo;t leave the company and a little less than 2000 that left.\nI experimented with oversampling the dataset to handle this imbalance, but it made the machine learning models worse. And undersampling isn\u0026rsquo;t an option with this dataset size, so I just decided to leave it the way it is, and when it\u0026rsquo;s time to split the training and test set I\u0026rsquo;ll stratify the dataset by the Churn feature.\nI also generated 16 plots for all the discrete data, to see all the plots check this notebook. I wanted to see if there was any behavior that made some clients more likely to leave the company. Is clear that all, except for gender, seems to play a role in determining if a client will leave the company or not. More specifically payment methods, contracts, online backup, tech support, and internet service.\nIn the tenure plot, I decided to make a distribution plot for the tenure, one plot for clients that didn\u0026rsquo;t churn and another for the clients that did churn. We can see that clients that left the company tend to do so at the beginning of their time in the company.\nThe average monthly charge for clients that didn\u0026rsquo;t churn is 61.27 monetary units, while clients that churn were paying 74.44. This is probably because of the type of contract they prefer, but either way is known that higher prices drive the customers away.\nThe Churn Profile Considering everything that I could see through plots and measures. I came up with a profile for clients that are more likely to churn.\nNew clients are more likely to churn than older clients.\nCustomers that use fewer services and products tend to leave the company. Also, when they aren\u0026rsquo;t tied down to a longer contract they seem to be more likely to quit.\nRegarding the payment method, clients that churn have a strong preference for electronic checks and usually are spending 13.17 monetary units more than the average client that didn\u0026rsquo;t leave.\nThird Week Preparing the dataset We start by making dummies variables dropping the first, so we would have n-1 dummies for n categories. Then we move on to look at features correlation.\nWe can see that the InternetService_No feature has a lot of strong correlations with many other features, this is because these other features depend on the client having internet service. So I\u0026rsquo;ll drop all features that are dependent on this one. The same thing happens with PhoneService_Yes.\ntenure and ChargesTotal also have a strong correlation, so I tried running the models without one of them and both, and it had a worse performance and took a long time to converge, so I decided to keep them as they are relevant as well.\nAfter dropping the features I finish preparing the dataset by normalizing the numeric data, ChargesTotal and tenure.\nTest and training dataset I split the dataset into training and testing sets, 20% for testing and the rest for training. I stratified the data by the Churn feature and I shuffle the dataset before splitting. The same split is used by all the models. After splitting the dataset I decided to oversample the train data using SMOTE1 because the dataset is imbalanced. The reason that I only used this technique on the training set is that I don\u0026rsquo;t want to have a biased result, oversampling all the datasets would mean that I\u0026rsquo;d be testing my models on the same data that I trained, and that\u0026rsquo;s not the goal here.\nModel Evaluation I\u0026rsquo;ll use a dummy classifier to have a baseline model for the accuracy score, and I\u0026rsquo;ll also use the metrics: precision, recall and f1 score2. Although the dummy model won\u0026rsquo;t have values for this metrics, I\u0026rsquo;ll keep it for comparison on how much the models improved.\nBaseline I made the baseline model using a dummy classifier that guessed that every client behaved the same. It is always guessed that no client will leave the company. By using this approach we got a baseline accuracy score of 0.73456.\nAll models moving forward will have the same random state.\nModel 1 - Random Forest I start by using a grid search with cross-validation to find the best parameters within a given pool of options using the recall as the strategy to evaluate the performance. The best model was:\n1 RandomForestClassifier(criterion=\u0026#39;entropy\u0026#39;, max_depth=5, max_leaf_nodes=70, random_state=22) After fitting this model, the evaluating metrics were:\nAccuracy Score: 0.72534 Precision Score: 0.48922 Recall Score: 0.78877 F1 Score: 0.60389 Model 2 - Linear SVC For this model, I just used the default parameters and set the ceiling for the maximum of iterations to 900000.\n1 LinearSVC(max_iter=900000, random_state=22) After fitting this model, the evaluating metrics were:\nAccuracy Score: 0.71966 Precision Score: 0.48217 Recall Score: 0.75936 F1 Score: 0.58982 Model 3 - Multi-layer Perceptron Here I fixed the solver to LBFGS, because according to the documentation it has a better performance in smaller datasets3, and used grid search with cross-validation to find a hidden layer size that would be the best. The best model was:\n1 MLPClassifier(hidden_layer_sizes=(1,), max_iter=9999, random_state=22, solver=\u0026#39;lbfgs\u0026#39;) After fitting this model, the evaluating metrics were:\nAccuracy Score: 0.72818 Precision Score: 0.49133 Recall Score: 0.68182 F1 Score: 0.57111 Conclusion After running the three models, all of them used the same random_state. I got the following accuracy scores and improvements (compared to the baseline model):\nIn the end, the Random Forest had the best metrics overall. This model can recall a great portion of clients that churn correctly, still is not perfect but is certainly a starting point. The accuracy score is not as high as I\u0026rsquo;d like, but in this particular problem, the goal is to keep clients from leaving the company and is better to use resources to keep a client that will not leave than to do nothing.\nIn the end, I liked this challenge, because I don\u0026rsquo;t usually practice machine learning, but thanks to the challenge I got the chance to make a small project in this area that is so relevant and important. This was my first time working with neural networks and tunning hyper-parameters, and I\u0026rsquo;m sure the next time I\u0026rsquo;ll get even better results.\nimbalanced-learn documentation\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nAccuracy, Precision, Recall or F1? - Koo Ping Shung\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nscikit-learn documentation\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://devmedeiros.com/post/2022-05-30-churn-rate-challenge/","summary":"Alura hosted a four-week Data Science Challenge using an imbalanced dataset of Churn Rate of a company Alura Voz","title":"Data Science Challenge - Churn Rate"},{"content":"What is UX/UI? UX is the acronym for User Experience, it is a recent concept about making design decisions while thinking about the user experience. The UX designer needs to be concerned about whether their product is easy to use and intuitive, making changes to it whenever necessary to suit the user\u0026rsquo;s needs.\nUI stands for User Interface. It is everything involved in the interaction of the user and the product. The UI designer is responsible for developing interfaces, not only limited to the visual aspects but also ensuring that the interfaces are functional and usable and generally contribute to a good user experience.\nHow to Improve the Experience of BI Dashboards? Many people see BI dashboards as web pages. This comes with some expectations. For example, most sites that have some navigation system use a top menu with buttons, a side menu (most common in Brazil being on the left, but in some countries, it is on the right), or a hamburger menu (the one we click on it and the options appear).\nJaqueline Medeiros - All rights reserved\nWith this, a majority of people who use the dashboards expect to find navigation buttons and data segmentation (filter) in these places, in addition to other information such as logo and title.\nData Segmentation Also commonly called a data filter, it is a fundamental part of several dashboards, its positioning needs to be defined carefully, because if it is in a place that the user does not expect, it can prevent your dashboard from being used efficiently, in addition to maintaining a visual standard for everyone its filters help people more easily recognize what is and isn\u0026rsquo;t a filter.\nYou can and should use and experiment with different themes in your projects. What matters, when making it easier for users, is consistency, pick a model for your filters with the desired colors and all the graphic specifications that are of interest to you and use it in all filters, as this will help people quickly find it and recognize that it\u0026rsquo;s a filter.\nIt\u0026rsquo;s common for people when they are reading something on the computer to position the mouse pointer where they are reading. For Power BI, this makes it easier for the user to find the clear data segmentation button because when hovering the mouse over the filter name, a rubber appears on the left side where the filter name is. If you use Power BI, you probably already knew that, but we can improve this by using something that the user already knows, which is the rubber, and create a button using it as an icon and make this button clear all filters at the same time. If you want to know how to make this button here, in the Power BI forum, explains how.\nThis feature needs may be unnoticed at first. Regularly dashboard users don\u0026rsquo;t realize which filters they have used or they use so many that they just want to be able to clear the selection faster and more efficiently, so this button makes the process easier.\nPage Navigation Power BI\u0026rsquo;s native page navigation is not intuitive for most people and you may want to direct the navigation in a specific flow that facilitates understanding and contributes to the intended storytelling. In this case, we have the option to hide all the report tabs, except for the opening/home page. But what\u0026rsquo;s the best way to direct the user to the other pages? Suppose your report is simple, you have an overview tab and another tab with a breakdown, a simple button would solve your problem, but if your report is extensive it may be unfeasible to put a button for each tab on all pages.\nJaqueline Medeiros - All rights reserved\nIn this case, it might be interesting to consider having a home page that leads to all the other pages and placing a home or back button on them.\nHow to Improve Dashboards Interface? Prototyping Programs Using a specific program to prototype your dashboard allows for greater artistic freedom compared to what Business Intelligence applications typically provide. Figma is a great tool for this, you can create advanced backgrounds and prototypes with amazing quality to use in your BI dashboards.\nHere\u0026rsquo;s an example of a dashboard I made a few months ago:\nPanel with data The background of this panel was done completely in Figma, even some of the titles of the BI visuals.\nBackground made in Figma You can check out more Power BI Dashboards I made for Alura Challenge BI.\nFigures and Icons Vector created by pch.vector - br.freepik.com\nFigures and icons when used correctly help make the panel stand out, and make it more eye-catching and beautiful. There are several ways to get images, if you or your team can\u0026rsquo;t create them yourself there is the option of using online platforms that provide vectorized images. On these platforms, there are free options, which require some type of attribution, and premium (paid) options, which often do not need to attribute the author and have a higher quality.\nICONS8 icon\n","permalink":"https://devmedeiros.com/post/2022-04-29-ux-power-bi-dashboards/","summary":"Defining UX/UI in the context of Bussiness Intelligence for Dashboards with examples","title":"UX/UI for Business Intelligence Dashboards"},{"content":"The Dataset CEAP stands for Cota para o Exercício da Atividade Parlamentar, in English, Exercise of Parliamentary Activity from Brazil. The quota is a monthly amount that can be used by the deputy/senator to defray typical expenses of the exercise of the parliamentary mandate.\nThe monthly amounts don\u0026rsquo;t accumulate over the months. Another important thing is Senators in Brazil have an 8-year mandate, and there are elections every 4 years.\nThe main dataset is provided by the Brazilian government through the transparency portal.1 I gathered additional data to link the quota for each senator and federal unit.23\nData Analysis Total Amount Refunded by Year On the image above we can see how much was refunded by year. The year 2022 is the smallest because is not over yet. The data was collected on April 16, 2022.\nThere seems to be a trend to spend slightly less money on election years.\nThe first three years were the three lowest spending years, 11.52M, 11.61M, and 10.64M from 2008 to 2010, respectively. This was probably because in the following years the expenses that could be refunded increased in 2011.\nTop Highest and Lowest Average Refunded Percentages by Senators These percentages represent the quota percentage a given senator refunded on average.\nIn the image above, it\u0026rsquo;s seen highest refunded senators note that they’re all closed together, the highest refundee, on average, is João Capiberibe, using 99.78% of his quota. On the lowest side (image below), it\u0026rsquo;s presented the lowest use of the refund quota, on average. Nailde Panta, is the lowest refundee, on average, using just 4.1% of her quota.\nExpense Types Over the Years Here we can see that in the years 2008 to 2010 there wasn’t any refund by transport and private security services.\nIn 2008, the main expenses were publication of parliamentary activity (over R$ 7,000.00) and locomotion/accommodation (almost R$ 6,500.00). In the subsequent years, those expenses dropped in half, for the publications and to less than R$ 1,000.00 for locomotion.\nIn 2013 and 2014 the expense of private security was the highest ever with over R$ 4,000.00. Probably due to protests in those years.\nTo learn more To learn how The Chamber of Deputies from Brazil works click here.\nThis is presentation is for learning purposes, from a challenge proposed by Alura #7DaysOfCode. In case you want to know what else can be done with the data, check the Serenata de Amor. It\u0026rsquo;s a Brazilian project that uses AI to track refunded requests by deputies.\nSee my code at my GitHub repo, there you can also find a slide presentation, learn about how the data was cleaned, and know more about the next task from the challenge.\nReferences https://www12.senado.leg.br/transparencia/dados-abertos-transparencia/dados-abertos-ceaps\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://pt.wikipedia.org/wiki/Lista_de_senadores_do_Brasil#Nova_Rep%C3%BAblica_(1987%E2%80%932023)\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://www2.camara.leg.br/legin/int/atomes/2009/atodamesa-43-21-maio-2009-588364-publicacaooriginal-112820-cd-mesa.html\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://devmedeiros.com/post/2022-04-17-storytelling-with-ceap/","summary":"The storytelling of how much each deputy/senator spends in Brazil","title":"Storytelling with CEAP"},{"content":"What is the p-value? In statistics, we have hypothesis tests, which are made to decide whether to reject or not reject the null hypothesis. Some examples of hypothesis testing are Neyman-Pearson, Shapiro-Wilk, Student\u0026rsquo;s T-test, and others.\nAll hypothesis tests have a test statistic specific to them. And this statistic is used to evaluate the test, but this task can be tiring even with the use of computers because most software does not return the comparison statistic as an output, only the sample statistic. In this case, the p-value comes to facilitate this comparison, as it is already a representation of this test statistic. It represents the probability of obtaining a test statistic equal to or more extreme than the one calculated in your sample, considering the null hypothesis to be true.\nThis makes it easier because by knowing the confidence level that you want to test your hypothesis, you just need to compare if the p-value is smaller or greater than your confidence level, while if you were to use the test statistic it would still be necessary to calculate the statistic for each different confidence level than you would compare. So suppose you want to compare 1%, 5%, and 10%, you would have to calculate three different test statistics to compare your sample statistic.\nHow to use it? It is very common when we are learning statistics on our own, to read that if the p-value is less than 5% we reject the hypothesis or that if it is higher we \u0026ldquo;accept\u0026rdquo; the hypothesis.\nThe value with which we compare the p-value must be defined together with people from the business area you are working with, it is very common in some sectors to use a very small p-value such as 1% or even 0.1% and in others use values ​​greater than 5%.\nNote that a p-value of 0.02 would be rejected if we consider α = 1%, but not if α = 5%. When in doubt about which one to use, it is first recommended to make this decision before taking the test. Second, keep in mind that an α = 1% will have a confidence of 99% (1-α), whereas if it were 5% it would be only 95%. It might seem like it\u0026rsquo;s best to take the value that gives you the most \u0026ldquo;confidence\u0026rdquo;, but a very small p-value can lead to more rejections of your hypothesis.\nThe truth is that hypothesis tests only give us the rejection information, when a null hypothesis is not rejected, it means that no evidence was found that contradicts what it claims, it does not mean that we have proved it to be correct. So you have to be very careful when using the p-value.\nInterpreting the measure A very common way of interpreting the measure is \u0026ldquo;The null hypothesis is rejected, with α% confidence\u0026rdquo;, in the case of rejection (where the p-value \u0026gt; α) and the case of no rejection (p-value \u0026lt; α) \u0026ldquo;Not enough evidence was found to reject the null hypothesis, with α% confidence\u0026rdquo;.\n","permalink":"https://devmedeiros.com/post/2022-04-12-comprehending-the-p-value/","summary":"P-value definition, how to use it, and interpreting the measurement","title":"Comprehending the P Value"},{"content":" The show is presented by Tyler Renelle from Depth, since 2021 it has been redone to update the content. The podcast offers a resource list where you can find all of the books, courses, and sites mentioned during the podcast. It\u0026rsquo;s a great podcast for anyone who is learning about machine learning, it can be useful for a complete beginner, an enthusiast, or someone who is looking to further even more on this topic.\nThe show starts by talking about what is data science, how it relates to machine learning and artificial intelligence. It also talks about the first attempts humans had made to create AI, with some examples coming from as early as the 13th century.\nI like that the show talks about concepts of machine learning in a different way than I\u0026rsquo;m used to. When people are talking about machine learning, as a Statistician, I always feel that I understand what people are talking about, except I never knew the keywords they were using. What machine learning engineers might call features I was taught as variable, so I got confused when talking to friends about machine learning when they came from a different background as me (e.g. computer science). This is one of the reasons why I like the podcast so much because it talks about deep details about every machine learning concept so if it is talking about something that I already know I can now make that connection.\n","permalink":"https://devmedeiros.com/post/2022-01-24-podcast-review-mlg/","summary":"A review about the podcast Machine Learning Guide","title":"Podcast Review - Machine Learning Guide"},{"content":"Recently I finished an Alura course named Python for Data Science and I want to put what I learned into practice, to do so I\u0026rsquo;ll make a descriptive analysis on this dataset Amazon Top 50 Bestselling Books 2009 - 2019. It contains 550 books and the data has been categorized as fiction and non-fiction by Goodreads. All of the code can be found here.\nI started checking the first five observations from the dataset.\nName Author User Rating Reviews Price Year Genre 10-Day Green Smoothie Cleanse JJ Smith 4.7 17350 8 2016 Non Fiction 11/22/63: A Novel Stephen King 4.6 2052 22 2011 Fiction 12 Rules for Life: An Antidote to Chaos Jordan B. Peterson 4.7 18979 15 2018 Non Fiction 1984 (Signet Classics) George Orwell 4.7 21424 6 2017 Fiction 5,000 Awesome Facts (About Everything!) (Natio\u0026hellip; National Geographic Kids 4.8 7665 12 2019 Non Fiction Here it\u0026rsquo;s possible to see that the data has the Year in which the book was on the top 50 list, it\u0026rsquo;s Price, the average User Rating, total Reviews, Author, Name and lastly, Genre.\nThere are no null values in the dataset. And from 550 books there are 248 unique authors, so let\u0026rsquo;s see which authors have had more books in the top 50 bestselling during this period.\nAuthor Number of Books Jeff Kinney 12 Gary Chapman 11 Rick Riordan 11 Suzanne Collins 11 American Psychological Association 10 Dr. Seuss 9 Gallup 9 Rob Elliott 8 Stephen R. Covey 7 Stephenie Meyer 7 Dav Pilkey 7 Bill O\u0026rsquo;Reilly 7 Eric Carle 7 The author with more books in the top 50 list was Jeff Kinney, tied at second, with 11 books, was Gary Chapman, Rick Riordan, and Suzanne Collins. Tied at 9th is Stephen R. Covey, Stephenie Meyer, Dav Pilkey, Bill O\u0026rsquo;Reilly, and Eric Carle, with 7 books.\nWith the violing plot, we can see how the user rating is concentrated and because our data is composed of bestsellers it makes sense that the user rating is mostly concentrated around 4.5 and 4.75.\nThis boxplot of reviews count by year shows that the variability increases through the years, having its peak at 2014 and gradually stabilizing. We can also see that in the first years, 2010 and 2011, there were more outliers in the data.\nI wanted to look at the user rating and price by book genre. So I calculated these average values.\nGenre User Rating Price Fiction 4.65 10.85 Non Fiction 4.60 14.84 The user rating average by genre seems to be similar just 0.05 difference, but the price has a bigger difference 10.85 for fiction and 14.84 for non-fiction books. To be sure that these differences are statistically significant I\u0026rsquo;ll use the Mann-Whitney test.\nThe Mann-Whitney null hypothesis is that the samples have the same distribution, and in both cases, we reject the null hypothesis with a 95% confidence level. The p-value for the price data was 8.34e-08 and the user rating was 1.495e-07.\nTo visually show how different their distribution is we can take a look at the following plots.\nThe distribution for the price of fiction books is heavily inclined to the left and consistently diminishes as the price goes up. While the non-fiction books price starts high and becomes even higher, 120 and almost 140 occurrences in the first two categories, then it rapidly diminishes.\nThe distribution for the user rating by the fiction genre slowly increases, having its peek at around 4.8. And the distribution of the non-fiction genre has its peak at a little over 4.6.\n","permalink":"https://devmedeiros.com/post/2021-12-28-amazon-top-50-books/","summary":"Exploratory Data Analysis of Amazon\u0026rsquo;s top 50 bestselling books 2009 - 2019","title":"EDA Amazon Top 50 Bestselling Books"},{"content":"Introduction I\u0026rsquo;m learning data visualization in Python and I see myself as a \u0026lsquo;hands on\u0026rsquo; learner, so I\u0026rsquo;ll be reproducing some basic plots using seaborn package that you can use as a reference everytime you need to fresh up your memory.\nAt first is required that the packages are properly imported, after that I load the iris dataset.\n1 2 3 4 5 6 7 import pandas as pd import seaborn as sns import matplotlib.pyplot as plt url = \u0026#34;https://git.io/JXciW\u0026#34; iris = pd.read_csv(url) If you\u0026rsquo;re not familiar with the iris dataset, you can see its first five rows below:\nsepal_length sepal_width petal_length petal_width species 5.1 3.5 1.4 0.2 setosa 4.9 3.0 1.4 0.2 setosa 4.7 3.2 1.3 0.2 setosa 4.6 3.1 1.5 0.2 setosa 5.0 3.6 1.4 0.2 setosa Barplots To create simple barplots.\n1 sns.barplot(x=\u0026#34;species\u0026#34;, y=\u0026#34;petal_width\u0026#34;, data=iris) Making a horizontal barplot.\n1 sns.barplot(x=\u0026#34;petal_width\u0026#34;, y=\u0026#34;species\u0026#34;, data=iris) Custom bar order.\n1 2 3 4 5 sns.barplot( x=\u0026#34;species\u0026#34;, y=\u0026#34;petal_width\u0026#34;, data=iris, order=[\u0026#34;virginica\u0026#34;, \u0026#34;setosa\u0026#34;, \u0026#34;versicolor\u0026#34;]) Add caps to error bars.\n1 sns.barplot(x=\u0026#34;species\u0026#34;, y=\u0026#34;petal_width\u0026#34;, data=iris, capsize=.2) Barplot withough error bar.\n1 sns.barplot(x=\u0026#34;species\u0026#34;, y=\u0026#34;petal_width\u0026#34;, data=iris, ci=None) Scatterplots A simple scatterplot.\n1 sns.scatterplot(x=\u0026#34;sepal_width\u0026#34;, y=\u0026#34;petal_width\u0026#34;, data=iris) Mapping groups to scatterplot.\n1 sns.scatterplot(x=\u0026#34;sepal_width\u0026#34;, y=\u0026#34;petal_width\u0026#34;, data=iris, hue=\u0026#34;species\u0026#34;) Mapping groups and scalling scatterplot.\n1 2 3 4 5 6 sns.scatterplot( x=\u0026#34;sepal_width\u0026#34;, y=\u0026#34;petal_width\u0026#34;, data=iris, hue=\u0026#34;sepal_length\u0026#34;, size=\u0026#34;sepal_length\u0026#34;) Legend and Axes To change the plot legend to the outside of the plot area, you can use bbox_to_anchor = (1,1), loc=2. The following plot has a custom title, a new x axis label, and a y axis label.\n1 2 3 4 5 6 7 8 sns.scatterplot(x=\u0026#34;sepal_width\u0026#34;, y=\u0026#34;petal_width\u0026#34;, data=iris, hue=\u0026#34;species\u0026#34;) plt.legend( title=\u0026#34;Species\u0026#34;, bbox_to_anchor = (1,1), loc=2) plt.xlabel(\u0026#34;Sepal Width\u0026#34;) plt.ylabel(\u0026#34;Petal Width\u0026#34;) plt.title(\u0026#34;Sepal Width x Petal Width\u0026#34;) ","permalink":"https://devmedeiros.com/post/2021-11-07-seaborn-package-guide/","summary":"A simple guide on how to make basic plots using the seaborn package from Python","title":"Python seaborn Package Guide"},{"content":"Introduction COVID-19 has changed many things in our day-to-day life, from ordering more delivery, working from home, or even just getting a new pet. And now I want to see if it has impacted traffic crashes in California. The dataset that I’ll be using comes from Kaggle. It covers collisions from January 2001 up to December 2020 from California. 1 The following are the tables present in the dataset:\ncase_id collisions parties victims The case_id contains the case_id and db_year. The collisions table contains all the information about each collision. While the parties table contains information about all the parties involved in the collisions, in this case, parties can be drivers, pedestrians, cyclists, and parked vehicles. The victims table contain information about all the victims, it also includes passengers.\nTo answer if the pandemic has impacted the collisions I need to separate my data. Knowing that the first case of COVID was on January 26, 2020, in California. 2 I’ll be separating the dataset as before COVID for every crash that happened before the first case and after COVID for every crash on the same day and forward.\nTo compare before COVID and after COVID I’ll looking at a couple of things, Proportion of DUIs and Fatality of Crashes.\nTo do this I’ll be querying the database with SQLite through R. For the complete code go to my GitHub repo.\nProportion of DUIs Alcohol use has changed in the US during the pandemic 3 and with all the lockdowns and working from home we may wonder if this alcohol use has caused more crashes or not. With this in mind we take a look at our data about California.\nAs we can see from the following table, the percentages before and after COVID are very similar. Only 7.3% and 8.96% of violations occurred from DUIs.\nBefore COVID After COVID Violation Qnty Perc Qnty Perc DUI 653467 7.3 42169 8.96 Other 8300399 92.7 428299 91.04 Fatality of Crashes Moving on to our next task, to see if crashes after COVID became more or less fatal. The analysis is going to look at how many people died from collisions, how many got some injury, and no injury at all.\nLooking at the table below you can see that the percentage of collisions that resulted in someone dying was 0.74% before the pandemic and is now 1.29%, is not a big difference, but when you look at no injury, you can see that before COVID 45.24% of collisions didn\u0026rsquo;t result in someone having an injury and after the pandemic, this number drops to 18.57%. This could indicate some influence of the pandemic, but further research is needed.\nBefore COVID After COVID Degree of Injury Qnty Perc Qnty Perc Death 68875 0.74 4131 1.29 Some injury 5033610 54.02 257057 80.14 No injury 4216085 45.24 59576 18.57 References Alexander Gude and California Highway Patrol, “California Traffic Collision Data from SWITRS.” Kaggle, 2021, doi: 10.34740/KAGGLE/DSV/2569326.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nTimeline of Coronavirus US\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nPollard MS, Tucker JS, Green HD. Changes in Adult Alcohol Use and Consequences During the COVID-19 Pandemic in the US. JAMA Netw Open. 2020;3(9):e2022942. doi:10.1001/jamanetworkopen.2020.22942\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://devmedeiros.com/post/2021-11-02-sql-california-traffic/","summary":"Using SQL to analyze if COVID impacted California Traffic Collisions","title":"Have COVID Impacted California Traffic Collisions?"},{"content":"The data.table package is one of the fastest packages for data manipulation, currently, it is even faster than pandas and dplyr 1. data.table syntax is dt[i, j, by], where:\ni is used to subset rows j is used to subset columns by is used to subset groups, like GROUP BY from SQL You can read it out loud as2:\nTake dt, subset/reorder rows using i, then calculate j, grouped by by.\nA data.table is also a data.frame and all of the basic data manipulations you can use in data.frames applies to data.table. Like ncol(), nrow(), names(), summary(). But it has more possibilities, for instance in data.table there is a special variable .N which is an integer that contains the row number in the group. If you use dt[.N] you\u0026rsquo;ll get the last row of your data.table.\nAnother cool feature of data.table is that if you want filter/subset a column you don\u0026rsquo;t need to use df$x[df$x == 1] you can simple use dt[x == 1] which make your code much more readable and clean.\nYou also get to use special operators: %like%, %in% and %between%. These operators work like SQL operators, LIKE, IN, and BETWEEN, respectively.\nIf you are familiar with SQL there is this one thing the package offers that will catch your eye. It\u0026rsquo;s called chaining, which allows you to perform a sequence of operations in a data.table, you just need to use dt[][] and chain multiple operations \u0026ldquo;[]\u0026rdquo;.\nBut that\u0026rsquo;s not all with the operator := you can alter data without making a new copy in the memory.\nIf you want to start using the package I suggest you use the cheatsheet. It\u0026rsquo;s really useful if you already have a basic knowledge about data.frames.\nhttps://h2oai.github.io/db-benchmark/\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://cran.r-project.org/web/packages/data.table/vignettes/datatable-intro.html\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://devmedeiros.com/post/2021-10-27-data-table/","summary":"One of the fastest packages for data manipulation on R","title":"An Overview on data.table R Package"},{"content":"I got inspired to simulate a blackjack game, so I decided to make a series of functions to emulate the dealer\u0026rsquo;s behavior, a newbie player, a cautious player, and a strategist. With this set of functions, you can run a single game with p players, d decks, any combination of players archetypes. And you can also run it n times.\nI also made a shiny app to display how the simulation works. In the app, you are limited by the number of players, but if you wish to run the code with more players you can check the GitHub repository. There you can find the rules considered for the simulation and the complete code.\nIf you are familiar with the R language you can also run the app locally, you just need to run the library(shiny) and runGitHub(\u0026quot;blackjack-simulation\u0026quot;, \u0026quot;devmedeiros\u0026quot;, ref = \u0026quot;main\u0026quot;).\nThe app is composed of a sidebar with a space to choose the archetypes, the number of decks to use, and how many rounds you want to see simulated. Depending on how many rounds you choose it can get slow as it\u0026rsquo;ll simulate according to your choices every time you hit the RUN SIMULATION RUN.\nIn the plot tab, we have the evolution of the lose rate through the rounds.\nA 1 means the player lost that round and a 0 means the player won.\nThe game setup tab shows all the cards dealt in the simulation, each card was dealt from left to right and a blank cell means the player didn\u0026rsquo;t ask for another card (hit).\nAnd lastly, the lose rate tab shows us the same data from the plot tab, but now as a table. This is useful if you want to take a deeper look at how one strategy was better than the other.\n","permalink":"https://devmedeiros.com/post/2021-10-24-blackjack-simulation/","summary":"A blackjack simulation using R and Shiny, you can run a single game with p players, d decks, n times","title":"Blackjack Simulation"},{"content":"I want to tackle sentiment analysis using R in a simple way, just to get me started. With this in mind we begin loading all the packages we\u0026rsquo;ll be using.\n1 2 3 4 5 6 library(readr) library(dplyr) library(tidytext) library(tokenizers) library(stopwords) library(ggplot2) Then we need to load our dataset. This data comes from Kaggle Fake and real news dataset.\n1 2 Fake \u0026lt;- read_csv(\u0026#39;~/fakenews/Fake.csv\u0026#39;) True \u0026lt;- read.csv(\u0026#39;~/fakenews/True.csv\u0026#39;) I want to merge the two datasets, but first we need to create a new column that will tell me where the data came from.\n1 2 3 4 Fake$news \u0026lt;- \u0026#39;fake\u0026#39; True$news \u0026lt;- \u0026#39;real\u0026#39; data \u0026lt;- rbind(Fake,True) Now we can start the data cleaning. In this first moment, we\u0026rsquo;ll do a simple tokenization on the title and text variables. Then we\u0026rsquo;ll be removing the stopwords according to the snowball source from the stopwords package.\n1 2 3 4 5 6 7 8 9 10 11 title \u0026lt;- tibble(news = data$news, text = data$title) corpus \u0026lt;- tibble(news = data$news, corpus = data$text) tidy_title \u0026lt;- title %\u0026gt;% unnest_tokens(word, text, token = \u0026#39;words\u0026#39;) %\u0026gt;% filter(!(word %in% stopwords(source = \u0026#39;snowball\u0026#39;))) tidy_corpus \u0026lt;- corpus %\u0026gt;% unnest_tokens(word, corpus, token = \u0026#39;words\u0026#39;) %\u0026gt;% filter(!(word %in% stopwords(source = \u0026#34;snowball\u0026#34;))) With the tidy data we can select the ten most frequent words from which title news\u0026rsquo; group.\n1 2 3 4 5 p0 \u0026lt;- tidy_title %\u0026gt;% group_by(news, word) %\u0026gt;% summarise(n = n()) %\u0026gt;% arrange(desc(n)) %\u0026gt;% slice(1:10) Fake news titles mention video and trump by a large margin, 8477 and 7874 respectively. On the real news titles, trump is also one of the most mentioned, coming on first with 4883 appearances, followed by u.s, 4187, and says with 2981.\nNow we prepare the data to the sentiment analysis. I\u0026rsquo;m interested in classifing the data into sentiments of joy, anger, fear or surprise, for example. So I\u0026rsquo;ll be using the nrc dataset from Saif Mohammad and Peter Turney.\n1 2 3 4 5 p1 \u0026lt;- tidy_title %\u0026gt;% inner_join(get_sentiments(\u0026#39;nrc\u0026#39;)) %\u0026gt;% group_by(sentiment, news) %\u0026gt;% summarise(n = n()) %\u0026gt;% mutate(prop = n/sum(n)) Disgust seems to be the most common sentiment around fake news titles while trust is the lowest, even though it still is more than 50%. Overall fake news titles seems to have more \u0026ldquo;sentiment\u0026rdquo; than real news in this particular dataset. Even positive sentiments like joy and surprise.\n1 2 3 4 5 p2 \u0026lt;- tidy_corpus %\u0026gt;% inner_join(get_sentiments(\u0026#39;nrc\u0026#39;)) %\u0026gt;% group_by(sentiment, news) %\u0026gt;% summarise(n = n()) %\u0026gt;% mutate(prop = n/sum(n)) For the news\u0026rsquo; corpus we can see the same sentiments are prevalent, but the proportion is lower compared to the title. A fake news article loses trust when the reader takes more time to read it. It also becames less negative and shows less fear.\nAn improvement we could do here is to use our own stopwords and change the way we made the tokens. We had instances were trump and trump\u0026rsquo;s didn\u0026rsquo;t correspond to the same thing and if we had used this data to train a model this could become problematic.\n","permalink":"https://devmedeiros.com/post/2021-10-12-fakenews-sentiment/","summary":"Sentiment Analysis comparing Fake News and Real News using R","title":"Sentiment Analysis of Fake News vs Real News"},{"content":"Hi! My name is Jaqueline Medeiros and I\u0026rsquo;m an Analytics Engineer from Brazil. I\u0026rsquo;ve always liked learning new things and exploring new possibilities and this blog is an outlet for me to create and discover new things related to Data Science.\nI want to write posts that people can read without a technical background, but I also want to showcase and talk about the coding aspects of what I\u0026rsquo;m currently learning, so you\u0026rsquo;ll be able to find a little bit of everything here.\nYou can find me at:\nmedeiros-jaqueline\ndevmedeiros\nrésumé\nDisclaimer This is my personal website. The opinions expressed here are my own and do not reflect the views of my current or previous employers. If you would like to learn about the terms and policies, please visit this page.\n","permalink":"https://devmedeiros.com/about/","summary":"Jaqueline Medeiros, Analytical Engineer passionate about Data Science. In her blog she mixes the technical with the accessible, offering content about programming and discoveries for readers of different skill levels.","title":"About"},{"content":"The following are a few projects that I have worked on in my free time. They are in no particular order.\nNota Fiscal Goiana Description: This project presents a complete solution to ETL the data from Nota Fiscal Goiana. It also presents the curated data in a Streamlit dashboard.\nStacks: Python, Power BI, Streamlit, SQLite, and Github Actions\nSkills: ETL, Data Visualization, and Scraping\nCredit Score Classification App Description: This project cleans a credit score dataset, and builds and serializes a machine learning model. This ML model is used on a Streamlit app where you can enter fictional data and get a credit score classification.\nStacks: Python (pandas, seaborn, matplotlib, numpy, sklearn, and pickle) and Streamlit\nSkils: Machine Learning, Data Cleaning, and Data Visualization\nPersonal blog Description: A blog (where you are right now!) in that I write about what I\u0026rsquo;m learning, basically a portfolio.\nStacks: Hugo, Papermod theme, and Github Actions\nRaio X - Entendendo o Tempo de Julgamento dos Processos Judiciais Description: This project marked my team’s participation in the “Hackathon CNJ - Desafio Tempo e Produtividade”.\nStacks: R, RShiny, PostgreSQL, SQLite, and Machine Learning\nSkills: ETL, Data Visualization, and Neural Network\nAlura 1st Data Science Challenge Description: This project presents my take on the first Alura Data Science Challenge. I was challenged to clean a dataset and predict the churn rate from imbalanced data.\nStacks: Python (pandas, seaborn, matplotlib, numpy, and sklearn)\nSkils: Machine Learning, Data Cleaning, Imbalanced Data, and Data Visualization\nAlura 2nd BI Challenge Description: This project presents my take on the second Alura BI Challenge. It consisted of making three Power BI interactive reports. Each presented its challenge.\nStacks: Power BI\nSkils: Data Visualization\n","permalink":"https://devmedeiros.com/projects/","summary":"Explore my personal projects on GitHub, including a credit score classification app, Nota Fiscal Goiana, and Alura\u0026rsquo;s Business Intelligence challenge.","title":"Projects"}]