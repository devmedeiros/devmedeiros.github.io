[{"content":"The show is presented by Tyler Renelle from Depth, since 2021 it has been redone to update the content. The podcast offers a resource list where you can find all of the books, courses, and sites mentioned during the podcast. It\u0026rsquo;s a great podcast for anyone who is learning about machine learning, it can be useful for a complete beginner, an enthusiast, or someone who is looking to further even more on this topic.\nThe show starts by talking about what is data science, how it relates to machine learning and artificial intelligence. It also talks about the first attempts humans had made to create AI, with some examples coming from as early as the 13th century.\nI like that the show talks about concepts of machine learning in a different way than I\u0026rsquo;m used to. When people are talking about machine learning, as a Statistician, I always feel that I understand what people are talking about, except I never knew the keywords they were using. What machine learning engineers might call features I was taught as variable, so I got confused when talking to friends about machine learning when they came from a different background as me (e.g. computer science). This is one of the reasons why I like the podcast so much because it talks about deep details about every machine learning concept so if it is talking about something that I already know I can now make that connection.\n","permalink":"https://devmedeiros.github.io/post/2022-01-24-podcast-review-mlg/","summary":"The show is presented by Tyler Renelle from Depth, since 2021 it has been redone to update the content. The podcast offers a resource list where you can find all of the books, courses, and sites mentioned during the podcast. It\u0026rsquo;s a great podcast for anyone who is learning about machine learning, it can be useful for a complete beginner, an enthusiast, or someone who is looking to further even more on this topic.","title":"Podcast Review - Machine Learning Guide"},{"content":"Recently I finished an Alura course named Python for Data Science and I want to put what I learned into practice, to do so I\u0026rsquo;ll make a descriptive analysis on this dataset Amazon Top 50 Bestselling Books 2009 - 2019. It contains 550 books and the data has been categorized as fiction and non-fiction by Goodreads. All of the code can be found here.\nI started checking the first five observations from the dataset.\n   Name Author User Rating Reviews Price Year Genre     10-Day Green Smoothie Cleanse JJ Smith 4.7 17350 8 2016 Non Fiction   11/22/63: A Novel Stephen King 4.6 2052 22 2011 Fiction   12 Rules for Life: An Antidote to Chaos Jordan B. Peterson 4.7 18979 15 2018 Non Fiction   1984 (Signet Classics) George Orwell 4.7 21424 6 2017 Fiction   5,000 Awesome Facts (About Everything!) (Natio\u0026hellip; National Geographic Kids 4.8 7665 12 2019 Non Fiction    Here it\u0026rsquo;s possible to see that the data has the Year in which the book was on the top 50 list, it\u0026rsquo;s Price, the average User Rating, total Reviews, Author, Name and lastly, Genre.\nThere are no null values in the dataset. And from 550 books there are 248 unique authors, so let\u0026rsquo;s see which authors have had more books in the top 50 bestselling during this period.\n   Author Number of Books     Jeff Kinney 12   Gary Chapman 11   Rick Riordan 11   Suzanne Collins 11   American Psychological Association 10   Dr. Seuss 9   Gallup 9   Rob Elliott 8   Stephen R. Covey 7   Stephenie Meyer 7   Dav Pilkey 7   Bill O\u0026rsquo;Reilly 7   Eric Carle 7    The author with more books in the top 50 list was Jeff Kinney, tied at second, with 11 books, was Gary Chapman, Rick Riordan, and Suzanne Collins. Tied at 9th is Stephen R. Covey, Stephenie Meyer, Dav Pilkey, Bill O\u0026rsquo;Reilly, and Eric Carle, with 7 books.\nWith the violing plot, we can see how the user rating is concentrated and because our data is composed of bestsellers it makes sense that the user rating is mostly concentrated around 4.5 and 4.75.\nThis boxplot of reviews count by year shows that the variability increases through the years, having its peak at 2014 and gradually stabilizing. We can also see that in the first years, 2010 and 2011, there were more outliers in the data.\nI wanted to look at the user rating and price by book genre. So I calculated these average values.\n   Genre User Rating Price     Fiction 4.65 10.85   Non Fiction 4.60 14.84    The user rating average by genre seems to be similar just 0.05 difference, but the price has a bigger difference 10.85 for fiction and 14.84 for non-fiction books. To be sure that these differences are statistically significant I\u0026rsquo;ll use the Mann-Whitney test.\nThe Mann-Whitney null hypothesis is that the samples have the same distribution, and in both cases, we reject the null hypothesis with a 95% confidence level. The p-value for the price data was 8.34e-08 and the user rating was 1.495e-07.\nTo visually show how different their distribution is we can take a look at the following plots.\nThe distribution for the price of fiction books is heavily inclined to the left and consistently diminishes as the price goes up. While the non-fiction books price starts high and becomes even higher, 120 and almost 140 occurrences in the first two categories, then it rapidly diminishes.\nThe distribution for the user rating by the fiction genre slowly increases, having its peek at around 4.8. And the distribution of the non-fiction genre has its peak at a little over 4.6.\n","permalink":"https://devmedeiros.github.io/post/2021-12-28-amazon-top-50-books/","summary":"Recently I finished an Alura course named Python for Data Science and I want to put what I learned into practice, to do so I\u0026rsquo;ll make a descriptive analysis on this dataset Amazon Top 50 Bestselling Books 2009 - 2019. It contains 550 books and the data has been categorized as fiction and non-fiction by Goodreads. All of the code can be found here.\nI started checking the first five observations from the dataset.","title":"Descriptive Analysis of Amazon Top 50 Bestselling Books 2009 - 2019"},{"content":"Introduction I\u0026rsquo;m learning data visualization in Python and I see myself as a \u0026lsquo;hands on\u0026rsquo; learner, so I\u0026rsquo;ll be reproducing some basic plots using seaborn package that you can use as a reference everytime you need to fresh up your memory.\nAt first is required that the packages are properly imported, after that I load the iris dataset.\nimport pandas as pd import seaborn as sns import matplotlib.pyplot as plt url = \u0026#34;https://git.io/JXciW\u0026#34; iris = pd.read_csv(url) If you\u0026rsquo;re not familiar with the iris dataset, you can see its first five rows below:\n   sepal_length sepal_width petal_length petal_width species     5.1 3.5 1.4 0.2 setosa   4.9 3.0 1.4 0.2 setosa   4.7 3.2 1.3 0.2 setosa   4.6 3.1 1.5 0.2 setosa   5.0 3.6 1.4 0.2 setosa    Barplots To create simple barplots.\nsns.barplot(x=\u0026#34;species\u0026#34;, y=\u0026#34;petal_width\u0026#34;, data=iris) Making a horizontal barplot.\nsns.barplot(x=\u0026#34;petal_width\u0026#34;, y=\u0026#34;species\u0026#34;, data=iris) Custom bar order.\nsns.barplot( x=\u0026#34;species\u0026#34;, y=\u0026#34;petal_width\u0026#34;, data=iris, order=[\u0026#34;virginica\u0026#34;, \u0026#34;setosa\u0026#34;, \u0026#34;versicolor\u0026#34;]) Add caps to error bars.\nsns.barplot(x=\u0026#34;species\u0026#34;, y=\u0026#34;petal_width\u0026#34;, data=iris, capsize=.2) Barplot withough error bar.\nsns.barplot(x=\u0026#34;species\u0026#34;, y=\u0026#34;petal_width\u0026#34;, data=iris, ci=None) Scatterplots A simple scatterplot.\nsns.scatterplot(x=\u0026#34;sepal_width\u0026#34;, y=\u0026#34;petal_width\u0026#34;, data=iris) Mapping groups to scatterplot.\nsns.scatterplot(x=\u0026#34;sepal_width\u0026#34;, y=\u0026#34;petal_width\u0026#34;, data=iris, hue=\u0026#34;species\u0026#34;) Mapping groups and scalling scatterplot.\nsns.scatterplot( x=\u0026#34;sepal_width\u0026#34;, y=\u0026#34;petal_width\u0026#34;, data=iris, hue=\u0026#34;sepal_length\u0026#34;, size=\u0026#34;sepal_length\u0026#34;) Legend and Axes To change the plot legend to the outside of the plot area, you can use bbox_to_anchor = (1,1), loc=2. The following plot has a custom title, a new x axis label, and a y axis label.\nsns.scatterplot(x=\u0026#34;sepal_width\u0026#34;, y=\u0026#34;petal_width\u0026#34;, data=iris, hue=\u0026#34;species\u0026#34;) plt.legend( title=\u0026#34;Species\u0026#34;, bbox_to_anchor = (1,1), loc=2) plt.xlabel(\u0026#34;Sepal Width\u0026#34;) plt.ylabel(\u0026#34;Petal Width\u0026#34;) plt.title(\u0026#34;Sepal Width x Petal Width\u0026#34;) ","permalink":"https://devmedeiros.github.io/post/2020-11-07-seaborn-package-guide/","summary":"Introduction I\u0026rsquo;m learning data visualization in Python and I see myself as a \u0026lsquo;hands on\u0026rsquo; learner, so I\u0026rsquo;ll be reproducing some basic plots using seaborn package that you can use as a reference everytime you need to fresh up your memory.\nAt first is required that the packages are properly imported, after that I load the iris dataset.\nimport pandas as pd import seaborn as sns import matplotlib.pyplot as plt url = \u0026#34;https://git.","title":"Python seaborn Package Guide"},{"content":"Tools used: SQL, R\nCategory: Data Analysis\n Introduction COVID-19 has changed many things in our day-to-day life, from ordering more delivery, working from home, or even just getting a new pet. And now I want to see if it has impacted traffic crashes in California. The dataset that I’ll be using comes from Kaggle. It covers collisions from January 2001 up to December 2020 from California. 1 The following are the tables present in the dataset:\n case_id collisions parties victims  The case_id contains the case_id and db_year. The collisions table contains all the information about each collision. While the parties table contains information about all the parties involved in the collisions, in this case, parties can be drivers, pedestrians, cyclists, and parked vehicles. The victims table contain information about all the victims, it also includes passengers.\nTo answer if the pandemic has impacted the collisions I need to separate my data. Knowing that the first case of COVID was on January 26, 2020, in California. 2 I’ll be separating the dataset as before COVID for every crash that happened before the first case and after COVID for every crash on the same day and forward.\nTo compare before COVID and after COVID I’ll looking at a couple of things, Proportion of DUIs and Fatality of Crashes.\nTo do this I’ll be querying the database with SQLite through R. For the complete code go to my GitHub repo.\nProportion of DUIs Alcohol use has changed in the US during the pandemic 3 and with all the lockdowns and working from home we may wonder if this alcohol use has caused more crashes or not. With this in mind we take a look at our data about California.\nAs we can see from the following table, the percentages before and after COVID are very similar. Only 7.3% and 8.96% of violations occurred from DUIs.\n    Before COVID  After COVID      Violation Qnty Perc Qnty Perc   DUI 653467 7.3 42169 8.96   Other 8300399 92.7 428299 91.04    Fatality of Crashes Moving on to our next task, to see if crashes after COVID became more or less fatal. The analysis is going to look at how many people died from collisions, how many got some injury, and no injury at all.\nLooking at the table below you can see that the percentage of collisions that resulted in someone dying was 0.74% before the pandemic and is now 1.29%, is not a big difference, but when you look at no injury, you can see that before COVID 45.24% of collisions didn\u0026rsquo;t result in someone having an injury and after the pandemic, this number drops to 18.57%. This could indicate some influence of the pandemic, but further research is needed.\n    Before COVID  After COVID      Degree of Injury Qnty Perc Qnty Perc   Death 68875 0.74 4131 1.29   Some injury 5033610 54.02 257057 80.14   No injury 4216085 45.24 59576 18.57    References   Alexander Gude and California Highway Patrol, “California Traffic Collision Data from SWITRS.” Kaggle, 2021, doi: 10.34740/KAGGLE/DSV/2569326.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n Timeline of Coronavirus US\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n Pollard MS, Tucker JS, Green HD. Changes in Adult Alcohol Use and Consequences During the COVID-19 Pandemic in the US. JAMA Netw Open. 2020;3(9):e2022942. doi:10.1001/jamanetworkopen.2020.22942\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n  ","permalink":"https://devmedeiros.github.io/post/2021-11-02-sql-california-traffic/","summary":"\u003cp\u003e\u003cstrong\u003eTools used:\u003c/strong\u003e SQL, R\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eCategory:\u003c/strong\u003e Data Analysis\u003c/p\u003e\n\u003chr\u003e","title":"Have COVID Impacted California Traffic Collisions?"},{"content":"The data.table package is one of the fastest packages for data manipulation, currently, it is even faster than pandas and dplyr 1. data.table syntax is dt[i, j, by], where:\n i is used to subset rows j is used to subset columns by is used to subset groups, like GROUP BY from SQL  You can read it out loud as2:\n Take dt, subset/reorder rows using i, then calculate j, grouped by by.\n A data.table is also a data.frame and all of the basic data manipulations you can use in data.frames applies to data.table. Like ncol(), nrow(), names(), summary(). But it has more possibilities, for instance in data.table there is a special variable .N which is an integer that contains the row number in the group. If you use dt[.N] you\u0026rsquo;ll get the last row of your data.table.\nAnother cool feature of data.table is that if you want filter/subset a column you don\u0026rsquo;t need to use df$x[df$x == 1] you can simple use dt[x == 1] which make your code much more readable and clean.\nYou also get to use special operators: %like%, %in% and %between%. These operators work like SQL operators, LIKE, IN, and BETWEEN, respectively.\nIf you are familiar with SQL there is this one thing the package offers that will catch your eye. It\u0026rsquo;s called chaining, which allows you to perform a sequence of operations in a data.table, you just need to use dt[][] and chain multiple operations \u0026ldquo;[]\u0026rdquo;.\nBut that\u0026rsquo;s not all with the operator := you can alter data without making a new copy in the memory.\nIf you want to start using the package I suggest you use the cheatsheet. It\u0026rsquo;s really useful if you already have a basic knowledge about data.frames.\n  https://h2oai.github.io/db-benchmark/\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n https://cran.r-project.org/web/packages/data.table/vignettes/datatable-intro.html\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n   ","permalink":"https://devmedeiros.github.io/post/2021-10-27-data-table/","summary":"The data.table package is one of the fastest packages for data manipulation, currently, it is even faster than pandas and dplyr 1. data.table syntax is dt[i, j, by], where:\n i is used to subset rows j is used to subset columns by is used to subset groups, like GROUP BY from SQL  You can read it out loud as2:\n Take dt, subset/reorder rows using i, then calculate j, grouped by by.","title":"An Overview on data.table R Package"},{"content":"Tools used: R, ggplot, Shiny\nCategory: Simulation\n I got inspired to simulate a blackjack game, so I decided to make a series of functions to emulate the dealer\u0026rsquo;s behavior, a newbie player, a cautious player, and a strategist. With this set of functions, you can run a single game with p players, d decks, any combination of players archetypes. And you can also run it n times.\nI also made a shiny app to display how the simulation works. In the app, you are limited by the number of players, but if you wish to run the code with more players you can check the GitHub repository. There you can find the rules considered for the simulation and the complete code.\nIf you are familiar with the R language you can also run the app locally, you just need to run the library(shiny) and runGitHub(\u0026quot;blackjack-simulation\u0026quot;, \u0026quot;devmedeiros\u0026quot;, ref = \u0026quot;main\u0026quot;).\nThe app is composed of a sidebar with a space to choose the archetypes, the number of decks to use, and how many rounds you want to see simulated. Depending on how many rounds you choose it can get slow as it\u0026rsquo;ll simulate according to your choices every time you hit the RUN SIMULATION RUN.\nIn the plot tab, we have the evolution of the lose rate through the rounds.\nA 1 means the player lost that round and a 0 means the player won.\nThe game setup tab shows all the cards dealt in the simulation, each card was dealt from left to right and a blank cell means the player didn\u0026rsquo;t ask for another card (hit).\nAnd lastly, the lose rate tab shows us the same data from the plot tab, but now as a table. This is useful if you want to take a deeper look at how one strategy was better than the other.\n","permalink":"https://devmedeiros.github.io/post/2021-10-24-blackjack-simulation/","summary":"\u003cp\u003e\u003cstrong\u003eTools used:\u003c/strong\u003e R, ggplot, Shiny\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eCategory:\u003c/strong\u003e Simulation\u003c/p\u003e\n\u003chr\u003e","title":"Blackjack Simulation"},{"content":"Here are 5 Tips to improve your Power BI Dashboards.\n1- Color scheme Unless you are working with a particular company that has defined rules on branding and how their logos and pamphlets should be displayed, deciding which colors to use will be a big part of your dashboard designing.\nWhen starting, it may not be easy to choose a color scheme with confidence. You may find yourself always going for monochromatic with black and white. To end this and bring more life to your dashboards you can use the assistance of websites or apps that suggest a color scheme for you. The suggestions are based on color harmony rules and you can get something unique for each dashboard that you make. You can easily find a website to help you with that, just look up the color wheel, particularly I like to use Adobe Color. For the color harmony rule, I recommend using the Triad and/or Complementary. You can just choose a color to start and the site will update your suggestions instantly or you can just keep refreshing until you find something you like.\nOne of the reasons I like triad and complementary is because when I want to make something pop on a dashboard it becomes really easy to do so, without compromising the aesthetic of the whole thing.\n2- Bookmarks Take advantage of Power BI bookmarks and selection tools (show/hide). Combining these two tools you can make a more responsive dashboard, where you can have buttons that opens a small window with information or a small graph that didn’t fit the overall look of the dashboard. You can also, use it to make multiple pages on your dashboard, a ‘clean all filters’ button, or as a way to guide the visualization of your dashboard.\n3- Wallpaper and Background Setting up a wallpaper image can make your dashboard stand out. But you need to choose carefully what kind of image you use. You don’t want to use something that will have a lot of details, as it can draw attention away from your data.\nOne of my go-to wallpaper images is a gradient image and coolors make this easy for you. You can select the colors you want, how you want the gradient to be and it will generate an image for you that you can download and use it. If you prefer a minimalist approach you can just go for a solid color wallpaper.\nDon’t forget. You can set the wallpaper image as transparent as well, so if you think the color is too strong, try it out with some level of transparency.\nOn top of your wallpaper, you can have a transparent background, you can play around with the percentages to see what is a better fit for you.\n4- Shadows Shadows are an excellent tool to use. When used correctly can add just enough depth to make your dashboard less boring, but if you used it too much it can become a noisy experience. So as a rule of thumb, if you have a lot of visuals on a page, choose one type of visual (I prefer box values) to be different from the others, usually, you’d go for something that you want them to draw their attention to. Suppose you have a pretty flat dashboard, with not much going on, you can use an inside shadow on your graph and make it distinctive from everything else. But avoid using shadows on different types of visuals and different types of shadows.\n5- Make a Theme So, you know what colors you’re going to use, decided what fonts you like and how you like to present your charts and graphs. It’s time to make your life easier and just make a theme.\nThere is a Power BI theme generator by PowerBI.Tips, with this website you can set up your color scheme (palette) and set up all the visuals properties. When you finish you can just download it and use it as a default option. This will make your life easier because you won’t need to go over basic selections like selecting a font every time you create a visual. And even if you want a particular visual to be different you’ll still be able to change it however you want.\n","permalink":"https://devmedeiros.github.io/post/2021-10-15-beautiful-power-bi/","summary":"Here are 5 Tips to improve your Power BI Dashboards.\n1- Color scheme Unless you are working with a particular company that has defined rules on branding and how their logos and pamphlets should be displayed, deciding which colors to use will be a big part of your dashboard designing.\nWhen starting, it may not be easy to choose a color scheme with confidence. You may find yourself always going for monochromatic with black and white.","title":"Making Beautiful Power BI Dashboards"},{"content":"I want to tackle sentiment analysis using R in a simple way, just to get me started. With this in mind we begin loading all the packages we\u0026rsquo;ll be using.\nlibrary(readr) library(dplyr) library(tidytext) library(tokenizers) library(stopwords) library(ggplot2) Then we need to load our dataset. This data comes from Kaggle Fake and real news dataset.\nFake \u0026lt;- read_csv(\u0026#39;~/fakenews/Fake.csv\u0026#39;) True \u0026lt;- read.csv(\u0026#39;~/fakenews/True.csv\u0026#39;) I want to merge the two datasets, but first we need to create a new column that will tell me where the data came from.\nFake$news \u0026lt;- \u0026#39;fake\u0026#39; True$news \u0026lt;- \u0026#39;real\u0026#39; data \u0026lt;- rbind(Fake,True) Now we can start the data cleaning. In this first moment, we\u0026rsquo;ll do a simple tokenization on the title and text variables. Then we\u0026rsquo;ll be removing the stopwords according to the snowball source from the stopwords package.\ntitle \u0026lt;- tibble(news = data$news, text = data$title) corpus \u0026lt;- tibble(news = data$news, corpus = data$text) tidy_title \u0026lt;- title %\u0026gt;% unnest_tokens(word, text, token = \u0026#39;words\u0026#39;) %\u0026gt;% filter(!(word %in% stopwords(source = \u0026#39;snowball\u0026#39;))) tidy_corpus \u0026lt;- corpus %\u0026gt;% unnest_tokens(word, corpus, token = \u0026#39;words\u0026#39;) %\u0026gt;% filter(!(word %in% stopwords(source = \u0026#34;snowball\u0026#34;))) With the tidy data we can select the ten most frequent words from which title news' group.\np0 \u0026lt;- tidy_title %\u0026gt;% group_by(news, word) %\u0026gt;% summarise(n = n()) %\u0026gt;% arrange(desc(n)) %\u0026gt;% slice(1:10) Fake news titles mention video and trump by a large margin, 8477 and 7874 respectively. On the real news titles, trump is also one of the most mentioned, coming on first with 4883 appearances, followed by u.s, 4187, and says with 2981.\nNow we prepare the data to the sentiment analysis. I\u0026rsquo;m interested in classifing the data into sentiments of joy, anger, fear or surprise, for example. So I\u0026rsquo;ll be using the nrc dataset from Saif Mohammad and Peter Turney.\np1 \u0026lt;- tidy_title %\u0026gt;% inner_join(get_sentiments(\u0026#39;nrc\u0026#39;)) %\u0026gt;% group_by(sentiment, news) %\u0026gt;% summarise(n = n()) %\u0026gt;% mutate(prop = n/sum(n)) Disgust seems to be the most common sentiment around fake news titles while trust is the lowest, even though it still is more than 50%. Overall fake news titles seems to have more \u0026ldquo;sentiment\u0026rdquo; than real news in this particular dataset. Even positive sentiments like joy and surprise.\np2 \u0026lt;- tidy_corpus %\u0026gt;% inner_join(get_sentiments(\u0026#39;nrc\u0026#39;)) %\u0026gt;% group_by(sentiment, news) %\u0026gt;% summarise(n = n()) %\u0026gt;% mutate(prop = n/sum(n)) For the news' corpus we can see the same sentiments are prevalent, but the proportion is lower compared to the title. A fake news article loses trust when the reader takes more time to read it. It also becames less negative and shows less fear.\nAn improvement we could do here is to use our own stopwords and change the way we made the tokens. We had instances were trump and trump\u0026rsquo;s didn\u0026rsquo;t correspond to the same thing and if we had used this data to train a model this could become problematic.\n","permalink":"https://devmedeiros.github.io/post/2021-10-12-fakenews-sentiment/","summary":"I want to tackle sentiment analysis using R in a simple way, just to get me started. With this in mind we begin loading all the packages we\u0026rsquo;ll be using.\nlibrary(readr) library(dplyr) library(tidytext) library(tokenizers) library(stopwords) library(ggplot2) Then we need to load our dataset. This data comes from Kaggle Fake and real news dataset.\nFake \u0026lt;- read_csv(\u0026#39;~/fakenews/Fake.csv\u0026#39;) True \u0026lt;- read.csv(\u0026#39;~/fakenews/True.csv\u0026#39;) I want to merge the two datasets, but first we need to create a new column that will tell me where the data came from.","title":"Sentiment Analysis of Fake News vs Real News"},{"content":"Hi! My name is Jaqueline Souza Medeiros and I’m a 25 years old Statistician who graduated from the Federal University of Goiás in Brazil. The primary purpose of this blog is to write about what I have learned and continue to learn about Data Science.\nI love learning about other cultures and languages, and because of this, I’ve come to have a great interest in Natural Language Processing. Also, I am interested in simulation and machine learning, so I hope to explore these areas soon.\nIn my free time, I like to practice yoga, play D\u0026amp;D, and play with my cats.\nYou can find me at:\n Linkedin: medeiros-jaqueline Github: devmedeiros  ","permalink":"https://devmedeiros.github.io/about/","summary":"about","title":"About"}]