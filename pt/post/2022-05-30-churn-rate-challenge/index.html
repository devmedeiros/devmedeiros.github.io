<!doctype html><html lang=pt dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Data Science Challenge - Churn Rate | Dev_Medeiros</title><meta name=keywords content="storytelling,python,análise de dados,aprendizado de máquina,rede neural"><meta name=description content="Ferramentas utilizadas: Python, seaborn, scikit-learn, imbalanced-learn
Categoria: Análise de Dados, Aprendizado de Máquina
"><meta name=author content="Jaqueline Souza Medeiros"><link rel=canonical href=https://devmedeiros.com/pt/post/2022-05-30-churn-rate-challenge/><link crossorigin=anonymous href=/assets/css/stylesheet.min.6f60056d44d3f7eb69a4bc6c332b59960f3a995802bded244750232f33713c49.css integrity="sha256-b2AFbUTT9+tppLxsMytZlg86mVgCve0kR1AjLzNxPEk=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.min.b95bacdc39e37a332a9f883b1e78be4abc1fdca2bc1f2641f55e3cd3dabd4d61.js integrity="sha256-uVus3DnjejMqn4g7Hni+Srwf3KK8HyZB9V4809q9TWE=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://devmedeiros.com/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=16x16 href=https://devmedeiros.com/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=32x32 href=https://devmedeiros.com/%3Clink%20/%20abs%20url%3E><link rel=apple-touch-icon href=https://devmedeiros.com/%3Clink%20/%20abs%20url%3E><link rel=mask-icon href=https://devmedeiros.com/%3Clink%20/%20abs%20url%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><meta name=generator content="Hugo 0.104.2"><link rel=alternate hreflang=en href=https://devmedeiros.com/post/2022-05-30-churn-rate-challenge/><link rel=alternate hreflang=pt href=https://devmedeiros.com/pt/post/2022-05-30-churn-rate-challenge/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><script async src="https://www.googletagmanager.com/gtag/js?id=G-D4W6Y2T4WX"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-D4W6Y2T4WX",{anonymize_ip:!1})}</script><meta property="og:title" content="Data Science Challenge - Churn Rate"><meta property="og:description" content="Ferramentas utilizadas: Python, seaborn, scikit-learn, imbalanced-learn
Categoria: Análise de Dados, Aprendizado de Máquina
"><meta property="og:type" content="article"><meta property="og:url" content="https://devmedeiros.com/pt/post/2022-05-30-churn-rate-challenge/"><meta property="og:image" content="https://devmedeiros.com/cover.png"><meta property="article:section" content="post"><meta property="article:published_time" content="2022-05-30T16:49:00-03:00"><meta property="article:modified_time" content="2022-06-09T18:08:00-03:00"><meta property="og:site_name" content="Dev_Medeiros"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://devmedeiros.com/cover.png"><meta name=twitter:title content="Data Science Challenge - Churn Rate"><meta name=twitter:description content="Ferramentas utilizadas: Python, seaborn, scikit-learn, imbalanced-learn
Categoria: Análise de Dados, Aprendizado de Máquina
"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://devmedeiros.com/pt/post/"},{"@type":"ListItem","position":2,"name":"Data Science Challenge - Churn Rate","item":"https://devmedeiros.com/pt/post/2022-05-30-churn-rate-challenge/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Data Science Challenge - Churn Rate","name":"Data Science Challenge - Churn Rate","description":"Ferramentas utilizadas: Python, seaborn, scikit-learn, imbalanced-learn\nCategoria: Análise de Dados, Aprendizado de Máquina\n","keywords":["storytelling","python","análise de dados","aprendizado de máquina","rede neural"],"articleBody":"Ferramentas utilizadas: Python, seaborn, scikit-learn, imbalanced-learn\nCategoria: Análise de Dados, Aprendizado de Máquina\nEu fui desafiada a tomar o papel da nova cientista de dados na Alura Voz. Essa empresa fictícia é do ramo de telecomunicação e precisa reduzir sua taxa de evasão de clientes.\nEsse desafio é dividido em quatro semanas. Para a primeira semana o objetivo é tratar o banco de dados proveniente de uma API. Em seguida, precisamos identificar clientes que são mais propensos a deixar a empresa, usando exploração e análise de dados. E então, na terceira semana, nós usamos modelos de machine learning para prever a taxa de evasão da Alura Voz. A última semana é para expor o que fizemos durante o desafio e construir nosso portfolio. Caso esteja interessado em ver o código, ele está disponível no meu repositório do GitHub.\nPrimeira Semana Lendo o Banco de Dados O banco de dados foi disponibilizado no formato JSON e num primeiro momento aparenta ser um data frame normal.\nEntretanto, como pode ser observado, customer, phone, internet, e account são suas próprias tabelas. Então eu normalizei elas separadamente e depois simplesmente concatenei todas essas tabelas em uma.\nDados Faltantes A primeira vez que eu procurei por dados faltantes nessa base nenhum foi encontrado, mas a medida que eu explorei os dados eu percebi que havia espaços em branco e vazios não sendo contados como NaN. Então eu corrigi isso e descobri que havia 224 dados faltantes para a variável Churn e 11 para Charges.Total.\nEu decidi desconsiderar os dados faltantes da variável Churn, pois este será nosso objeto de estudo e não há sentido em estudar algo que não existe. No caso dos dados faltantes de Charges.Total, eu imagino que representa um cliente que não pagou nada ainda, pois todos eles possuem 0 meses de contrato, ou seja, eles acabaram de se tornar clientes, então eu simplesmente substitui o valor faltante por 0.\nCodificação de Variáveis A variável SeniorCitizen foi a única que veio com 0 e 1 ao invés de Yes e No. Por hora eu irei trocar esses valores por “yes” e “no”, pois isto torna a análise mais simples de ser lida.\nCharges.Monthly e Charges.Total foram renomeadas para perderem o ponto, pois isto atrapalha na hora de lidar com elas no python.\nSegunda Semana Análise de Dados No primeiro gráfico podemos ver o quão desbalanceado nosso banco de dados é. Há mais de 5000 clientes que não deixaram a empresa e um pouco menos de 2000 que deixaram.\nEu experimentei usar técnicas de sobreamostragem (oversampling) para lidar com esse deslanceamento, mas isto fez com que os modelos de aprendizado de máquina tivessem uma performance pior. E subamostragem (undersampling) não é uma opção com um banco de dados desse tamanho, então eu decidi deixar do jeito que está, e quando for hora de separar os dados de treino e teste eu irei estratificar o banco de acordo com a variável Churn.\nEu também gerei 16 gráficos para todas as variáveis discretas, para ver todos os gráficos olhe este notebook. O objetivo é ver se havia algum comportamento que fazia alguns cliente mais propensos a deixar a empresa. É claro que todas, exceto por gender, parecem ter algum papel em determinar se um cliente vai ou não deixar a empresa. Mais especificamente forma de pagamento, contratos, backup online, suporte técnico, e serviço de internet.\nNo gráfico de tenure, eu decidi fazer gráficos de distribuição dos meses de contrato do cliente, um gráfico para os cliente que não evadiram e um para os que evadiram. Podemos ver que clientes que evadiram o fizeram no início do seu tempo na empresa.\nA cobrança mensal média para os cliente que não evadiram é de 61,27 unidades monetária, enquanto que clientes que evadiram pagam 74,44. Isso provavelmente é por conta do tipo de contrato que esses tipo de clintes preferem, mas de qualquer forma é senso comum que preços altos afastam clientes.\nO Perfil de Evasão Considerando tudo que eu pude observar através de gráficos e medidas, eu fiz um perfil de clientes que são mais propensos a evadir a empresa.\nClientes novos são mais propensos a evadir do que clientes antigos.\nClientes com poucos serviços e produtos tendem a deixar a empresa. Se eles não estão presos a um contrato mais longo eles aparentam ser mais propensos a abandonar a empresa.\nSobre os meios de pagamentos, cliente que evadem possuem uma preferência forte por cheques eletrônicos e usualmente gastam 13,17 unidades monetárias a mais que a média de clientes que não deixaram a empresa.\nTerceira Semana Preparando o Banco de Dados Damos início fazendo variáveis dummies, de forma que teremos n-1 dummies para n variáveis. Então fazemos uma matriz de correlação para avaliar a correlação das nossas variáveis.\nPodemos ver que a variável InternetService_No possui correlações altas com diversas outras variáveis, isso se dá porque as outras variáveis depende do cliente ter ou não acesso a internet. Então irei tirar essas variáveis dependentes do modelo. A mesma coisa ocorre com PhoneService_Yes.\ntenure e ChargesTotal também possuem uma alta correlação, mas eu testei rodar os modelos sem uma das duas ou ambas e os modelos tiveram uma performance pior e levaram mais tempo para covergirem, então eu decidi manter elas no modelo, e elas são relevantes para o problema.\nApós retirar essas variáveis eu termino de preparar o banco de dados com uma normalização das variáveis numéricas, ChargesTotal e tenure.\nBanco de Dados de Teste e Treino Eu dividi o banco de dados em treino e teste, 20% para teste e o resto para treino. Eu estratifiquei os dados de acordo com a variável Churn e embaralhei os dados antes de separar. A mesma divisão de dados é usada em todos os modelos. Após separar os dados eu decidi fazer uma sobreamostragem (oversampling) dos dados de teste usando SMOTE1, pois os dados são muito desbalanceados. O motivo de eu usar essa técnica apenas nos dados de teste é que eu não quero ter um resultado viesado, se eu sobreamostrar todo o banco de dados isso quer dizer que eu vou testar meu modelo no mesmo dado que eu o treinei, e este não é meu objetivo.\nAvaliação dos Modelos Eu vou utilizar um classificador dummy para ter uma base para a medida de acurácia, e eu também vou utilizar as métricas: precision (precisão), recall (recordação) and f1 score (medida f1)2. Apesar de que o modelo dummy não ter valor para essas métricas eu vou manter ele para comparar a melhora dos modelos.\nModelo Base O modelo base foi feito através de um classificador dummy, basicamente ele diz que todos os clientes se comportam da mesma forma. Neste caso o modelo chutou que nenhum cliente iria deixar a empresa. Usando essa abordagem o modelo base obteve uma acurácia de 0,73456.\nA seguir todos os modelos terão a mesma semente aleatória (random state).\nModelo 1 - Florestas Aleatórias Eu inicio usando uma busca no grid com validação cruzada (grid search with cross-validation) para encontrar os melhores parâmetros dentro de uma seleção de opções utilizando o recall como estratégia para avaliar a performance. O melhor modelo encontrado pela busca foi:\nRandomForestClassifier(criterion='entropy', max_depth=5, max_leaf_nodes=70, random_state=22) Após ajustar o modelo, as medidas de avaliação foram:\nMedida Accuracy: 0,72534 Medida Precision: 0,48922 Medida Recall: 0,78877 Medida F1: 0,60389 Modelo 2 - Classificação de Vetores de Suporte Linear Neste modelo eu usei os parâmetros padrões e aumentei o teto para o máximo de iterações para 900000.\nLinearSVC(max_iter=900000, random_state=22) Após ajustar o modelo, as medidas de avaliação foram:\nMedida Accuracy: 0,71966 Medida Precision: 0,48217 Medida Recall: 0,75936 Medida F1: 0,58982 Modelo 3 - Rede Neural Multicamada Perceptron Aqui eu fixei o solucionador LBFGS, pois de acordo com a documentação do scikit-learn ele tem uma performance melhor em banco de dados pequenos 3, e também fiz uma busca no grid com validação cruzada para encontrar o melhor tamanho da camada oculta. O melhor modelo foi:\nMLPClassifier(hidden_layer_sizes=(1,), max_iter=9999, random_state=22, solver='lbfgs') Após ajustar o modelo, as medidas de avaliação foram:\nMedida Accuracy: 0,72818 Medida Precision: 0,49133 Medida Recall: 0,68182 Medida F1: 0,57111 Conclusão Após rodar os três modelos, todos usando o mesmo random_state. Eu encontrei as seguintes medidas de acurácia e melhorias no desempenho (comparado com o modelo base):\nNo fim, a Floresta Aleatória teve as melhores métricas. Este modelo consegue recordar uma grande parte dos clientes que evadem corretamente, ainda não é perfeito, mas já é um ponto de partida. A medida de acurácia não é tão alta como eu gostaria, mas para este problema em particular o objetivo é impedir os clientes de deixar a empresa e é melhor utilizar recursos para manter um cliente que não vai deixar a empresa do que não fazer nada.\nNo fim, eu gostei desse desafio, pois é raro eu praticar aprendizado de máquina, mas graças ao desafio eu tive a oportunidade de fazer um pequeno projeto nessa área que é tão importante e relevante. Essa foi a minha primeira vez trabalhando com redes neurais e ajuste de hiperparâmetros, e tenho certeza que na próxima vez terei resultados ainda melhores.\nimbalanced-learn documentation ↩︎\nAccuracy, Precision, Recall or F1? - Koo Ping Shung ↩︎\nscikit-learn documentation ↩︎\n","wordCount":"1522","inLanguage":"pt","datePublished":"2022-05-30T16:49:00-03:00","dateModified":"2022-06-09T18:08:00-03:00","author":{"@type":"Person","name":"Jaqueline Souza Medeiros"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://devmedeiros.com/pt/post/2022-05-30-churn-rate-challenge/"},"publisher":{"@type":"Organization","name":"Dev_Medeiros","logo":{"@type":"ImageObject","url":"https://devmedeiros.com/%3Clink%20/%20abs%20url%3E"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://devmedeiros.com/pt/ accesskey=h title="Dev_Medeiros (Alt + H)">Dev_Medeiros</a>
<span class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button><ul class=lang-switch><li>|</li><li><a href=https://devmedeiros.com/ title=En aria-label=En>En</a></li></ul></span></div><ul id=menu><li><a href=https://devmedeiros.com/pt/about/ title=sobre><span>sobre</span></a></li><li><a href=https://devmedeiros.com/pt/tags/ title=tags><span>tags</span></a></li><li><a href=https://devmedeiros.com/pt/archives/ title=arquivos><span>arquivos</span></a></li><li><a href=https://devmedeiros.com/pt/search/ title="pesquisar (Alt + /)" accesskey=/><span>pesquisar</span></a></li></ul></nav></header><main class=main><script src=https://code.jquery.com/jquery-3.4.1.slim.min.js integrity=sha384-J6qa4849blE2+poT4WnyKhv5vZF5SrPo0iEjwBvKU7imGFAV0wwj1yYfoRSJoZ+n crossorigin=anonymous></script>
<script src=https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js integrity=sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/js/bootstrap.min.js integrity=sha384-wfSDF2E50Y2D1uUdj0O3uMBJnjuUD4Ih7YwaYd1iqfktj0Uod8GCExl3Og8ifwB6 crossorigin=anonymous></script><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://devmedeiros.com/pt/>Home</a>&nbsp;»&nbsp;<a href=https://devmedeiros.com/pt/post/>Posts</a></div><h1 class=post-title>Data Science Challenge - Churn Rate</h1><div class=post-meta><span title='2022-05-30 16:49:00 -0300 -0300'>May 30, 2022</span>&nbsp;·&nbsp;<span title='2022-06-09 18:08:00 -0300 -0300'>Updated June 9, 2022</span>&nbsp;·&nbsp;8 minutos&nbsp;·&nbsp;Jaqueline Souza Medeiros&nbsp;|&nbsp;<ul class=i18n_list>Traduções:<li><a href=https://devmedeiros.com/post/2022-05-30-churn-rate-challenge/>En</a></li></ul>&nbsp;|&nbsp;<a href=https://github.com/devmedeiros.github.io/content/post/2022-05-30-churn-rate-challenge.pt.md rel="noopener noreferrer" target=_blank>Suggest Changes</a></div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Conteúdo</span></summary><div class=inner><ul><li><a href=#primeira-semana aria-label="Primeira Semana">Primeira Semana</a><ul><li><a href=#lendo-o-banco-de-dados aria-label="Lendo o Banco de Dados">Lendo o Banco de Dados</a></li><li><a href=#dados-faltantes aria-label="Dados Faltantes">Dados Faltantes</a></li><li><a href=#codifica%c3%a7%c3%a3o-de-vari%c3%a1veis aria-label="Codificação de Variáveis">Codificação de Variáveis</a></li></ul></li><li><a href=#segunda-semana aria-label="Segunda Semana">Segunda Semana</a><ul><li><a href=#an%c3%a1lise-de-dados aria-label="Análise de Dados">Análise de Dados</a></li><li><a href=#o-perfil-de-evas%c3%a3o aria-label="O Perfil de Evasão">O Perfil de Evasão</a></li></ul></li><li><a href=#terceira-semana aria-label="Terceira Semana">Terceira Semana</a><ul><li><a href=#preparando-o-banco-de-dados aria-label="Preparando o Banco de Dados">Preparando o Banco de Dados</a></li><li><a href=#banco-de-dados-de-teste-e-treino aria-label="Banco de Dados de Teste e Treino">Banco de Dados de Teste e Treino</a></li><li><a href=#avalia%c3%a7%c3%a3o-dos-modelos aria-label="Avaliação dos Modelos">Avaliação dos Modelos</a></li><li><a href=#modelo-base aria-label="Modelo Base">Modelo Base</a></li><li><a href=#modelo-1---florestas-aleat%c3%b3rias aria-label="Modelo 1 - Florestas Aleatórias">Modelo 1 - Florestas Aleatórias</a></li><li><a href=#modelo-2---classifica%c3%a7%c3%a3o-de-vetores-de-suporte-linear aria-label="Modelo 2 - Classificação de Vetores de Suporte Linear">Modelo 2 - Classificação de Vetores de Suporte Linear</a></li><li><a href=#modelo-3---rede-neural-multicamada-perceptron aria-label="Modelo 3 - Rede Neural Multicamada Perceptron">Modelo 3 - Rede Neural Multicamada Perceptron</a></li><li><a href=#conclus%c3%a3o aria-label=Conclusão>Conclusão</a></li></ul></li></ul></div></details></div><div class=post-content><p><strong>Ferramentas utilizadas:</strong> Python, seaborn, scikit-learn, imbalanced-learn</p><p><strong>Categoria:</strong> Análise de Dados, Aprendizado de Máquina</p><hr><p><img loading=lazy src=https://raw.githubusercontent.com/devmedeiros/Challenge-Data-Science/main/aluravoz.png#center alt="heart with an A inside and you can read &amp;lsquo;Alura Voz telecommunication company&amp;rsquo;"></p><p>Eu fui desafiada a tomar o papel da nova cientista de dados na Alura Voz. Essa empresa fictícia é do ramo de telecomunicação e precisa reduzir sua taxa de evasão de clientes.</p><p>Esse desafio é dividido em quatro semanas. Para a primeira semana o objetivo é tratar o banco de dados proveniente de uma API. Em seguida, precisamos identificar clientes que são mais propensos a deixar a empresa, usando exploração e análise de dados. E então, na terceira semana, nós usamos modelos de <em>machine learning</em> para prever a taxa de evasão da Alura Voz. A última semana é para expor o que fizemos durante o desafio e construir nosso portfolio. Caso esteja interessado em ver o código, ele está disponível no meu <a href=https://github.com/devmedeiros/Challenge-Data-Science>repositório</a> do GitHub.</p><h1 id=primeira-semana>Primeira Semana<a hidden class=anchor aria-hidden=true href=#primeira-semana>#</a></h1><h2 id=lendo-o-banco-de-dados>Lendo o Banco de Dados<a hidden class=anchor aria-hidden=true href=#lendo-o-banco-de-dados>#</a></h2><p>O banco de dados foi disponibilizado no formato JSON e num primeiro momento aparenta ser um <em>data frame</em> normal.</p><p><img loading=lazy src=https://raw.githubusercontent.com/devmedeiros/Challenge-Data-Science/main/1%20-%20Data%20Cleaning/table_head.png#center alt="table head with the first five rows"></p><p>Entretanto, como pode ser observado, <code>customer</code>, <code>phone</code>, <code>internet</code>, e <code>account</code> são suas próprias tabelas. Então eu normalizei elas separadamente e depois simplesmente concatenei todas essas tabelas em uma.</p><h2 id=dados-faltantes>Dados Faltantes<a hidden class=anchor aria-hidden=true href=#dados-faltantes>#</a></h2><p>A primeira vez que eu procurei por dados faltantes nessa base nenhum foi encontrado, mas a medida que eu explorei os dados eu percebi que havia espaços em branco e vazios não sendo contados como <code>NaN</code>. Então eu corrigi isso e descobri que havia 224 dados faltantes para a variável <code>Churn</code> e 11 para <code>Charges.Total</code>.</p><p>Eu decidi desconsiderar os dados faltantes da variável <code>Churn</code>, pois este será nosso objeto de estudo e não há sentido em estudar algo que não existe. No caso dos dados faltantes de <code>Charges.Total</code>, eu imagino que representa um cliente que não pagou nada ainda, pois todos eles possuem 0 meses de contrato, ou seja, eles acabaram de se tornar clientes, então eu simplesmente substitui o valor faltante por 0.</p><h2 id=codificação-de-variáveis>Codificação de Variáveis<a hidden class=anchor aria-hidden=true href=#codificação-de-variáveis>#</a></h2><p>A variável <code>SeniorCitizen</code> foi a única que veio com <code>0</code> e <code>1</code> ao invés de <code>Yes</code> e <code>No</code>. Por hora eu irei trocar esses valores por &ldquo;yes&rdquo; e &ldquo;no&rdquo;, pois isto torna a análise mais simples de ser lida.</p><p><code>Charges.Monthly</code> e <code>Charges.Total</code> foram renomeadas para perderem o ponto, pois isto atrapalha na hora de lidar com elas no <em>python</em>.</p><h1 id=segunda-semana>Segunda Semana<a hidden class=anchor aria-hidden=true href=#segunda-semana>#</a></h1><h2 id=análise-de-dados>Análise de Dados<a hidden class=anchor aria-hidden=true href=#análise-de-dados>#</a></h2><p>No primeiro gráfico podemos ver o quão desbalanceado nosso banco de dados é. Há mais de 5000 clientes que não deixaram a empresa e um pouco menos de 2000 que deixaram.</p><p><img loading=lazy src=https://raw.githubusercontent.com/devmedeiros/Challenge-Data-Science/main/2%20-%20Data%20Analysis/churn.jpg#center alt="bar plot with two bars, the first one is for &amp;rsquo;no&amp;rsquo; and the second is for &amp;lsquo;yes&amp;rsquo;, the first bar is over 5000 count and the second one is around 2000"></p><p>Eu experimentei usar técnicas de sobreamostragem (<em>oversampling</em>) para lidar com esse deslanceamento, mas isto fez com que os modelos de aprendizado de máquina tivessem uma performance pior. E subamostragem (<em>undersampling</em>) não é uma opção com um banco de dados desse tamanho, então eu decidi deixar do jeito que está, e quando for hora de separar os dados de treino e teste eu irei estratificar o banco de acordo com a variável <code>Churn</code>.</p><p>Eu também gerei 16 gráficos para todas as variáveis discretas, para ver todos os gráficos olhe este <a href=https://github.com/devmedeiros/Challenge-Data-Science/blob/main/2%20-%20Data%20Analysis/data_analysis.ipynb>notebook</a>. O objetivo é ver se havia algum comportamento que fazia alguns cliente mais propensos a deixar a empresa. É claro que todas, exceto por <code>gender</code>, parecem ter algum papel em determinar se um cliente vai ou não deixar a empresa. Mais especificamente forma de pagamento, contratos, <em>backup online</em>, suporte técnico, e serviço de internet.</p><p>No gráfico de <code>tenure</code>, eu decidi fazer gráficos de distribuição dos meses de contrato do cliente, um gráfico para os cliente que não evadiram e um para os que evadiram. Podemos ver que clientes que evadiram o fizeram no início do seu tempo na empresa.</p><p><img loading=lazy src=https://raw.githubusercontent.com/devmedeiros/Challenge-Data-Science/main/2%20-%20Data%20Analysis/tenure.jpg#center alt="there are two plots side-by-side, in the first one the title is &amp;lsquo;Churn = No&amp;rsquo; the data is along the tenure axis and is in a U shape. the second plot has the title &amp;lsquo;Churn = Yes&amp;rsquo; and starts high and drops fast along the tenure line"></p><p>A cobrança mensal média para os cliente que não evadiram é de 61,27 unidades monetária, enquanto que clientes que evadiram pagam 74,44. Isso provavelmente é por conta do tipo de contrato que esses tipo de clintes preferem, mas de qualquer forma é senso comum que preços altos afastam clientes.</p><h2 id=o-perfil-de-evasão>O Perfil de Evasão<a hidden class=anchor aria-hidden=true href=#o-perfil-de-evasão>#</a></h2><p><img loading=lazy src=https://64.media.tumblr.com/tumblr_lojvnhHFH91qlh1s6o1_400.gifv#center alt="person jumping through the window"></p><p>Considerando tudo que eu pude observar através de gráficos e medidas, eu fiz um perfil de clientes que são mais propensos a evadir a empresa.</p><ul><li><p>Clientes novos são mais propensos a evadir do que clientes antigos.</p></li><li><p>Clientes com poucos serviços e produtos tendem a deixar a empresa. Se eles não estão presos a um contrato mais longo eles aparentam ser mais propensos a abandonar a empresa.</p></li><li><p>Sobre os meios de pagamentos, cliente que evadem possuem uma preferência <strong>forte</strong> por cheques eletrônicos e usualmente gastam 13,17 unidades monetárias a mais que a média de clientes que não deixaram a empresa.</p></li></ul><h1 id=terceira-semana>Terceira Semana<a hidden class=anchor aria-hidden=true href=#terceira-semana>#</a></h1><h2 id=preparando-o-banco-de-dados>Preparando o Banco de Dados<a hidden class=anchor aria-hidden=true href=#preparando-o-banco-de-dados>#</a></h2><p>Damos início fazendo variáveis <em>dummies</em>, de forma que teremos n-1 <em>dummies</em> para n variáveis. Então fazemos uma matriz de correlação para avaliar a correlação das nossas variáveis.</p><p><img loading=lazy src=https://raw.githubusercontent.com/devmedeiros/Challenge-Data-Science/main/3%20-%20Model%20Selection/corr_matrix.jpg#center alt="correlation matrix with all the features"></p><p>Podemos ver que a variável <code>InternetService_No</code> possui correlações altas com diversas outras variáveis, isso se dá porque as outras variáveis depende do cliente ter ou não acesso a internet. Então irei tirar essas variáveis dependentes do modelo. A mesma coisa ocorre com <code>PhoneService_Yes</code>.</p><p><code>tenure</code> e <code>ChargesTotal</code> também possuem uma alta correlação, mas eu testei rodar os modelos sem uma das duas ou ambas e os modelos tiveram uma performance pior e levaram mais tempo para covergirem, então eu decidi manter elas no modelo, e elas são relevantes para o problema.</p><p>Após retirar essas variáveis eu termino de preparar o banco de dados com uma normalização das variáveis numéricas, <code>ChargesTotal</code> e <code>tenure</code>.</p><h2 id=banco-de-dados-de-teste-e-treino>Banco de Dados de Teste e Treino<a hidden class=anchor aria-hidden=true href=#banco-de-dados-de-teste-e-treino>#</a></h2><p>Eu dividi o banco de dados em treino e teste, 20% para teste e o resto para treino. Eu estratifiquei os dados de acordo com a variável <code>Churn</code> e embaralhei os dados antes de separar. A mesma divisão de dados é usada em todos os modelos. Após separar os dados eu decidi fazer uma sobreamostragem (<em>oversampling</em>) dos dados de <strong>teste</strong> usando SMOTE<sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup>, pois os dados são muito desbalanceados. O motivo de eu usar essa técnica apenas nos dados de teste é que eu não quero ter um resultado viesado, se eu sobreamostrar todo o banco de dados isso quer dizer que eu vou testar meu modelo no mesmo dado que eu o treinei, e este não é meu objetivo.</p><h2 id=avaliação-dos-modelos>Avaliação dos Modelos<a hidden class=anchor aria-hidden=true href=#avaliação-dos-modelos>#</a></h2><p>Eu vou utilizar um classificador <em>dummy</em> para ter uma base para a medida de acurácia, e eu também vou utilizar as métricas: <code>precision</code> (precisão), <code>recall</code> (recordação) and <code>f1 score</code> (medida f1)<sup id=fnref:2><a href=#fn:2 class=footnote-ref role=doc-noteref>2</a></sup>. Apesar de que o modelo <em>dummy</em> não ter valor para essas métricas eu vou manter ele para comparar a melhora dos modelos.</p><h2 id=modelo-base>Modelo Base<a hidden class=anchor aria-hidden=true href=#modelo-base>#</a></h2><p>O modelo base foi feito através de um classificador <em>dummy</em>, basicamente ele diz que todos os clientes se comportam da mesma forma. Neste caso o modelo chutou que nenhum cliente iria deixar a empresa. Usando essa abordagem o modelo base obteve uma acurácia de <code>0,73456</code>.</p><p>A seguir todos os modelos terão a mesma semente aleatória (<em>random state</em>).</p><h2 id=modelo-1---florestas-aleatórias>Modelo 1 - Florestas Aleatórias<a hidden class=anchor aria-hidden=true href=#modelo-1---florestas-aleatórias>#</a></h2><p>Eu inicio usando uma busca no grid com validação cruzada (<em>grid search with cross-validation</em>) para encontrar os melhores parâmetros dentro de uma seleção de opções utilizando o <code>recall</code> como estratégia para avaliar a performance. O melhor modelo encontrado pela busca foi:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>RandomForestClassifier(criterion<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;entropy&#39;</span>, max_depth<span style=color:#f92672>=</span><span style=color:#ae81ff>5</span>, max_leaf_nodes<span style=color:#f92672>=</span><span style=color:#ae81ff>70</span>, random_state<span style=color:#f92672>=</span><span style=color:#ae81ff>22</span>)
</span></span></code></pre></div><p>Após ajustar o modelo, as medidas de avaliação foram:</p><ul><li>Medida Accuracy: 0,72534</li><li>Medida Precision: 0,48922</li><li>Medida Recall: 0,78877</li><li>Medida F1: 0,60389</li></ul><h2 id=modelo-2---classificação-de-vetores-de-suporte-linear>Modelo 2 - Classificação de Vetores de Suporte Linear<a hidden class=anchor aria-hidden=true href=#modelo-2---classificação-de-vetores-de-suporte-linear>#</a></h2><p>Neste modelo eu usei os parâmetros padrões e aumentei o teto para o máximo de iterações para <code>900000</code>.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>LinearSVC(max_iter<span style=color:#f92672>=</span><span style=color:#ae81ff>900000</span>, random_state<span style=color:#f92672>=</span><span style=color:#ae81ff>22</span>)
</span></span></code></pre></div><p>Após ajustar o modelo, as medidas de avaliação foram:</p><ul><li>Medida Accuracy: 0,71966</li><li>Medida Precision: 0,48217</li><li>Medida Recall: 0,75936</li><li>Medida F1: 0,58982</li></ul><h2 id=modelo-3---rede-neural-multicamada-perceptron>Modelo 3 - Rede Neural Multicamada Perceptron<a hidden class=anchor aria-hidden=true href=#modelo-3---rede-neural-multicamada-perceptron>#</a></h2><p>Aqui eu fixei o solucionador LBFGS, pois de acordo com a documentação do <code>scikit-learn</code> ele tem uma performance melhor em banco de dados pequenos <sup id=fnref:3><a href=#fn:3 class=footnote-ref role=doc-noteref>3</a></sup>, e também fiz uma busca no grid com validação cruzada para encontrar o melhor tamanho da camada oculta. O melhor modelo foi:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>MLPClassifier(hidden_layer_sizes<span style=color:#f92672>=</span>(<span style=color:#ae81ff>1</span>,), max_iter<span style=color:#f92672>=</span><span style=color:#ae81ff>9999</span>, random_state<span style=color:#f92672>=</span><span style=color:#ae81ff>22</span>, solver<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;lbfgs&#39;</span>)
</span></span></code></pre></div><p>Após ajustar o modelo, as medidas de avaliação foram:</p><ul><li>Medida Accuracy: 0,72818</li><li>Medida Precision: 0,49133</li><li>Medida Recall: 0,68182</li><li>Medida F1: 0,57111</li></ul><h2 id=conclusão>Conclusão<a hidden class=anchor aria-hidden=true href=#conclusão>#</a></h2><p>Após rodar os três modelos, todos usando o mesmo <code>random_state</code>. Eu encontrei as seguintes medidas de acurácia e melhorias no desempenho (comparado com o modelo base):</p><p><img loading=lazy src=https://raw.githubusercontent.com/devmedeiros/Challenge-Data-Science/main/3%20-%20Model%20Selection/results_table.png#center alt="results table"></p><p>No fim, a Floresta Aleatória teve as melhores métricas. Este modelo consegue <em>recordar</em> uma grande parte dos clientes que evadem corretamente, ainda não é perfeito, mas já é um ponto de partida. A medida de <em>acurácia</em> não é tão alta como eu gostaria, mas para este problema em particular o objetivo é impedir os clientes de deixar a empresa e é melhor utilizar recursos para manter um cliente que não vai deixar a empresa do que não fazer nada.</p><p>No fim, eu gostei desse desafio, pois é raro eu praticar aprendizado de máquina, mas graças ao desafio eu tive a oportunidade de fazer um pequeno projeto nessa área que é tão importante e relevante. Essa foi a minha primeira vez trabalhando com redes neurais e ajuste de hiperparâmetros, e tenho certeza que na próxima vez terei resultados ainda melhores.</p><div class=footnotes role=doc-endnotes><hr><ol><li id=fn:1><p><a href=https://imbalanced-learn.org/stable/references/generated/imblearn.over_sampling.SMOTE.html>imbalanced-learn documentation</a>&#160;<a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:2><p><a href=https://towardsdatascience.com/accuracy-precision-recall-or-f1-331fb37c5cb9>Accuracy, Precision, Recall or F1? - Koo Ping Shung</a>&#160;<a href=#fnref:2 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:3><p><a href=https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html>scikit-learn documentation</a>&#160;<a href=#fnref:3 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li></ol></div></div><footer class=post-footer><ul class=post-tags><li><a href=https://devmedeiros.com/pt/tags/storytelling/>storytelling</a></li><li><a href=https://devmedeiros.com/pt/tags/python/>python</a></li><li><a href=https://devmedeiros.com/pt/tags/an%C3%A1lise-de-dados/>análise de dados</a></li><li><a href=https://devmedeiros.com/pt/tags/aprendizado-de-m%C3%A1quina/>aprendizado de máquina</a></li><li><a href=https://devmedeiros.com/pt/tags/rede-neural/>rede neural</a></li></ul><nav class=paginav><a class=prev href=https://devmedeiros.com/pt/post/alura-challenge-bi-2/><span class=title>« Página Anterior</span><br><span>Alura Challenge BI 2</span></a>
<a class=next href=https://devmedeiros.com/pt/post/2022-04-29-ux-power-bi-dashboards/><span class=title>Próxima Página »</span><br><span>UX/UI em Dashboards de Business Intelligence</span></a></nav><div class=share-buttons><a target=_blank rel="noopener noreferrer" aria-label="share Data Science Challenge - Churn Rate on twitter" href="https://twitter.com/intent/tweet/?text=Data%20Science%20Challenge%20-%20Churn%20Rate&url=https%3a%2f%2fdevmedeiros.com%2fpt%2fpost%2f2022-05-30-churn-rate-challenge%2f&hashtags=storytelling%2cpython%2can%c3%a1lisededados%2caprendizadodem%c3%a1quina%2credeneural"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM195.519 424.544c135.939.0 210.268-112.643 210.268-210.268.0-3.218.0-6.437-.153-9.502 14.406-10.421 26.973-23.448 36.935-38.314-13.18 5.824-27.433 9.809-42.452 11.648 15.326-9.196 26.973-23.602 32.49-40.92-14.252 8.429-30.038 14.56-46.896 17.931-13.487-14.406-32.644-23.295-53.946-23.295-40.767.0-73.87 33.104-73.87 73.87.0 5.824.613 11.494 1.992 16.858-61.456-3.065-115.862-32.49-152.337-77.241-6.284 10.881-9.962 23.601-9.962 37.088.0 25.594 13.027 48.276 32.95 61.456-12.107-.307-23.448-3.678-33.41-9.196v.92c0 35.862 25.441 65.594 59.311 72.49-6.13 1.686-12.72 2.606-19.464 2.606-4.751.0-9.348-.46-13.946-1.38 9.349 29.426 36.628 50.728 68.965 51.341-25.287 19.771-57.164 31.571-91.8 31.571-5.977.0-11.801-.306-17.625-1.073 32.337 21.15 71.264 33.41 112.95 33.41z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Data Science Challenge - Churn Rate on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&url=https%3a%2f%2fdevmedeiros.com%2fpt%2fpost%2f2022-05-30-churn-rate-challenge%2f&title=Data%20Science%20Challenge%20-%20Churn%20Rate&summary=Data%20Science%20Challenge%20-%20Churn%20Rate&source=https%3a%2f%2fdevmedeiros.com%2fpt%2fpost%2f2022-05-30-churn-rate-challenge%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Data Science Challenge - Churn Rate on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fdevmedeiros.com%2fpt%2fpost%2f2022-05-30-churn-rate-challenge%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Data Science Challenge - Churn Rate on whatsapp" href="https://api.whatsapp.com/send?text=Data%20Science%20Challenge%20-%20Churn%20Rate%20-%20https%3a%2f%2fdevmedeiros.com%2fpt%2fpost%2f2022-05-30-churn-rate-challenge%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Data Science Challenge - Churn Rate on telegram" href="https://telegram.me/share/url?text=Data%20Science%20Challenge%20-%20Churn%20Rate&url=https%3a%2f%2fdevmedeiros.com%2fpt%2fpost%2f2022-05-30-churn-rate-challenge%2f"><svg viewBox="2 2 28 28"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></div></footer></article></main><footer class=footer><span>&copy; 2021 - 2022 <a href=https://devmedeiros.com/terms/>Dev_Medeiros</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://git.io/hugopapermod rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerText="copiar";function s(){t.innerText="copiado!",setTimeout(()=>{t.innerText="copiar"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>