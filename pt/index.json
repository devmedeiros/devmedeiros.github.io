[{"content":" Nos últimos meses mais uma vez IA se tornou o assunto do momento. Depois de diversas pesquisas na área de arte usando inteligência artificial agora é a vez da popularização de textos escritos por IA.\nModelos de aprendizado de máquina já estavam auxiliando as pessoas há muito tempo, como corretores ortográficos, ferramentas que reescrevem um texto para retirar plágio e até ferramentas que fazem de tudo - desde que você saiba como pedir. Ferramentas como o ChatGPT não são novas, apenas explodiram em popularidade recentemente, você com certeza já ouviu falar de diversas formas de usar essas ferramentas de inteligência artificial, mas eu quero propor mais uma forma em que podemos aproveitar-las.\nPesquisar coisas na internet pode ser algo difícil, as vezes você precisa de muita informação sobre o que está procurando, o que dificulta a sua jornada. Nesses casos, usar ferramentas de IA pode ser proveitoso. Diferente de buscadores como o Google, ferramentas de inteligência artificial são capazes de entender uma pergunta e lhe responder de forma mais variada sem a necessidade de usar \u0026ldquo;códigos\u0026rdquo; ou filtros especiais, usando apenas linguagem natural.\nRecentemente eu estava procurando como encontrar círculos em imagens usando Python. Todos os resultados da primeira página do Google retornaram respostas usando o pacote OpenCV, que é um dos principais pacotes na área de Processamento Digital de Imagens (PDI), mas eu queria algo diferente.\nMesmo alterando os parâmetros das minhas buscas, os buscadores da internet continuaram insistindo em me sugerir páginas com o pacote opencv, mesmo quando eu explicitamente escrevia \u0026ldquo;sem usar opencv\u0026rdquo;, então eu decidi testar o ChatGPT. Primeiro eu tentei pedir que ele escrevesse uma função para encontrar círculos.\nEle escreveu uma função que funciona, bem semelhante ao que eu tinha escrito, mas ainda assim usava OpenCV. Então eu decidi ser mais específica e pedir outras formas de fazer isso, ao invés de pedir pelo código.\nNão sei se todos os métodos citados funcionam ou sequer existem, mas essas respostas me ajudaram a quebrar meu bloqueio de ideias e voltar a progredir no meu projeto. Agora eu posso estudar sobre o scikit-image, que eu não conhecia, e estou podendo testar com outras bibliotecas. Foi muito satisfatório poder encontrar outras formas de resolver um mesmo problema. Assim eu posso garantir que estou explorando vários caminhos diferentes e não fico presa apenas ao que já conheço e uso.\n","permalink":"https://devmedeiros.com/pt/post/using-ai-in-your-favor/","summary":"Uma nova forma de usar ferramentas de inteligência artificial, como ChatGPT, ao seu favor.","title":"Como usar IA ao seu favor?"},{"content":"Aprendendo a fazer melhores visualizações vem com estudo e prática, hoje quero falar sobre fontes que você pode usar para orientá-lo a fazer gráficos melhores e outras visualizações de dados em geral.\nFrom Data to Viz From Data to Viz traz muitas ferramentas para ajudar a navegar no mundo da visualização de dados. Na guia Explore você se depara com um infográfico interativo que ajuda a escolher um enredo, você pode escolher entre: numérico, categórico, numérico\u0026amp;categórico, mapas, rede e série temporal. Mas ele não mostra apenas gráficos que seriam bons para o seu tipo de dados, mas também uma descrição explicando o gráfico e os erros comuns que as pessoas cometem ao usar esse gráfico específico.\nNo final de cada ramo do infográfico, há também um link story, onde você pode ler sobre uma implementação desse tipo de plot usando dados reais.\nCaso você esteja apenas começando e não conhece muitos termos ou não tem certeza se seus dados contêm muitos ou poucos pontos quando você passa o mouse sobre os nós de opções, você poderá ver uma descrição com uma tabela de exemplo representando o que isso significa.\nAo ir para a guia Caveats você pode ver uma coleção de advertências e pode filtrá-las por:\nImprovement: todas as ressalvas que irão sugerir algum tipo de melhoria nas parcelas.\nMisleading: alguns gráficos podem ser enganosos ao omitir algumas informações, seja o tamanho da amostra ou a diferença proporcional entre duas categorias. Ao ler isso, você poderá identificar práticas ruins que levam à confusão ou manipulação.\nEste site também fornece links para galerias Python e R, onde você pode obter snippets de código para reproduzir esses gráficos.\nSe você preferir aprender observando o que não fazer, confira WTF Visualizations. Ele mostra visualizações ruins que não fazem sentido ou são de alguma forma enganosas.\nLá você encontrará vários gráficos de pizza que não somam 100%, gráficos de área onde os números não correspondem à área e apenas manipulação direta como no exemplo abaixo.\ngráfico de pizza de WTF Visualizations\n","permalink":"https://devmedeiros.com/pt/post/choose-the-best-graph/","summary":"Choosing the best graph or plot can be a hard task, but it doesn\u0026rsquo;t need to be.","title":"Fazendo Gráficos Melhores"},{"content":"Como é trabalhar como um Analytics Engineer? Analytics Engineer refere-se a um profissional de ciência de dados focado em transformar dados em informações de fácil acesso ao usuário final. Eles fornecem relatórios estatísticos e dinâmicos que capacitam a equipe de negócios sem que eles precisem pensar na complexidade por trás da análise de dados.\nNeste estudo de caso, quero falar sobre quais seriam as tarefas comuns que um Analytics Engineer precisaria executar e como eu as conduziria.\nNesse cenário, o Analytics Engineer trabalha para o Bankio, um banco digital do Brasil. Como a maioria dos bancos digitais no Brasil, o Bankio oferece transferências gratuitas para todas as contas bancárias do país. Também possui muitos produtos como conta de investimento, conta poupança, conta bancária individual, cartão de crédito sem anuidade e muito mais.\nTarefa 1: Consulta SQL Um Analista de Negócios (Bussiness Analyst) do Bankio pede sua ajuda para escrever uma consulta SQL para obter todo o saldo mensal da conta entre janeiro de 2020 e dezembro de 2020.\nSolução da consulta SQL (clique para expandir) SELECT a.*, -- Aqui eu calculo a soma cumulativa do total de depósitos de cada cliente ordenando -- por mês, se for nulo, altero o valor para 0, então subtraio a soma cumulativa do -- total de retiradas para cada cliente ordenando por mês, se for nulo, altero o -- valor para 0 e eu salvo isso como account_monthly_balance NVL(SUM(total_transfer_in) OVER (PARTITION BY customer_id ORDER BY action_month), 0) - NVL(SUM(total_transfer_out) OVER (PARTITION BY customer_id ORDER BY action_month), 0) AS account_monthly_balance FROM -- subconsulta total de transações de entrada/saída ( SELECT * FROM -- subconsulta de depósitos totais ( SELECT action_month, customer_id, SUM(amount) AS total_transfer_in FROM ( SELECT * FROM -- subconsulta de depósitos regulares ( SELECT d_month.action_month, accounts.customer_id, SUM(transfer_ins.amount) AS amount FROM d_time INNER JOIN transfer_ins ON transfer_ins.transaction_completed_at = d_time.time_id LEFT JOIN d_month USING(month_id) LEFT JOIN accounts USING(account_id) WHERE transfer_ins.status = \u0026#39;completed\u0026#39; GROUP BY d_month.action_month, accounts.customer_id ) transfer_in UNION ALL SELECT * FROM -- subconsulta de depósitos pix ( SELECT d_month.action_month, accounts.customer_id, SUM(pix_movements.pix_amount) AS amount FROM d_time INNER JOIN pix_movements ON pix_movements.pix_completed_at = d_time.time_id LEFT JOIN d_month USING(month_id) LEFT JOIN accounts USING(account_id) WHERE pix_movements.status = \u0026#39;completed\u0026#39; AND pix_movements.in_or_out = \u0026#39;pix_in\u0026#39; GROUP BY d_month.action_month, accounts.customer_id ) ) GROUP BY action_month, customer_id ) FULL JOIN ( SELECT action_month, customer_id, SUM(amount) AS total_transfer_out FROM -- subconsulta de saques totais ( SELECT * FROM -- subconsulta de retirada regular ( SELECT d_month.action_month, accounts.customer_id, SUM(transfer_outs.amount) AS amount FROM d_time INNER JOIN transfer_outs ON transfer_outs.transaction_completed_at = d_time.time_id LEFT JOIN d_month USING(month_id) LEFT JOIN accounts USING(account_id) WHERE transfer_outs.status = \u0026#39;completed\u0026#39; GROUP BY d_month.action_month, accounts.customer_id ) UNION ALL SELECT * FROM -- subconsulta de retirada de pix ( SELECT d_month.action_month, accounts.customer_id, SUM(pix_movements.pix_amount) AS amount FROM d_time INNER JOIN pix_movements ON pix_movements.pix_completed_at = d_time.time_id LEFT JOIN d_month USING(month_id) LEFT JOIN accounts USING(account_id) WHERE pix_movements.status = \u0026#39;completed\u0026#39; AND pix_movements.in_or_out = \u0026#39;pix_out\u0026#39; GROUP BY d_month.action_month, accounts.customer_id ) ) GROUP BY action_month, customer_id ) USING (action_month, customer_id) ) a; Tarefa 2: Indicadores Chaves de Performance (KPI) Foto de Stephen Dawson de Unsplash\nOutro colega do Bankio está interessado em analisar o sucesso do produto PIX da empresa em nível comercial e técnico. Então eles pediram que você apresentasse alguns indicadores-chave para medir isso.\nTempo médio de processamento das transações PIX (clique para expandir) Isso pode ser obtido usando o tempo que um cliente solicita uma transação PIX e quando ela é concluída, calculamos isso para todas as transações PIX e, em seguida, fazemos a média. O PIX deve ser instantâneo, portanto, essa métrica deve ser a menor possível. A proporção de falhas do PIX (clique para expandir) Este indicador é importante porque é inconveniente para o cliente que sua transação falhe. Podemos calcular isso dividindo a soma das transações PIX com falha pelo total de transações PIX. Esta medida deve ser minimizada. A proporção de transações usando PIX (clique para expandir) O sucesso do PIX pode ser medido pela proporção de movimentações usando PIX sobre transações normais. Então, basta contar quantas transações foram concluídas usando o PIX e dividir pelo valor total de transações concluídas. Um valor maior reflete o sucesso do PIX sobre transações regulares.\nAlternativamente, em vez de apenas contar as transações, podemos avaliar quanto dinheiro cada tipo de transação está movimentando.\nA proporção de entrada/saída do PIX (clique para expandir) Essa medida é boa para analisar se os clientes estão usando seu PIX mais para receber dinheiro ou para enviar dinheiro. Seria melhor se mais clientes estivessem recebendo mais dinheiro do que enviando. Como o Bankio já tinha transações gratuitas para qualquer banco, antes do PIX aparecer, outros ainda tinham que pagar taxas para enviar dinheiro para sua conta do Bankio. Por esse motivo, é melhor contar quantas transações estão chegando pelo PIX e dividi-las por todas as transações PIX. Quanto mais alto melhor.\nNesse caso, também podemos somar o saldo de depósitos e saques da conta do Bankio usando o PIX e compará-lo com transações regulares.\nTarefa 3: Retorno Diário do Investimento O Bankio tem uma conta bancária em que o cliente pode investir num produto de rendimento fixo. Considere que este produto oferece aos clientes um retorno diário de 0,01% de acordo com o valor do saldo diário investido. Calcule quanto cada cliente tem em sua conta bankio durante o ano de 2020.\nEste retorno é calculado diariamente após todos os saques e/ou depósitos feitos em um determinado dia. E todos os dias, mesmo finais de semana, geram algum retorno.\nO exemplo a seguir descreve o cliente A que começa a investir nesse produto de renda fixa no dia 16 do primeiro mês. O saldo anterior era zero, pois esse consumidor está fazendo um primeiro depósito no investimento. Seu depósito inicial foi de 1.000 e, no final do dia, produziu a uma taxa de renda diária de 0,01% de seu saldo. O mesmo produto continua sendo consumido por esse cliente em momentos diferentes ao longo do mês. Lembre-se que esta é apenas uma amostra fictícia do log de transações com cálculos diários aplicados. Observe que a receita desse dia deve ser zerada no caso de Movimentos negativos.\nDia Mês ID Conta Depósito Retirada Renda no final do Dia Rendimento Diário da Conta 16 1 A 1000 0 0,1 1000,10 20 1 A 500 0 0,15 1500,55 2 2 A 0 200 0,13 1302,48 19 2 A 1000 200 0,21 2104,78 Movimentos = Saldo do dia anterior + Depósito - Retirada\nRenda no final do dia = Movimentos * Taxa de Renda\nSaldo Diário da Conta = Movimentos + Renda no Fim do Dia\nGlossário Saldo Mensal da Conta É a quantidade de dinheiro que um cliente tinha em sua conta no final de um determinado mês.\nInformações da conta (agência, número da conta e dígito de verificação) No Brasil, uma conta bancária pode ser identificada exclusivamente por três números. O código agência, que indica em qual agência bancária as contas foram abertas, vem primeiro. O segundo é o número da conta que uma agência usa para identificar contas. O dígito de verificação, que é usado apenas para detecção de erros, é o último.\nCPF É a identificação cadastral do contribuinte individual brasileiro.\nPIX No Brasil, este é o método mais recente de transferência de dinheiro. Não é pago. É imediato, e tudo o que é necessário para concluir uma transação é a chave PIX associada à conta.\nTransferências não PIX Estes são os métodos convencionais para transferir dinheiro entre contas bancárias. Esse tipo de transação exige que sejam fornecidos o CPF, o código da agência, o número da conta e o dígito verificador da conta que receberá os recursos. A maioria dos bancos cobra uma taxa nessas transações, e a confirmação da transação normalmente leva várias horas a dias.\n","permalink":"https://devmedeiros.com/pt/post/case-study-analytics-engineer/","summary":"Projeto explorando tarefas comuns que um Analytics Engineer faz diariamente.","title":"Estudo de Caso Analytics Engineer"},{"content":"Visão Geral do Projeto Este projeto mostra um ciclo de vida de ciência de dados, onde eu limpo e preparo o banco de dados, feature engineering, aprendizado de máquina, deploy e visualização de dados.\nO conjunto de dados vem do kaggle, e possui muitas informações sobre o crédito e os dados bancários de uma pessoa, mas ele também tem muitos erros de digitação, dados ausentes e dados censurados. Este conjunto de dados precisava ser limpo e também precisava de feature engineering, eu precisava alterar algumas features, para que pudessem ser lidos pelo modelo. Assim, quando apresentada com dados categóricos, eu precisava identificar se era ordinal ou nominal, se fosse uma variável ordinal, seria mapeada para números sequenciais, caso contrário, eu faria uma dummy. Para as variáveis ​​_sim_ e não eu escolhi fazer apenas uma dummy, mas para os tipos de empréstimos fiz uma dummy para cada tipo de empréstimo e se alguém não tiver um empréstimo, simplesmente obtém 0 em todos as features de tipo de empréstimo. Eu falo sobre o processo de limpeza e feature engeeniring neste conjunto de dados aqui.\nEntão eu precisava de um modelo de ML que pudesse prever o score de crédito de uma pessoa com base em algumas features. Para decidir quais features eu iria usar eu analisei quais variáveis são usadas em empresas reais, e eu também escolhi variáveis que eu achei que faziam sentido. Eu acabei escolhendo as seguintes variáveis:\nIdade Renda anual Quantidade de contas bancárias Quantidade de cartões de crédito Quantidade de pagamentos atrasados Proporção de uso do cartão de crédito Quantidade de pagamento de empréstimos Idade do histórico de crédito em meses Empréstimos Perdeu algum pagamento nos últimos 12 meses Pagou o valor mínimo em pelo menos um cartão de crédito Com as features prontas, eu comecei a trabalhar no modelo, por enquanto eu decidi usar um modelo simples de Floresta Aleatória (Random Forest), eu pretendo melhorar esse modelo, mas nesse primeiro momento eu foquei eu fazer o app do streamlit.\nDepois que terminei o modelo eu serializei ele e o scaler usando o pacote pickle. Para fazer o deploy do modelo e construir a visualização, usei o streamlit.\nNeste aplicativo, você pode preencher um formulário ou apenas selecionar um dos três perfis padrão fornecidos para ver como o modelo avalia a pontuação de crédito de cada pessoa. Ele também apresenta \u0026ldquo;a certeza\u0026rdquo; do modelo exibindo um gráfico de pizza com a probabilidade (em porcentagem) de cada grupo de score de crédito em que as respostas se encaixam. Ele também mostra o quanto cada recurso conta para sua pontuação de crédito, de acordo com esse modelo. Você pode ver o aplicativo ao vivo aqui.\nTodo o código está disponível no meu repositório do GitHub. Além do código, lá você encontra a documentação, os dados originais e tratados (todas as etapas do tratamento), todos os requisitos para a construção deste projeto e como executá-lo localmente.\n","permalink":"https://devmedeiros.com/pt/post/credit-score-classification-app/","summary":"Usando o Streamlit para fazer um aplicativo da web que classifica sua pontuação de crédito usando Python","title":"Aplicativo de Classificação de Score de Crédito"},{"content":" Profissão foto criada por freepik - freepik.com\nAviso: Eu estarei falando sobre como chegar no código python, caso queira ler o código em si, dirija-se a este repositório.\nConheça a Base para Classificação de Score de Crédito A base de dados que vamos limpar vem do kaggle, a qual está no arquivo train.csv, mas os passos que iremos falar também podem ser usados para test.csv.\nHá 28 colunas e 100 mil linhas neste banco de dados. Eu compilei uma descrição das variáveis na tabela a seguir.\nVariável Descrição ID Representa uma identificação única de uma entrada Customer_ID Representa uma identificação única de uma pessoa Month Representa o mês do ano Name Representa o nome de uma pessoa Age Representa a idade de uma pessoa SSN Representa o número social de uma pessoa americana Occupation Representa a profissão de uma pessoa Annual_Income Representa o salário anual de uma pessoa Monthly_Inhand_Salary Representa o salário mensal base de uma pessoa Num_Bank_Accounts Representa quantas contas bancárias uma pessoa possui Num_Credit_Card Representa o número de cartões de crédito que uma pessoa possui Interest_Rate Representa a taxa de juros de um cartão de crédito Num_of_Loan Representa quantos empréstimos uma pessoa fez com o banco Type_of_Loan Representa os tipos de empréstimos feitos por uma pessoa Delay_from_due_date Representa o numero médio de dias atrasados da data de pagamento Num_of_Delayed_Payment Representa o numério médio de pagamentos atrasados feitos por uma pessoa Changed_Credit_Limit Representa, em porcentagem, a mudança de limite do cartão de crédito Num_Credit_Inquiries Representa o número de consultas de cartão de crédito Credit_Mix Representa a classificação da mistura de créditos Outstanding_Debt Representa a quantidade de saldo devedor a ser pago em dólares americanos Credit_Utilization_Ratio Representa a proporção de utilização de cartões de crédito Credit_History_Age Representa a idade do histórico de uso de crédito de uma pessoa Payment_of_Min_Amount Representa se uma pessoa pagou apenas o mínimo Total_EMI_per_month Representa os pagamentos das parcelas mensais de empréstimos em dólares americanos Amount_invested_monthly Representa a quantidade de dinheiro investido pelo cliente em dólares americanos Payment_Behaviour Representa o comportamento de pagamento do cliente em dólares americanos Monthly_Balance Representa o extrato mensal do cliente em dólares americanos Credit_Score Representa as faixas de score de crédito (Baixo, Médio, Bom) Mesmo tendo 100 mil linhas, dentro dessas linhas existem apenas 12.500 clientes diferentes, cada cliente aparece 8 vezes (de Janeiro a Agosto). Então basicamente nós podemos selecionar um cliente específico e olhar as informações dele e facilmente identificar dados incorretos e somos capazes de ajustá-los.\nLidando com Erros de Digitação e Outliers Neste banco de dados há diversos erros de digitação ou simplesmente coisas que não fazem sentido. Você irá encontrar valores que são: _, !@9#%8, __10000__, NM ou _______. Eu acredito que esses erros estão na base para representar o caos que você pode encontrar quando estiver lidando com dados reais e a maioria deles significam que esse valor é nulo.\nPor um momento eu pensei que __10000__ poderia ser apenas um erro de digitação, mas não há nenhuma quantidade de dinheiro investida mensalmente que é superior a 200 dólares americanos.\n__10000__ 4305 0.0 169 80.41529543900253 1 36.66235139442514 1 89.7384893604547 1 ... 36.541908593249026 1 93.45116318631192 1 140.80972223052834 1 38.73937670100975 1 167.1638651610451 1 Name: Amount_invested_monthly, Length: 91049, dtype: int64 Seguindo essa lógica, eu procurei por mais coisas que não faziam sentido no data frame e comecei a substituir esses valores pelos nans do numpy. Eu também procurei por outliers olhando a distribuição dos valores, se havia um valor que aparecia apenas uma vez e ele estava isolado, eu o substituia por um valor nulo. Eu não baseiei essa decisão apenas nisso, eu também procurei por clientes que tinham esse outlier e observei todos os dados desse cliente específico, e eu sempre encontrava coisas estranhas como:\nOlhando para este cliente em específico é fácil ver que ele não ganhou todo esse dinheiro anualmente apenas um mês do ano.\nAo terminar essa busca por erros de digitação e outliers, não esqueça de passar o novo tipo de dados para suas variáveis. Algumas variáveis como age começaram com caracteres string entre as idades e por isso ao ser feito a leitura dos dados o pandas os reconheceu como objeto e não como tipo int ou float.\nPreenchendo Valores em Branco Depois de lidar com todos os outliers e erros de digitação, nós terminamos com diversos valores nulos, como pode ser visto a baixo:\ndf.isna().sum()[df.isna().sum() \u0026gt; 0] Age 2776 Occupation 7062 Annual_Income 993 Monthly_Inhand_Salary 15002 Num_Credit_Card 2271 Interest_Rate 2034 Num_of_Loan 4348 Type_of_Loan 11408 Num_of_Delayed_Payment 7002 Changed_Credit_Limit 2091 Num_Credit_Inquiries 1965 Credit_Mix 20195 Credit_History_Age 9030 Payment_of_Min_Amount 12007 Amount_invested_monthly 8784 Payment_Behaviour 7600 Monthly_Balance 1200 dtype: int64 Ao invés de apagar todos os valores em branco eu primeiro tento preencher eles usando informação que já temos disponível. Lembra que eu mencionei que um cliente possui dados históricos de 8 meses atrás? Nós podemos apenas usar esses dados históricos para preencher os valores nulos usando uma medida resumo da nossa escolha filtrando para o cliente, isso será mais preciso do que apenas calcular a média do nosso banco de dados.\nEu decidir usar o valor médio para as seguintes colunas:\nmean_columns = [ \u0026#39;Num_of_Delayed_Payment\u0026#39;, \u0026#39;Changed_Credit_Limit\u0026#39;, \u0026#39;Num_Credit_Inquiries\u0026#39;, \u0026#39;Amount_invested_monthly\u0026#39;, \u0026#39;Monthly_Balance\u0026#39;, \u0026#39;Num_of_Loan\u0026#39;, \u0026#39;Num_Credit_Card\u0026#39;, \u0026#39;Interest_Rate\u0026#39;, \u0026#39;Annual_Income\u0026#39;, \u0026#39;Monthly_Inhand_Salary\u0026#39; ] E o último valor não nulo para estas:\nlast_columns = [\u0026#39;Age\u0026#39;, \u0026#39;Occupation\u0026#39;, \u0026#39;Type_of_Loan\u0026#39;, \u0026#39;Credit_Mix\u0026#39;] O motivo de não usar a média para todos os meus valores é porque eu não queria ter que lidar com uma pessoa tendo 20,5 anos de idade e Occupation, Type_of_Loan, e Credit_Mix são dados discretos.\nEngenharia das Features Com os dados limpos, nós podemos seguir com a engenharia das features. Vamos começar com a variável Type_of_Loan, que possui algumas ocorrências em que em uma única célula possui diversos tipos de empréstimos, como você pode ver:\nNot Specified, Mortgage Loan, Auto Loan, and Payday Loan 8 Payday Loan, Mortgage Loan, Debt Consolidation Loan, and Student Loan 8 Debt Consolidation Loan, Auto Loan, Personal Loan, Debt Consolidation Loan, Student Loan, and Credit-Builder Loan 8 Student Loan, Auto Loan, Student Loan, Credit-Builder Loan, Home Equity Loan, Debt Consolidation Loan, and Debt Consolidation Loan 8 Personal Loan, Auto Loan, Mortgage Loan, Student Loan, and Student Loan 8 Name: Type_of_Loan, Length: 5380, dtype: int64 Então eu vou salvar todos os tipos de empréstimos diferentes em um vetor, separando todos os empréstimos sempre que surgir uma , ou , and.\nloan_types = [] for index in df.index: temp = df.Type_of_Loan[index].replace(\u0026#39;and \u0026#39;, \u0026#39;\u0026#39;).split(\u0026#39;, \u0026#39;) for i in temp: #loan in temp array if i not in loan_types: #if loan is not in loan_types loan_types.append(i) #add it Agora podemos criar variáveis dummies usando loan_types, assim um cliente recebe o número 1 se ele tiver esse empréstimo ou 0 caso não tenha.\nfor loan in loan_types: df[loan] = 0 #create the loan column in the df with 0 for index in df.index: temp = df.Type_of_Loan[index].replace(\u0026#39;and \u0026#39;, \u0026#39;\u0026#39;).split(\u0026#39;, \u0026#39;) if loan in temp: df.loc[index, loan] = 1 Agora eu continuo trabalhando nesse banco para o tornar pronto para o treinamento de um modelo de aprendizado de máquina. Para isso, eu preciso transformar todos os meus dados discretos em numéricos.\nA variável Credit_History_Age tem os valores como strings \u0026ldquo;22 Years and 5 Months\u0026rdquo; e esse padrão se repete, então podemos aproveitar isso e selecionar o ano multiplicado por 12 e somar o mês, resultando em um novo recurso com o crédito idade da história em meses. Quando acabarmos com isso ainda haverá valores nulos e, para preenchê-los, escolho interpolar os valores. Isso funciona muito bem quando o valor ausente é de fevereiro até julho porque interpola com a idade do histórico de crédito do cliente, mas torna-se uma suposição incorreta quando o valor ausente é em janeiro ou agosto.\nOs nomes dos meses serão substituidos pelo seu representante numérico, logo janeiro será 1, fevereiro será 2, e assim por diante. credit_mix e credit_score possuem 3 categorias sequenciais, eu escolhi usar -1, 0, e 1, mas você também pode usar 1, 2, 3 e irá produzir o mesmo resultado.\nNão se esqueça de checar o repositório no GitHub caso queira ver o código completo mencionado aqui e para baixar a base tratada.\n","permalink":"https://devmedeiros.com/pt/post/data-cleaning-credit-score/","summary":"Como surgir com formas de limpar um banco de dados usando Python","title":"Limpando Base de Dados para Classificação de Score de Crédito"},{"content":"O Alura Challenge BI 2 foi um evento que durou quatro semanas, no qual nós fomos desafiados a fazer um painél de Business Intelligence com base nos requisitos de três empresas fictícias. Durante o evento eu fiz esses três painés usando o Power BI, você pode ver também os arquivos fontes dos painéis no meu repositório.\nAlura Films Alura Food Alura Skimó O objetivo deste painel é ajudar a encontrar a melhor seleção para um filme que será produzido.\nCom isso em mente, eu explorei na primeira aba um pequeno resumo sobre os filmes no banco de dados, com ela você pode ter uma ideia geral dos nossos dados. Na segunda aba é apresentado a nota no IMDB e o Meta Score dos filmes. Eu também criei uma medida que mostra o quanto essas duas avaliações concordam entre si. Uma medida de 100 quer dizer que não concordam nem um pouco e 0 quer dizer concordam completamente, a medida de concordância foi de 9,48.\nNossa terceira aba apresenta informação sobre as estrelas dos filmes, os 10 atores com a maior rentabilidade, e os 10 atores com a maior contagem de filmes.\nA quarta aba apresenta a distribuição da renda bruta pela quantidade de gêneros diferentes que um filme tem. Ela também mostra quantos filmes tem um certo gênero, como por exemplo, Drama é a escolha mais popular para gênero de filme, com 72% dos filmes do banco de dados. Por fim, essa aba também mostra a renda bruta média por gênero.\nA quinta e última aba mostra um pouco de informação sobre a classificação indicativa brasileira e mostra o Meta Score médio para filmes com certificação. Ela também mostra a renda bruta, em média, para cada classificação indicativa.\nA Alura Food está interessada em expandir seus negócios para o mercado indiano. Para isso, a empresa pediu para calcular medidas que os ajudem a tomarem uma melhor decisão.\nPrimeiramente, eu fiz uma junção dos bancos de dados e limpei eles através do Power BI, traduzi alguns dos textos de inglês para português através da Planilhas Google, e converti o preço das refeições da sua moeda original para o real (moeda brasileira). Por fim, eu usei o Figma para fazer o background, incluindo imagens e títulos.\nNeste projeto, eu optei por usar apenas uma página para mostrar toda a informação pedida, pois eu acredito que isso facilita a análise dos dados.\nA maioria dos restaurantes neste mercado indiano não oferecem entrega online, com apenas 19,21% deles tendo este serviço. A avaliação média dos restaurantes é de 3,72, de um total de 5, essa avaliação também é apresentada em formato textual, com um Muito Bom sendo uma nota acima de 4.\nO preço médio de uma refeição por pessoa é de R$ 39,48 (em torno de USD 7,65). E há 9577 restaurantes diferentes em todo o banco de dados, dos quais 3968, especializam na culinária do norte indiano. Nova Delhi é, de longe, o local mais popular para se abrir um restaurante, com 5473 deles, a segunda cidade é Gurgaon, com 1118 restaurantes.\nA Alura Skimo está interessada em analisar seus dados de vendas, para ajudar com isso eu fiz este dashboard.\nEle é composto de três páginas. A primeira apresenta um resumo com todas as principais medidas que encontramos no painel, filtrado para mostrar o mês mais recente. Nesta primeira página, você pode parar o cursor do mouse sobre uma medida e isso fará com que apareça um pequeno gráfico com a série histórica com uma linha de tendência.\nSeguindo para a próxima página, nela você pode ver toda a informação sobre os produtos vendidos pela Alura Skimo. Você pode filtrar os dados por sabor de sorvete, tipo de embalagem, categoria, e custo do produto. Ela mostra a informação básica sobre os produtos, junto de um ranque dos produtos que são os mais bem vendidos e dois gráficos que mostram as vendas por sabor e vendas por categoria.\nNa última página, você pode encontrar informações sobre os vendedores da nossa empresa. Ele mostra quando o funcionário entrou para a empresa, a comissão deles, o faturamento e quantas vendas cada um fez, tudo isso no último ano (2018).\nEsse painel foi complexo comparado aos outros dois, pois nosso banco de dados veio de arquivos SQL. Primeiro eu tive que criar o banco de dados e carregar cada arquivo SQL para ele, então eu só tive que carregar o banco para o Power BI. Por fim, toda a limpeza, tratamento e processamento de dados foi feito no próprio Power BI.\n","permalink":"https://devmedeiros.com/pt/post/alura-challenge-bi-2/","summary":"Alura ofereceu um desafio de quatro semanas Challenge BI, onde os participantes precisavam fazer 3 dashboards","title":"Alura Challenge BI 2"},{"content":"Ferramentas utilizadas: Python, seaborn, scikit-learn, imbalanced-learn\nCategoria: Análise de Dados, Aprendizado de Máquina\nEu fui desafiada a tomar o papel da nova cientista de dados na Alura Voz. Essa empresa fictícia é do ramo de telecomunicação e precisa reduzir sua taxa de evasão de clientes.\nEsse desafio é dividido em quatro semanas. Para a primeira semana o objetivo é tratar o banco de dados proveniente de uma API. Em seguida, precisamos identificar clientes que são mais propensos a deixar a empresa, usando exploração e análise de dados. E então, na terceira semana, nós usamos modelos de machine learning para prever a taxa de evasão da Alura Voz. A última semana é para expor o que fizemos durante o desafio e construir nosso portfolio. Caso esteja interessado em ver o código, ele está disponível no meu repositório do GitHub.\nPrimeira Semana Lendo o Banco de Dados O banco de dados foi disponibilizado no formato JSON e num primeiro momento aparenta ser um data frame normal.\nEntretanto, como pode ser observado, customer, phone, internet, e account são suas próprias tabelas. Então eu normalizei elas separadamente e depois simplesmente concatenei todas essas tabelas em uma.\nDados Faltantes A primeira vez que eu procurei por dados faltantes nessa base nenhum foi encontrado, mas a medida que eu explorei os dados eu percebi que havia espaços em branco e vazios não sendo contados como NaN. Então eu corrigi isso e descobri que havia 224 dados faltantes para a variável Churn e 11 para Charges.Total.\nEu decidi desconsiderar os dados faltantes da variável Churn, pois este será nosso objeto de estudo e não há sentido em estudar algo que não existe. No caso dos dados faltantes de Charges.Total, eu imagino que representa um cliente que não pagou nada ainda, pois todos eles possuem 0 meses de contrato, ou seja, eles acabaram de se tornar clientes, então eu simplesmente substitui o valor faltante por 0.\nCodificação de Variáveis A variável SeniorCitizen foi a única que veio com 0 e 1 ao invés de Yes e No. Por hora eu irei trocar esses valores por \u0026ldquo;yes\u0026rdquo; e \u0026ldquo;no\u0026rdquo;, pois isto torna a análise mais simples de ser lida.\nCharges.Monthly e Charges.Total foram renomeadas para perderem o ponto, pois isto atrapalha na hora de lidar com elas no python.\nSegunda Semana Análise de Dados No primeiro gráfico podemos ver o quão desbalanceado nosso banco de dados é. Há mais de 5000 clientes que não deixaram a empresa e um pouco menos de 2000 que deixaram.\nEu experimentei usar técnicas de sobreamostragem (oversampling) para lidar com esse deslanceamento, mas isto fez com que os modelos de aprendizado de máquina tivessem uma performance pior. E subamostragem (undersampling) não é uma opção com um banco de dados desse tamanho, então eu decidi deixar do jeito que está, e quando for hora de separar os dados de treino e teste eu irei estratificar o banco de acordo com a variável Churn.\nEu também gerei 16 gráficos para todas as variáveis discretas, para ver todos os gráficos olhe este notebook. O objetivo é ver se havia algum comportamento que fazia alguns cliente mais propensos a deixar a empresa. É claro que todas, exceto por gender, parecem ter algum papel em determinar se um cliente vai ou não deixar a empresa. Mais especificamente forma de pagamento, contratos, backup online, suporte técnico, e serviço de internet.\nNo gráfico de tenure, eu decidi fazer gráficos de distribuição dos meses de contrato do cliente, um gráfico para os cliente que não evadiram e um para os que evadiram. Podemos ver que clientes que evadiram o fizeram no início do seu tempo na empresa.\nA cobrança mensal média para os cliente que não evadiram é de 61,27 unidades monetária, enquanto que clientes que evadiram pagam 74,44. Isso provavelmente é por conta do tipo de contrato que esses tipo de clintes preferem, mas de qualquer forma é senso comum que preços altos afastam clientes.\nO Perfil de Evasão Considerando tudo que eu pude observar através de gráficos e medidas, eu fiz um perfil de clientes que são mais propensos a evadir a empresa.\nClientes novos são mais propensos a evadir do que clientes antigos.\nClientes com poucos serviços e produtos tendem a deixar a empresa. Se eles não estão presos a um contrato mais longo eles aparentam ser mais propensos a abandonar a empresa.\nSobre os meios de pagamentos, cliente que evadem possuem uma preferência forte por cheques eletrônicos e usualmente gastam 13,17 unidades monetárias a mais que a média de clientes que não deixaram a empresa.\nTerceira Semana Preparando o Banco de Dados Damos início fazendo variáveis dummies, de forma que teremos n-1 dummies para n variáveis. Então fazemos uma matriz de correlação para avaliar a correlação das nossas variáveis.\nPodemos ver que a variável InternetService_No possui correlações altas com diversas outras variáveis, isso se dá porque as outras variáveis depende do cliente ter ou não acesso a internet. Então irei tirar essas variáveis dependentes do modelo. A mesma coisa ocorre com PhoneService_Yes.\ntenure e ChargesTotal também possuem uma alta correlação, mas eu testei rodar os modelos sem uma das duas ou ambas e os modelos tiveram uma performance pior e levaram mais tempo para covergirem, então eu decidi manter elas no modelo, e elas são relevantes para o problema.\nApós retirar essas variáveis eu termino de preparar o banco de dados com uma normalização das variáveis numéricas, ChargesTotal e tenure.\nBanco de Dados de Teste e Treino Eu dividi o banco de dados em treino e teste, 20% para teste e o resto para treino. Eu estratifiquei os dados de acordo com a variável Churn e embaralhei os dados antes de separar. A mesma divisão de dados é usada em todos os modelos. Após separar os dados eu decidi fazer uma sobreamostragem (oversampling) dos dados de teste usando SMOTE1, pois os dados são muito desbalanceados. O motivo de eu usar essa técnica apenas nos dados de teste é que eu não quero ter um resultado viesado, se eu sobreamostrar todo o banco de dados isso quer dizer que eu vou testar meu modelo no mesmo dado que eu o treinei, e este não é meu objetivo.\nAvaliação dos Modelos Eu vou utilizar um classificador dummy para ter uma base para a medida de acurácia, e eu também vou utilizar as métricas: precision (precisão), recall (recordação) and f1 score (medida f1)2. Apesar de que o modelo dummy não ter valor para essas métricas eu vou manter ele para comparar a melhora dos modelos.\nModelo Base O modelo base foi feito através de um classificador dummy, basicamente ele diz que todos os clientes se comportam da mesma forma. Neste caso o modelo chutou que nenhum cliente iria deixar a empresa. Usando essa abordagem o modelo base obteve uma acurácia de 0,73456.\nA seguir todos os modelos terão a mesma semente aleatória (random state).\nModelo 1 - Florestas Aleatórias Eu inicio usando uma busca no grid com validação cruzada (grid search with cross-validation) para encontrar os melhores parâmetros dentro de uma seleção de opções utilizando o recall como estratégia para avaliar a performance. O melhor modelo encontrado pela busca foi:\nRandomForestClassifier(criterion=\u0026#39;entropy\u0026#39;, max_depth=5, max_leaf_nodes=70, random_state=22) Após ajustar o modelo, as medidas de avaliação foram:\nMedida Accuracy: 0,72534 Medida Precision: 0,48922 Medida Recall: 0,78877 Medida F1: 0,60389 Modelo 2 - Classificação de Vetores de Suporte Linear Neste modelo eu usei os parâmetros padrões e aumentei o teto para o máximo de iterações para 900000.\nLinearSVC(max_iter=900000, random_state=22) Após ajustar o modelo, as medidas de avaliação foram:\nMedida Accuracy: 0,71966 Medida Precision: 0,48217 Medida Recall: 0,75936 Medida F1: 0,58982 Modelo 3 - Rede Neural Multicamada Perceptron Aqui eu fixei o solucionador LBFGS, pois de acordo com a documentação do scikit-learn ele tem uma performance melhor em banco de dados pequenos 3, e também fiz uma busca no grid com validação cruzada para encontrar o melhor tamanho da camada oculta. O melhor modelo foi:\nMLPClassifier(hidden_layer_sizes=(1,), max_iter=9999, random_state=22, solver=\u0026#39;lbfgs\u0026#39;) Após ajustar o modelo, as medidas de avaliação foram:\nMedida Accuracy: 0,72818 Medida Precision: 0,49133 Medida Recall: 0,68182 Medida F1: 0,57111 Conclusão Após rodar os três modelos, todos usando o mesmo random_state. Eu encontrei as seguintes medidas de acurácia e melhorias no desempenho (comparado com o modelo base):\nNo fim, a Floresta Aleatória teve as melhores métricas. Este modelo consegue recordar uma grande parte dos clientes que evadem corretamente, ainda não é perfeito, mas já é um ponto de partida. A medida de acurácia não é tão alta como eu gostaria, mas para este problema em particular o objetivo é impedir os clientes de deixar a empresa e é melhor utilizar recursos para manter um cliente que não vai deixar a empresa do que não fazer nada.\nNo fim, eu gostei desse desafio, pois é raro eu praticar aprendizado de máquina, mas graças ao desafio eu tive a oportunidade de fazer um pequeno projeto nessa área que é tão importante e relevante. Essa foi a minha primeira vez trabalhando com redes neurais e ajuste de hiperparâmetros, e tenho certeza que na próxima vez terei resultados ainda melhores.\nimbalanced-learn documentation\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nAccuracy, Precision, Recall or F1? - Koo Ping Shung\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nscikit-learn documentation\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://devmedeiros.com/pt/post/2022-05-30-churn-rate-challenge/","summary":"\u003cp\u003e\u003cstrong\u003eFerramentas utilizadas:\u003c/strong\u003e Python, seaborn, scikit-learn, imbalanced-learn\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eCategoria:\u003c/strong\u003e Análise de Dados, Aprendizado de Máquina\u003c/p\u003e\n\u003chr\u003e","title":"Data Science Challenge - Churn Rate"},{"content":"O que é UX/UI? UX é a sigla em inglês para Experiência de Usuário, é um conceito recente que fala sobre tomar decisões de design pensando na experiência do usuário final. O designer de UX precisa se preocupar se seu produto é fácil de usar e intuitivo, fazendo mudanças nele sempre que necessário para se adequar as necessidades do usuário.\nUI, do inglês, significa Interface do Usuário. É tudo aquilo que está envolvido na interação do usuário e o produto. O designer de UI é responsável por desenvolver interfaces, não limitado apenas aos aspectos visuais, também é importante garantir que sejam funcionais, usáveis, e que em geral, contribuem para uma boa experiência do usuário.\nComo Melhorar a Experiência dos Painéis de BI? Muitos pessoas veem os painéis de BI como páginas de web e isso traz algumas expectativas de uso. Por exemplo, a maioria dos sites que possuem algum sistema de navegação usam um menu superior com botões, um menu lateral (mais comum no Brasil sendo na esquerda, mas em alguns países é na direita) ou um menu sanduíche (aquele que clicamos no botão e aparece as opções).\nJaqueline Medeiros - Todos os direitos reservados\nCom isso uma grande parcela das pessoas que utilizam os painéis esperam encontrar botões de navegação e segmentação (filtro) de dados nestes locais, além de outras informações como logo e título.\nSegmentação de Dados Também comummente chamado de filtro de dados é uma peça fundamental de diversos dashboards, seu posicionamento precisa ser definido com cuidado, pois se estiver num lugar que o usuário não espera pode impedir que seu painel seja usado eficientemente, além disso manter um padrão visual para todos os seus filtros ajudam as pessoas a reconhecerem mais facilmente o que é ou não é um filtro.\nVocê pode e deve usar e experimentar com diversos temas nos seus trabalhos. O que importa, na hora de facilitar para os usuários, é a consistência, escolha um modelo de filtro com as cores desejadas e todas as especificações gráficas que são do seu interesse e utilize ela em todos os filtros, pois isso irá ajudar as pessoas a baterem o olho e reconhecer que aquilo é um filtro.\nÉ muito comum as pessoas quando estão lendo algo pelo computador posicionarem o ponteiro do mouse onde elas estão lendo. No caso do Power BI, isso torna mais fácil para o usuário encontrar o botão de limpar a segmentação de dados, pois ao passar o mouse no nome do filtro aparece uma borrachinha no lado esquerdo onde fica o nome do filtro. Tá mais se você usa o Power BI provavelmente já sabia disso, mas podemos melhorar isso usando algo que o usuário já conhece, que é a borrachinha, e criar um botão usando a borrachinha como ícone e fazer com que esse botão limpe todos os filtros ao mesmo tempo. Caso queira saber como fazer esse botão aqui no fórum do Power BI explica como.\nEssa é uma característica que pode ter sua importância despercebida num primeiro momento, mas algo que ocorre muito é os usuários do painel não perceberem quais filtros eles usaram ou usarem tantos que só querem poder limpar a seleção de forma mais rápida e eficiente, então esse botão torna o processo muito mais user friendly.\nNavegação de Página A navegação de páginas nativa do Power BI não é intuitiva para a maioria das pessoas e em muitas vezes você pode querer direcionar a navegação num fluxo específico que facilita o entendimento e contribui para o storytelling planejado. Neste caso temos a opção de ocultar todas as abas do relatório, exceto uma que é a página de abertura/inicial. Mas qual a melhor forma de direcionar o usuário para as demais páginas? Bem, isso depende do que você está fazendo. Suponha que seu relatório seja bem simples, você tem uma aba de visão geral e outra aba com um detalhamento, um simples botão resolveria o seu problema, mas caso seu relatório seja muito extenso pode ser inviável colocar um botão para cada aba em todas as páginas.\nJaqueline Medeiros - Todos os direitos reservados\nNeste caso pode ser interessante considerar ter uma página inicial que leva a todas as páginas do relatório e colocar um botão de home ou voltar nas outras páginas.\nComo Melhorar a Interface dos Dashboards? Programas de Prototipagem Usar um programa específico para fazer o protótipo do seu painel permite uma liberdade artística maior comparado ao que os aplicativos de Business Intelligence normalmente fornecem. O Figma é ótimo para isso, você pode criar backgrounds e protótipos avançados com ótima qualidade para usar em seus painéis de BI.\nVeja um exemplo de um painel que eu fiz a alguns meses atrás:\nPainel com os dados O plano de fundo desse painel foi feito completamente no Figma, até mesmo alguns dos títulos dos visuais do BI.\nBackground feito no Figma Você pode encontrar mais painéis de Power BI que eu fiz para o Alura Challenge.\nFiguras e Ícones Vetor criado por pch.vector - br.freepik.com\nFiguras e ícones quando usados corretamente ajudam a destacar o painel, o torna mais chamativo e bonito. Há diversas formas de se conseguir imagens, caso você ou sua equipe não consigam criar vocês mesmo existe a opção de usar plataformas online que disponibilizam imagens vetorizadas. Nessas plataformas existem as opções gratuitas, em que exigem algum tipo de atribuição, e opções premium (pagas), em que muitas vezes não precisa atribuir o autor e possuem uma qualidade maior.\nÍcone de ICONS8\n","permalink":"https://devmedeiros.com/pt/post/2022-04-29-ux-power-bi-dashboards/","summary":"Definindo UX/UI no contexto de Bussiness Intelligence para Dashboards com exemplos","title":"UX/UI em Dashboards de Business Intelligence"},{"content":"O que é o p-valor? Em estatística temos os testes de hipóteses, que são feitos para tomar uma decisão, de rejeitar ou não rejeitar a hipótese nula. Alguns exemplos de teste de hipótese são: Neyman-Pearson, Shapiro-Wilk, T de Student, entre outros.\nTodos testes de hipóteses possuem uma estatística de teste específico dele. E essa estatística é utilizada para avaliar o teste, mas essa tarefa pode ser cansativa até com o uso de computadores, pois a maioria dos softwares não devolvem como resposta a estatística de comparação, apenas a estatística amostral. Nesse caso o p-valor vem para facilitar essa comparação, pois ele já é uma representação dessa estatística de teste. Ele representa a probabilidade de se obter uma estatística de teste igual ou mais extrema que a calculada na sua amostra, considerando a hipótese nula como verdadeira.\nIsso facilita, pois sabendo o nível de confiança que você quer testar a sua hipótese basta comparar se o p-valor é menor ou maior que o seu nível de confiança, enquanto que se fosse usar a estatística de teste ainda seria necessário calcular a estatística para cada nível de confiança diferente que você fosse comparar. Então suponha que queira comparar 1%, 5% e 10%, você teria que calcular três estatísticas de teste diferentes para comparar a sua estatística amostral.\nComo usar ele? É muito comum quando estamos aprendendo estatística por conta própria lermos que se o p-valor for menor que 5% rejeitamos a hipótese ou que se for maior \u0026ldquo;aceitamos\u0026rdquo; a hipótese.\nO valor com o qual comparamos o p-valor deve ser definido juntamente com pessoas da área de negócio do que você está trabalhando, é muito comum em alguns setores utilizarem um p-valor muito pequeno como 1% ou até 0,1% e em outros usarem valores maiores de que 5%.\nVeja que um p-valor de 0,02 seria rejeitado se considerarmos α = 1%, mas não seria se α = 5%. Na dúvida sobre qual usar, primeiramente é recomendado tomar essa decisão antes de fazer o teste. Segundo, tenha em mente que um α = 1% vai ter uma confiança de 99% (1-α), enquanto que se fosse 5% seria apenas 95%. Pode parecer que é melhor pegar o valor que lhe dá mais \u0026ldquo;confiança\u0026rdquo;, mas um p-valor muito pequeno pode levar a mais rejeições da sua hipótese.\nA verdade é que os testes de hipóteses só nos dão a informação da rejeição, quando uma hipótese nula não é rejeitada isso quer dizer que não foram encontradas evidências que contradizem o que ela afirma, isso não quer dizer que provamos que ela está correta. Então é preciso tomar muito cuidado ao utilizar o p-valor.\nInterpretando a medida Uma forma de interpretar a medida muito comum é \u0026ldquo;Rejeita-se a hipótese nula, com α% de confiança\u0026rdquo;, no caso de rejeição (em que o p-valor \u0026gt; α) e no caso de não rejeição (p-valor \u0026lt; α) \u0026ldquo;Não foram encontradas evidências suficientes para rejeitar a hipótese nula, com α% de confiança\u0026rdquo;.\n","permalink":"https://devmedeiros.com/pt/post/2022-04-12-comprehending-the-p-value/","summary":"A definição do p-valor, como usar ele, e como interpretar a medida","title":"Compreendendo o P-Valor"},{"content":" O show, apresentado por Tyler Renelle da Depth, oferece uma lista com recursos na qual você pode encontrar todos os livros, cursos, e sites mencionados durante o podcast. É um ótimo podcast para quem está aprendendo sobre aprendizado de máquina, podendo ser útil para iniciantes, entusiastas, ou para alguém que está procurando se aprofundar ainda mais no tópico. Desde 2021 o show está sendo renovado para atualizar o conteúdo.\nO show começa falando sobre o que é ciência de dados e como ela se relaciona com aprendizado de máquina e inteligência artificial. Ele também fala sobre as primeiras tentativas em criar IA, com alguns exemplos vindos desde o século 13.\nEu gosto que o show fala sobre conceitos de aprendizado de máquina de uma forma diferente da qual eu estou acostumada. Quando pessoas falam sobre aprendizado de máquina, eu, como Estatística, sempre senti que entendia sobre o que as pessoas estavam falando, exceto que eu não conhecia algumas palavras chaves que elas usavam. Por exemplo, o que engenheiros de aprendizado de máquina chamam de features eu aprendi como variável, então eu ficava confusa quando conversava com amigos sobre aprendizado de máquina quando eles tinham um antecedente diferente do meu (ciência da computação, por exemplo). Esse é um dos motivos que faz eu gostar tanto do podcast, ele fala em ricos detalhes sobre cada conceito de aprendizado de máquina, então se ele fala sobre algo que eu conheço por outro nome eu consigo fazer a conexão.\n","permalink":"https://devmedeiros.com/pt/post/2022-01-24-podcast-review-mlg/","summary":"Uma resenha sobre o podcast Machine Learning Guide","title":"Review do Podcast - Machine Learning Guide"},{"content":"Recentemente eu terminei um curso na Alura chamado Python para Data Science e eu quero colocar o que eu aprendi em prática, para isso eu vou fazer uma análise descritiva nesse banco de dados Amazon Top 50 Bestselling Books 2009 - 2019. Nele há 550 livros e eles foram categorizados como fiction (ficção) e non-fiction (não ficção) pelo Goodreads. Todo o código pode ser visto aqui.\nEu comecei olhando as cinco primeiras observações do banco de dados.\nName Author User Rating Reviews Price Year Genre 10-Day Green Smoothie Cleanse JJ Smith 4.7 17350 8 2016 Non Fiction 11/22/63: A Novel Stephen King 4.6 2052 22 2011 Fiction 12 Rules for Life: An Antidote to Chaos Jordan B. Peterson 4.7 18979 15 2018 Non Fiction 1984 (Signet Classics) George Orwell 4.7 21424 6 2017 Fiction 5,000 Awesome Facts (About Everything!) (Natio\u0026hellip; National Geographic Kids 4.8 7665 12 2019 Non Fiction Aqui é possível ver que os dados tem o Year (ano) em que o livro estava no top 50 de mais vendidos, seu Price (preço), a média dos User Rating (avaliação dos usuários), total de Reviews (avaliações), Author (autor), Name (nome do livro) e por fim, Genre (gênero).\nNão há valores nulos no banco de dados. E dos 550 livros há 248 autores diferentes, então vamos ver quais autores possuem mais livros no top 50 dos mais vendidos neste período.\nAutor Número de livros Jeff Kinney 12 Gary Chapman 11 Rick Riordan 11 Suzanne Collins 11 American Psychological Association 10 Dr. Seuss 9 Gallup 9 Rob Elliott 8 Stephen R. Covey 7 Stephenie Meyer 7 Dav Pilkey 7 Bill O\u0026rsquo;Reilly 7 Eric Carle 7 O autor com mais livros no top 50 foi Jeff Kinney, empatado em segundo, com 11 livros, foi Gary Chapman, Rick Riordan, e Suzanne Collins. Empatado em 9º, está Stephen R. Covey, Stephenie Meyer, Dav Pilkey, Bill O\u0026rsquo;Reilly, e Eric Carle, com 7 livros cada.\nCom o gráfico de violino podemos ver como está concentrado a avaliação dos usuários e como os dados são compostos de livros bestsellers faz sentido que a avaliação dos usuários está em sua maioria concentrada em torno de 4.5 e 4.75.\nEsse boxplot da quantidade de avaliações por ano mostra que a variabilidade aumentou através dos anos, tendo o seu pico em 2014 e gradualmente estabilizando. Podemos ver também que nos primeiros anos, 2010 e 2011, havia mais outliers nos dados.\nGênero Avaliação do Usuário Preço Ficção 4.65 10.85 Não Ficção 4.60 14.84 A avaliação média do usuário por gênero parece ser semelhante, com apenas 0.05 de diferença, mas o preço já apresenta uma diferença maior, 10.85 para ficção e 14.84 para não ficção. Para termos certezas de que essas diferenças são estatisticamente significantes, eu vou utilizar o teste de Mann-Whitney.\nA hipótese nula do teste de Mann-Whitney é de que as amostras possuem a mesma distribuição, e em ambos os casos, nós rejeitamos a hipótese nula com 95% de confiança. O p-valor para os dados do preço foi de 8.34e-08 e o p-valor para a avaliação do usuário foi de 1.495e-07.\nPara mostrar visualmente quão diferente as suas distribuição são, podemos olhar para os seguintes gráficos.\nA distribuição para os preços de livros de ficção é fortemente inclinados para a esquerda e consistentemente diminuem a medida que o preço aumenta. Enquanto que os livros de ficção começam altos e se tornam ainda mais altos, com 120 e quase 140 ocorrências nas duas primeiras categorias, então ele rapidamente diminui.\nA distribuição para a avaliação do usuário do gênero de ficção lentamente aumenta, tendo seu pico próximo de 4.8. E a distribuição para o gênero de não ficção tem seu pico logo após 4.6.\n","permalink":"https://devmedeiros.com/pt/post/2021-12-28-amazon-top-50-books/","summary":"Análise Exploratória de Dados dos 50 livros mais bem vendidos da Amazon de 2009 - 2019","title":"Análise Descritiva do Top 50 Livros Bestsellers da Amazon 2009 - 2019"},{"content":"Introdução Eu estou aprendendo visualização de dados no Python e eu me vejo como alguém que aprende fazendo, por isso eu vou fazer alguns gráficos simples usando o pacote seaborn que poderão ser utilizados como referência sempre que precisar refrescar a memória.\nPrimeiramente é necessários que os pacotes estejam propriamente importados, após isso eu carrego o banco de dados iris.\nimport pandas as pd import seaborn as sns import matplotlib.pyplot as plt url = \u0026#34;https://git.io/JXciW\u0026#34; iris = pd.read_csv(url) Caso não esteja familiarizado com o banco de dados iris, veja as cinco primeiras linhas dele a seguir:\nsepal_length sepal_width petal_length petal_width species 5.1 3.5 1.4 0.2 setosa 4.9 3.0 1.4 0.2 setosa 4.7 3.2 1.3 0.2 setosa 4.6 3.1 1.5 0.2 setosa 5.0 3.6 1.4 0.2 setosa Gráfico de barras Criar um simples gráfico de barras.\nsns.barplot(x=\u0026#34;species\u0026#34;, y=\u0026#34;petal_width\u0026#34;, data=iris) Fazendo um gráfico de barras horizontais.\nsns.barplot(x=\u0026#34;petal_width\u0026#34;, y=\u0026#34;species\u0026#34;, data=iris) Ordem das barras personalizada.\nsns.barplot( x=\u0026#34;species\u0026#34;, y=\u0026#34;petal_width\u0026#34;, data=iris, order=[\u0026#34;virginica\u0026#34;, \u0026#34;setosa\u0026#34;, \u0026#34;versicolor\u0026#34;]) Acrescentar limites para as barras de erro.\nsns.barplot(x=\u0026#34;species\u0026#34;, y=\u0026#34;petal_width\u0026#34;, data=iris, capsize=.2) Gráfico de barra sem barras de erro.\nsns.barplot(x=\u0026#34;species\u0026#34;, y=\u0026#34;petal_width\u0026#34;, data=iris, ci=None) Gráfico de dispersão Um gráfico de dispersão simples.\nsns.scatterplot(x=\u0026#34;sepal_width\u0026#34;, y=\u0026#34;petal_width\u0026#34;, data=iris) Acrescentando grupos no gráfico de dispersão.\nsns.scatterplot(x=\u0026#34;sepal_width\u0026#34;, y=\u0026#34;petal_width\u0026#34;, data=iris, hue=\u0026#34;species\u0026#34;) Acrescentando grupos e escalando os pontos de um gráfico de dispersão.\nsns.scatterplot( x=\u0026#34;sepal_width\u0026#34;, y=\u0026#34;petal_width\u0026#34;, data=iris, hue=\u0026#34;sepal_length\u0026#34;, size=\u0026#34;sepal_length\u0026#34;) Legenda e Eixos Para mover a legenda do gráfico para fora da área de plotagem, você pode utilizar bbox_to_anchor = (1,1), loc=2. O gráfico a seguir possui um titulo personalizado, um novo título para o eixo x e pro eixo y.\nsns.scatterplot(x=\u0026#34;sepal_width\u0026#34;, y=\u0026#34;petal_width\u0026#34;, data=iris, hue=\u0026#34;species\u0026#34;) plt.legend( title=\u0026#34;Species\u0026#34;, bbox_to_anchor = (1,1), loc=2) plt.xlabel(\u0026#34;Sepal Width\u0026#34;) plt.ylabel(\u0026#34;Petal Width\u0026#34;) plt.title(\u0026#34;Sepal Width x Petal Width\u0026#34;) ","permalink":"https://devmedeiros.com/pt/post/2021-11-07-seaborn-package-guide/","summary":"Um guia simples de como fazer gráficos básicos usando o pacote seaborn do Python","title":"Guia do Pacote de Python seaborn"},{"content":"O pacote data.table é um dos pacotes de manipulação de dados mais rápido, atualmente ele é mais rápido até que o pandas e dplyr 1. A sintaxe de um data.table é dt[i, j, by], em que:\ni é utilizado para amostrar linhas j é utilizado para amostrar colunas by é utilizado para amostrar grupos, igual ao GROUP BY do SQL Você pode ler em voz alta como2:\nPegue dt, amostra/reordene as linhas usando i, então calcule j, agrupando por by.\nUm data.table também é um data.frame e todas as manipulações de dados básicas que você pode usar em um data.frame se aplica a um data.table. Como ncol(), nrow(), names(), summary(). Mas ele não para por aí, por exemplo data.table possui uma variável especial .N que é um integral que contains os números das linhas no grupo. Se você usar dt[.N] você verá a última linha do seu data.table.\nOutra coisa interessante do data.table é que se você quiser filtrar/amostrar uma coluna, você não precisa utilizar df$x[df$x == 1] você pode simplesmente usar dt[x == 1] o que torna o seu código mais limpo e fácil de ler.\nVocê também tem a possibilidade de utilizar operadores especiais como: %like%, %in% e %between%. Estes operadores funcionam como operadores de SQL, LIKE, IN, e BETWEEN, respectivamente.\nSe você está familiarizado com SQL tem uma coisa que esse pacote oferece que irá chamar a sua atenção. No inglês se chama chaining (acorrentar), que permite a você a fazer uma sequência de operações em um data.table, você apenas precisa utilizar dt[][] e acorrentar múltiplas operações com \u0026ldquo;[]\u0026rdquo;.\nMas este não são todos os operadores, com := você pode alterar dados sem fazer uma nova cópia na memória.\nCaso queira começar a usar o pacote eu sugiro que use a cheatsheet (folha de referência/colinha). É extramente útil caso já tenha um conhecimento básico sobre data.frames.\nhttps://h2oai.github.io/db-benchmark/\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://cran.r-project.org/web/packages/data.table/vignettes/datatable-intro.html\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://devmedeiros.com/pt/post/2021-10-27-data-table/","summary":"Um dos pacotes de manipulação de dados mais rápidos do R","title":"Uma Visão Geral Sobre o Pacote data.table do R"},{"content":"Ferramentas utilizadas: R, ggplot, Shiny\nCategoria: Simulação\nO meu objetivo com este projeto é de simular o ambiente de um jogo de 21, também conhecido como blackjack. Assim, eu decidi fazer diversas funções para emular o comportamento do dealer, de um jogador iniciante, um jogador cauteloso e um estrategista. Com esse conjunto de funções você pode rodar um jogo com p jogadores, d baralhos e quaisquer combinações de arquétipos de jogadores. Além disso, também pode rodar o jogo n vezes.\nEu também fiz uma aplicação shiny para demonstrar como a simulação funciona. No aplicativo, você é limitado no número de jogadores, mas caso queira rodar o código com mais jogadores eu sugiro que olhe o respositório do GitHub. Nele você encontra as regras consideradas para a simulação e o código completo.\nCaso seja familiar com a linguagem de programação R, você também pode rodar o aplicativo localmente, basta carregar a biblioteca do shiny library(shiny) e rodar o código runGitHub(\u0026quot;blackjack-simulation\u0026quot;, \u0026quot;devmedeiros\u0026quot;, ref = \u0026quot;main\u0026quot;).\nO app é composto de uma barra lateral com um espaço para escolher os arquétipos, o número de baralhos a ser usados e quantas rodadas você quer simular. Dependendo de quantas rodadas você escolher a simulação pode ficar mais lenta, pois a simulação roda de acordo com as suas escolhas todas vez que clica no botão RUN SIMULATION RUN.\nNa aba plot, nota-se a evolução da taxa de perda através das rodadas.\n1 quer dizer que o jogador perdeu aquela rodada e um 0 quer dizer que ele ganhou.\nA aba game setup mostra todas as cartas distribuídas na simulação, cada carta foi entregue da esquerda para a direita e uma célula em branco quer dizer que o jogador não pediu pra receber outra carta (hit, bate).\nPor fim, a aba lose rate mostra os mesmo dados da aba plot, mas em formato de tabela. Isso é útil quando se quer analisar como uma estratégia foi melhor do que outra.\n","permalink":"https://devmedeiros.com/pt/post/2021-10-24-blackjack-simulation/","summary":"\u003cp\u003e\u003cstrong\u003eFerramentas utilizadas:\u003c/strong\u003e R, ggplot, Shiny\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eCategoria:\u003c/strong\u003e Simulação\u003c/p\u003e\n\u003chr\u003e","title":"Simulação de 21"},{"content":"Eu quero fazer uma análise de sentimentos usando o R como uma forma de aprender. Com isso em mente começamos carregando todos os pacotes que iremos utilizar.\nlibrary(readr) library(dplyr) library(tidytext) library(tokenizers) library(stopwords) library(ggplot2) Então precisamos carregar nosso banco de dados. Esses dados são do Kaggle Fake and real news dataset.\nFake \u0026lt;- read_csv(\u0026#39;~/fakenews/Fake.csv\u0026#39;) True \u0026lt;- read.csv(\u0026#39;~/fakenews/True.csv\u0026#39;) Eu quero unir ambos os dados, mas antes é preciso criar uma nova columa que irá informar de onde os dados vieram.\nFake$news \u0026lt;- \u0026#39;fake\u0026#39; True$news \u0026lt;- \u0026#39;real\u0026#39; data \u0026lt;- rbind(Fake,True) Agora podemos iniciar a limpeza dos dados. Neste primeiro momento, fazemos os tokens nas variáveis title e text. Em seguida iremos remover as stopwords (palavras redundantes) de acordo com a fonte snowball do pacote stopwords.\ntitle \u0026lt;- tibble(news = data$news, text = data$title) corpus \u0026lt;- tibble(news = data$news, corpus = data$text) tidy_title \u0026lt;- title %\u0026gt;% unnest_tokens(word, text, token = \u0026#39;words\u0026#39;) %\u0026gt;% filter(!(word %in% stopwords(source = \u0026#39;snowball\u0026#39;))) tidy_corpus \u0026lt;- corpus %\u0026gt;% unnest_tokens(word, corpus, token = \u0026#39;words\u0026#39;) %\u0026gt;% filter(!(word %in% stopwords(source = \u0026#34;snowball\u0026#34;))) Com os dados limpos podemos selecionar as dez palavras mais frequentes dos títulos das noticias, separado por grupo real ou falso.\np0 \u0026lt;- tidy_title %\u0026gt;% group_by(news, word) %\u0026gt;% summarise(n = n()) %\u0026gt;% arrange(desc(n)) %\u0026gt;% slice(1:10) Títulos de notícias falsas mencionam muito mais video e trump, 8477 e 7874, respectivamente. Já no caso de títulos de notícias reais, trump também é uma das palavras mais mencionadas, aparecendo em primeiro com 4883 aparições, seguido por u.s com 4187 e says com 2981.\nAgora iremos preparar os dados para a análise de sentimento. Eu estou interessada em classificar os dados em sentimentos de alegria, raiva, medo ou surpresa, por exemplo. Então eu iriei utilizar o os dados de nrc Saif Mohammad and Peter Turney.\np1 \u0026lt;- tidy_title %\u0026gt;% inner_join(get_sentiments(\u0026#39;nrc\u0026#39;)) %\u0026gt;% group_by(sentiment, news) %\u0026gt;% summarise(n = n()) %\u0026gt;% mutate(prop = n/sum(n)) Disgust (nojo) aparenta ser o sentimento mais comum envolvendo títulos de notícias falsas, enquanto que trust (confiança) é o menor, mesmo que ainda seja maior que 50%. Em geral, títulos de notícias falsas aparentam ter mais \u0026ldquo;sentimento\u0026rdquo; do que títulos de notícias reais, neste banco de dados. Isso vale até para sentimentos positivos como joy (alegria) e surprise (surpresa).\np2 \u0026lt;- tidy_corpus %\u0026gt;% inner_join(get_sentiments(\u0026#39;nrc\u0026#39;)) %\u0026gt;% group_by(sentiment, news) %\u0026gt;% summarise(n = n()) %\u0026gt;% mutate(prop = n/sum(n)) Para o corpo das notícias reais pode-se notas que os mesmos sentimentos são prevalentes, mas a proporção é menor comparado ao título. Um artigo de notícias falsas perde confiança (trust) quando o leitor lê o corpo do artigo. Ele também se torna menos negativo (negative) e apresenta menos medo (fear)\nUma melhoria que poderia ser feito aqui é tentar construir nosso próprio dicionário de palavras redundantes (stopwords) e alterar a forma que a toneização foi feita. Pois houveram momentos em que trump e trump\u0026rsquo;s não corresponderam a mesma coisa e se estes dados tivessem sido usados para treinar um modelo isso poderia se tornar um problema.\n","permalink":"https://devmedeiros.com/pt/post/2021-10-12-fakenews-sentiment/","summary":"Análise de Sentimento comparando notícias falsas e notícias reais","title":"Análise de Sentimentos de Notícias Falsas vs Notícias Reais"},{"content":"A seguir são alguns projetos que eu fiz no meu tempo livre. Eles não estão em nenhuma ordem em particular.\nNota Fiscal Goiana Descrição: Esse projeto apresenta uma solução completa de ETL usando os dados da Nota Fiscal Goiana. Ele também apresenta os dados tratados num relatório de Power BI.\nStacks: Python, Power BI, SQL, MySQL, e Github Actions\nHabilidades: ETL, Visualização de Dados, e Scraping\nBlog pessoal Descrição: Um blog (onde você está agora!) no qual eu escrevo sobre o que estou aprendendo, basicamente um portfólio.\nHabilidades: Hugo, tema Papermod, e Github Actions\nPrimeiro Data Science Challenge da Alura Descrição: Esse projeto trás a minha abordagem no primeiro desafio da Alura de Data Science. Eu fui desafiada a limpar um banco de dados e prever a taxa de evasão (churn rate) de dados desbalanceados.\nStacks: Python (pandas, seaborn, matplotlib, numpy, e sklearn)\nHabilidades: Aprendizado de Máquina (Machine Learning), Limpeza de Dados, Dados Desbalanceados, e Visualização de Dados\nSegundo BI Challenge da Alura Descrição: Esse projeto trás a minha abordagem no segundo desagio de BI da Alura. Ele consiste em fazer três painéis interativos de Power BI em que cada um trazia seu desafio próprio.\nStacks: Power BI\nHabilidades: Visualização de Dados\nAplicativo de Classificação de Score de Crédito Descrição: Este projeto limpa um banco de dados de score de crédito, e contrói e serializa um modelo de aprendizado de máquina. Esse modelo de ML é usado num aplicativo Streamlit onde você pode entrar com dados fícticios e recebe uma classificação de score de crédito.\nStacks: Python (pandas, seaborn, matplotlib, numpy, sklearn, e pickle) e Streamlit\nHabilidades: Aprendizado de Máquina (Machine Learning), Limpeza de Dados, e Visualização de Dados\n","permalink":"https://devmedeiros.com/pt/projects/","summary":"projects","title":"Projetos"},{"content":"Oi! Meu nome é Jaqueline Medeiros e eu sou uma Analista de Dados. Eu sempre gostei de aprender coisas novas e explorar novas possibilidades e este blog é um canal no qual eu crio e descubro coisas novas relacionadas a Ciência de Dados.\nEu quero escrever posts que possam ser lidos por pessoas que não possuem um background técnico, mas também quero mostrar e falar sobre os aspectos de programação do que estou aprendendo, então você irá encontrar um pouco de tudo aqui.\nVocê pode me encontrar em:\nLinkedin: medeiros-jaqueline Github: devmedeiros Twitter: dev_medeiros ","permalink":"https://devmedeiros.com/pt/about/","summary":"about","title":"Sobre"}]