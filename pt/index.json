[{"content":"Recentemente eu terminei um curso na Alura chamado Python para Data Science e eu quero colocar o que eu aprendi em prática, para isso eu vou fazer uma análise descritiva nesse banco de dados Amazon Top 50 Bestselling Books 2009 - 2019. Nele há 550 livros e eles foram categorizados como fiction (ficção) e non-fiction (não ficção) pelo Goodreads. Todo o código pode ser visto aqui.\nEu comecei olhando as cinco primeiras observações do banco de dados.\n   Name Author User Rating Reviews Price Year Genre     10-Day Green Smoothie Cleanse JJ Smith 4.7 17350 8 2016 Non Fiction   11/22/63: A Novel Stephen King 4.6 2052 22 2011 Fiction   12 Rules for Life: An Antidote to Chaos Jordan B. Peterson 4.7 18979 15 2018 Non Fiction   1984 (Signet Classics) George Orwell 4.7 21424 6 2017 Fiction   5,000 Awesome Facts (About Everything!) (Natio\u0026hellip; National Geographic Kids 4.8 7665 12 2019 Non Fiction    Aqui é possível ver que os dados tem o Year (ano) em que o livro estava no top 50 de mais vendidos, seu Price (preço), a média dos User Rating (avaliação dos usuários), total de Reviews (avaliações), Author (autor), Name (nome do livro) e por fim, Genre (gênero).\nNão há valores nulos no banco de dados. E dos 550 livros há 248 autores diferentes, então vamos ver quais autores possuem mais livros no top 50 dos mais vendidos neste período.\n   Autor Número de livros     Jeff Kinney 12   Gary Chapman 11   Rick Riordan 11   Suzanne Collins 11   American Psychological Association 10   Dr. Seuss 9   Gallup 9   Rob Elliott 8   Stephen R. Covey 7   Stephenie Meyer 7   Dav Pilkey 7   Bill O\u0026rsquo;Reilly 7   Eric Carle 7    O autor com mais livros no top 50 foi Jeff Kinney, empatado em segundo, com 11 livros, foi Gary Chapman, Rick Riordan, e Suzanne Collins. Empatado em 9º, está Stephen R. Covey, Stephenie Meyer, Dav Pilkey, Bill O\u0026rsquo;Reilly, e Eric Carle, com 7 livros cada.\nCom o gráfico de violino podemos ver como está concentrado a avaliação dos usuários e como os dados são compostos de livros bestsellers faz sentido que a avaliação dos usuários está em sua maioria concentrada em torno de 4.5 e 4.75.\nEsse boxplot da quantidade de avaliações por ano mostra que a variabilidade aumentou através dos anos, tendo o seu pico em 2014 e gradualmente estabilizando. Podemos ver também que nos primeiros anos, 2010 e 2011, havia mais outliers nos dados.\n   Gênero Avaliação do Usuário Preço     Ficção 4.65 10.85   Não Ficção 4.60 14.84    A avaliação média do usuário por gênero parece ser semelhante, com apenas 0.05 de diferença, mas o preço já apresenta uma diferença maior, 10.85 para ficção e 14.84 para não ficção. Para termos certezas de que essas diferenças são estatisticamente significantes, eu vou utilizar o teste de Mann-Whitney.\nA hipótese nula do teste de Mann-Whitney é de que as amostras possuem a mesma distribuição, e em ambos os casos, nós rejeitamos a hipótese nula com 95% de confiança. O p-valor para os dados do preço foi de 8.34e-08 e o p-valor para a avaliação do usuário foi de 1.495e-07.\nPara mostrar visualmente quão diferente as suas distribuição são, podemos olhar para os seguintes gráficos.\nA distribuição para os preços de livros de ficção é fortemente inclinados para a esquerda e consistentemente diminuem a medida que o preço aumenta. Enquanto que os livros de ficção começam altos e se tornam ainda mais altos, com 120 e quase 140 ocorrências nas duas primeiras categorias, então ele rapidamente diminui.\nA distribuição para a avaliação do usuário do gênero de ficção lentamente aumenta, tendo seu pico próximo de 4.8. E a distribuição para o gênero de não ficção tem seu pico logo após 4.6.\n","permalink":"https://devmedeiros.github.io/pt/post/2021-12-28-amazon-top-50-books/","summary":"Recentemente eu terminei um curso na Alura chamado Python para Data Science e eu quero colocar o que eu aprendi em prática, para isso eu vou fazer uma análise descritiva nesse banco de dados Amazon Top 50 Bestselling Books 2009 - 2019. Nele há 550 livros e eles foram categorizados como fiction (ficção) e non-fiction (não ficção) pelo Goodreads. Todo o código pode ser visto aqui.\nEu comecei olhando as cinco primeiras observações do banco de dados.","title":"Análise Descritiva do Top 50 Livros Bestsellers da Amazon 2009 - 2019"},{"content":"Introdução Eu estou aprendendo visualização de dados no Python e eu me vejo como alguém que aprende fazendo, por isso eu vou fazer alguns gráficos simples usando o pacote seaborn que poderão ser utilizados como referência sempre que precisar refrescar a memória.\nPrimeiramente é necessários que os pacotes estejam propriamente importados, após isso eu carrego o banco de dados iris.\nimport pandas as pd import seaborn as sns import matplotlib.pyplot as plt url = \u0026#34;https://git.io/JXciW\u0026#34; iris = pd.read_csv(url) Caso não esteja familiarizado com o banco de dados iris, veja as cinco primeiras linhas dele a seguir:\n   sepal_length sepal_width petal_length petal_width species     5.1 3.5 1.4 0.2 setosa   4.9 3.0 1.4 0.2 setosa   4.7 3.2 1.3 0.2 setosa   4.6 3.1 1.5 0.2 setosa   5.0 3.6 1.4 0.2 setosa    Gráfico de barras Criar um simples gráfico de barras.\nsns.barplot(x=\u0026#34;species\u0026#34;, y=\u0026#34;petal_width\u0026#34;, data=iris) Fazendo um gráfico de barras horizontais.\nsns.barplot(x=\u0026#34;petal_width\u0026#34;, y=\u0026#34;species\u0026#34;, data=iris) Ordem das barras personalizada.\nsns.barplot( x=\u0026#34;species\u0026#34;, y=\u0026#34;petal_width\u0026#34;, data=iris, order=[\u0026#34;virginica\u0026#34;, \u0026#34;setosa\u0026#34;, \u0026#34;versicolor\u0026#34;]) Acrescentar limites para as barras de erro.\nsns.barplot(x=\u0026#34;species\u0026#34;, y=\u0026#34;petal_width\u0026#34;, data=iris, capsize=.2) Gráfico de barra sem barras de erro.\nsns.barplot(x=\u0026#34;species\u0026#34;, y=\u0026#34;petal_width\u0026#34;, data=iris, ci=None) Gráfico de dispersão Um gráfico de dispersão simples.\nsns.scatterplot(x=\u0026#34;sepal_width\u0026#34;, y=\u0026#34;petal_width\u0026#34;, data=iris) Acrescentando grupos no gráfico de dispersão.\nsns.scatterplot(x=\u0026#34;sepal_width\u0026#34;, y=\u0026#34;petal_width\u0026#34;, data=iris, hue=\u0026#34;species\u0026#34;) Acrescentando grupos e escalando os pontos de um gráfico de dispersão.\nsns.scatterplot( x=\u0026#34;sepal_width\u0026#34;, y=\u0026#34;petal_width\u0026#34;, data=iris, hue=\u0026#34;sepal_length\u0026#34;, size=\u0026#34;sepal_length\u0026#34;) Legenda e Eixos Para mover a legenda do gráfico para fora da área de plotagem, você pode utilizar bbox_to_anchor = (1,1), loc=2. O gráfico a seguir possui um titulo personalizado, um novo título para o eixo x e pro eixo y.\nsns.scatterplot(x=\u0026#34;sepal_width\u0026#34;, y=\u0026#34;petal_width\u0026#34;, data=iris, hue=\u0026#34;species\u0026#34;) plt.legend( title=\u0026#34;Species\u0026#34;, bbox_to_anchor = (1,1), loc=2) plt.xlabel(\u0026#34;Sepal Width\u0026#34;) plt.ylabel(\u0026#34;Petal Width\u0026#34;) plt.title(\u0026#34;Sepal Width x Petal Width\u0026#34;) ","permalink":"https://devmedeiros.github.io/pt/post/2020-11-07-seaborn-package-guide/","summary":"Introdução Eu estou aprendendo visualização de dados no Python e eu me vejo como alguém que aprende fazendo, por isso eu vou fazer alguns gráficos simples usando o pacote seaborn que poderão ser utilizados como referência sempre que precisar refrescar a memória.\nPrimeiramente é necessários que os pacotes estejam propriamente importados, após isso eu carrego o banco de dados iris.\nimport pandas as pd import seaborn as sns import matplotlib.pyplot as plt url = \u0026#34;https://git.","title":"Guia do Pacote de Python seaborn"},{"content":"O pacote data.table é um dos pacotes de manipulação de dados mais rápido, atualmente ele é mais rápido até que o pandas e dplyr 1. A sintaxe de um data.table é dt[i, j, by], em que:\n i é utilizado para amostrar linhas j é utilizado para amostrar colunas by é utilizado para amostrar grupos, igual ao GROUP BY do SQL  Você pode ler em voz alta como2:\n Pegue dt, amostra/reordene as linhas usando i, então calcule j, agrupando por by.\n Um data.table também é um data.frame e todas as manipulações de dados básicas que você pode usar em um data.frame se aplica a um data.table. Como ncol(), nrow(), names(), summary(). Mas ele não para por aí, por exemplo data.table possui uma variável especial .N que é um integral que contains os números das linhas no grupo. Se você usar dt[.N] você verá a última linha do seu data.table.\nOutra coisa interessante do data.table é que se você quiser filtrar/amostrar uma coluna, você não precisa utilizar df$x[df$x == 1] você pode simplesmente usar dt[x == 1] o que torna o seu código mais limpo e fácil de ler.\nVocê também tem a possibilidade de utilizar operadores especiais como: %like%, %in% e %between%. Estes operadores funcionam como operadores de SQL, LIKE, IN, e BETWEEN, respectivamente.\nSe você está familiarizado com SQL tem uma coisa que esse pacote oferece que irá chamar a sua atenção. No inglês se chama chaining (acorrentar), que permite a você a fazer uma sequência de operações em um data.table, você apenas precisa utilizar dt[][] e acorrentar múltiplas operações com \u0026ldquo;[]\u0026rdquo;.\nMas este não são todos os operadores, com := você pode alterar dados sem fazer uma nova cópia na memória.\nCaso queira começar a usar o pacote eu sugiro que use a cheatsheet (folha de referência/colinha). É extramente útil caso já tenha um conhecimento básico sobre data.frames.\n  https://h2oai.github.io/db-benchmark/\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n https://cran.r-project.org/web/packages/data.table/vignettes/datatable-intro.html\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n   ","permalink":"https://devmedeiros.github.io/pt/post/2021-10-27-data-table/","summary":"O pacote data.table é um dos pacotes de manipulação de dados mais rápido, atualmente ele é mais rápido até que o pandas e dplyr 1. A sintaxe de um data.table é dt[i, j, by], em que:\n i é utilizado para amostrar linhas j é utilizado para amostrar colunas by é utilizado para amostrar grupos, igual ao GROUP BY do SQL  Você pode ler em voz alta como2:\n Pegue dt, amostra/reordene as linhas usando i, então calcule j, agrupando por by.","title":"Uma Visão Geral Sobre o Pacote data.table do R"},{"content":"Ferramentas utilizadas: R, ggplot, Shiny\nCategoria: Simulação\n O meu objetivo com este projeto é de simular o ambiente de um jogo de 21, também conhecido como blackjack. Assim, eu decidi fazer diversas funções para emular o comportamento do dealer, de um jogador iniciante, um jogador cauteloso e um estrategista. Com esse conjunto de funções você pode rodar um jogo com p jogadores, d baralhos e quaisquer combinações de arquétipos de jogadores. Além disso, também pode rodar o jogo n vezes.\nEu também fiz uma aplicação shiny para demonstrar como a simulação funciona. No aplicativo, você é limitado no número de jogadores, mas caso queira rodar o código com mais jogadores eu sugiro que olhe o respositório do GitHub. Nele você encontra as regras consideradas para a simulação e o código completo.\nCaso seja familiar com a linguagem de programação R, você também pode rodar o aplicativo localmente, basta carregar a biblioteca do shiny library(shiny) e rodar o código runGitHub(\u0026quot;blackjack-simulation\u0026quot;, \u0026quot;devmedeiros\u0026quot;, ref = \u0026quot;main\u0026quot;).\nO app é composto de uma barra lateral com um espaço para escolher os arquétipos, o número de baralhos a ser usados e quantas rodadas você quer simular. Dependendo de quantas rodadas você escolher a simulação pode ficar mais lenta, pois a simulação roda de acordo com as suas escolhas todas vez que clica no botão RUN SIMULATION RUN.\nNa aba plot, nota-se a evolução da taxa de perda através das rodadas.\n1 quer dizer que o jogador perdeu aquela rodada e um 0 quer dizer que ele ganhou.\nA aba game setup mostra todas as cartas distribuídas na simulação, cada carta foi entregue da esquerda para a direita e uma célula em branco quer dizer que o jogador não pediu pra receber outra carta (hit, bate).\nPor fim, a aba lose rate mostra os mesmo dados da aba plot, mas em formato de tabela. Isso é útil quando se quer analisar como uma estratégia foi melhor do que outra.\n","permalink":"https://devmedeiros.github.io/pt/post/2021-10-24-blackjack-simulation/","summary":"\u003cp\u003e\u003cstrong\u003eFerramentas utilizadas:\u003c/strong\u003e R, ggplot, Shiny\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eCategoria:\u003c/strong\u003e Simulação\u003c/p\u003e\n\u003chr\u003e","title":"Simulação de 21"},{"content":"Eu quero fazer uma análise de sentimentos usando o R como uma forma de aprender. Com isso em mente começamos carregando todos os pacotes que iremos utilizar.\nlibrary(readr) library(dplyr) library(tidytext) library(tokenizers) library(stopwords) library(ggplot2) Então precisamos carregar nosso banco de dados. Esses dados são do Kaggle Fake and real news dataset.\nFake \u0026lt;- read_csv(\u0026#39;~/fakenews/Fake.csv\u0026#39;) True \u0026lt;- read.csv(\u0026#39;~/fakenews/True.csv\u0026#39;) Eu quero unir ambos os dados, mas antes é preciso criar uma nova columa que irá informar de onde os dados vieram.\nFake$news \u0026lt;- \u0026#39;fake\u0026#39; True$news \u0026lt;- \u0026#39;real\u0026#39; data \u0026lt;- rbind(Fake,True) Agora podemos iniciar a limpeza dos dados. Neste primeiro momento, fazemos os tokens nas variáveis title e text. Em seguida iremos remover as stopwords (palavras redundantes) de acordo com a fonte snowball do pacote stopwords.\ntitle \u0026lt;- tibble(news = data$news, text = data$title) corpus \u0026lt;- tibble(news = data$news, corpus = data$text) tidy_title \u0026lt;- title %\u0026gt;% unnest_tokens(word, text, token = \u0026#39;words\u0026#39;) %\u0026gt;% filter(!(word %in% stopwords(source = \u0026#39;snowball\u0026#39;))) tidy_corpus \u0026lt;- corpus %\u0026gt;% unnest_tokens(word, corpus, token = \u0026#39;words\u0026#39;) %\u0026gt;% filter(!(word %in% stopwords(source = \u0026#34;snowball\u0026#34;))) Com os dados limpos podemos selecionar as dez palavras mais frequentes dos títulos das noticias, separado por grupo real ou falso.\np0 \u0026lt;- tidy_title %\u0026gt;% group_by(news, word) %\u0026gt;% summarise(n = n()) %\u0026gt;% arrange(desc(n)) %\u0026gt;% slice(1:10) Títulos de notícias falsas mencionam muito mais video e trump, 8477 e 7874, respectivamente. Já no caso de títulos de notícias reais, trump também é uma das palavras mais mencionadas, aparecendo em primeiro com 4883 aparições, seguido por u.s com 4187 e says com 2981.\nAgora iremos preparar os dados para a análise de sentimento. Eu estou interessada em classificar os dados em sentimentos de alegria, raiva, medo ou surpresa, por exemplo. Então eu iriei utilizar o os dados de nrc Saif Mohammad and Peter Turney.\np1 \u0026lt;- tidy_title %\u0026gt;% inner_join(get_sentiments(\u0026#39;nrc\u0026#39;)) %\u0026gt;% group_by(sentiment, news) %\u0026gt;% summarise(n = n()) %\u0026gt;% mutate(prop = n/sum(n)) Disgust (nojo) aparenta ser o sentimento mais comum envolvendo títulos de notícias falsas, enquanto que trust (confiança) é o menor, mesmo que ainda seja maior que 50%. Em geral, títulos de notícias falsas aparentam ter mais \u0026ldquo;sentimento\u0026rdquo; do que títulos de notícias reais, neste banco de dados. Isso vale até para sentimentos positivos como joy (alegria) e surprise (surpresa).\np2 \u0026lt;- tidy_corpus %\u0026gt;% inner_join(get_sentiments(\u0026#39;nrc\u0026#39;)) %\u0026gt;% group_by(sentiment, news) %\u0026gt;% summarise(n = n()) %\u0026gt;% mutate(prop = n/sum(n)) Para o corpo das notícias reais pode-se notas que os mesmos sentimentos são prevalentes, mas a proporção é menor comparado ao título. Um artigo de notícias falsas perde confiança (trust) quando o leitor lê o corpo do artigo. Ele também se torna menos negativo (negative) e apresenta menos medo (fear)\nUma melhoria que poderia ser feito aqui é tentar construir nosso próprio dicionário de palavras redundantes (stopwords) e alterar a forma que a toneização foi feita. Pois houveram momentos em que trump e trump\u0026rsquo;s não corresponderam a mesma coisa e se estes dados tivessem sido usados para treinar um modelo isso poderia se tornar um problema.\n","permalink":"https://devmedeiros.github.io/pt/post/2021-10-12-fakenews-sentiment/","summary":"Eu quero fazer uma análise de sentimentos usando o R como uma forma de aprender. Com isso em mente começamos carregando todos os pacotes que iremos utilizar.\nlibrary(readr) library(dplyr) library(tidytext) library(tokenizers) library(stopwords) library(ggplot2) Então precisamos carregar nosso banco de dados. Esses dados são do Kaggle Fake and real news dataset.\nFake \u0026lt;- read_csv(\u0026#39;~/fakenews/Fake.csv\u0026#39;) True \u0026lt;- read.csv(\u0026#39;~/fakenews/True.csv\u0026#39;) Eu quero unir ambos os dados, mas antes é preciso criar uma nova columa que irá informar de onde os dados vieram.","title":"Análise de Sentimentos de Notícias Falsas vs Notícias Reais"},{"content":"Oi! Meu nome é Jaqueline Souza Medeiros, tenho 25 anos e sou formada em Estatística pela Universidade Federal de Goiás. O principal propósito do blog é escrever sobre o que eu aprendi e estou aprendendo sobre Ciência de Dados.\nEu amo aprender sobre outras culturas e linguagens, e por isso, eu desenvolvi um grande interesse em Processamento Natural de Linguagem. Também estou interessada em simulação e aprendizado de máquina, então espero poder explorar estas áreas em breve.\nNo meu tempo livre eu gosto de praticar yoga, jogar D\u0026amp;D e brincar com meus gatos.\nVocê pode me encontrar em:\n Linkedin: medeiros-jaqueline Github: devmedeiros  ","permalink":"https://devmedeiros.github.io/pt/about/","summary":"about","title":"Sobre"}]